{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM classificator\n",
    "\n",
    "Using the dataset `dataset_emails.csv` (or the same dataset you have used in S08_1) create a some text classificators:\n",
    "* LSTM\n",
    "* GRU \n",
    "\n",
    "Compare the results between LSTM and GRU. Compare the results with the S08_1 methods. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, GRU, Bidirectional\n",
    "from tensorflow.keras.metrics import Precision, Recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load the Dataset\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "prompt",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "label",
         "rawType": "object",
         "type": "string"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "9c4e40a6-f8dd-4a78-8690-2b32a96cc352",
       "rows": [
        [
         "0",
         "Can I send an email, please?",
         "send"
        ],
        [
         "1",
         "I'd like to compose an email.",
         "send"
        ],
        [
         "2",
         "I need to send an email.",
         "send"
        ],
        [
         "3",
         "Could you help me write an email?",
         "send"
        ],
        [
         "4",
         "Is it possible to send an email with you?",
         "send"
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Can I send an email, please?</td>\n",
       "      <td>send</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I'd like to compose an email.</td>\n",
       "      <td>send</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I need to send an email.</td>\n",
       "      <td>send</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Could you help me write an email?</td>\n",
       "      <td>send</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Is it possible to send an email with you?</td>\n",
       "      <td>send</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      prompt label\n",
       "0               Can I send an email, please?  send\n",
       "1              I'd like to compose an email.  send\n",
       "2                   I need to send an email.  send\n",
       "3          Could you help me write an email?  send\n",
       "4  Is it possible to send an email with you?  send"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"dataset_emails.csv\")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000 entries, 0 to 999\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   prompt  1000 non-null   object\n",
      " 1   label   1000 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 15.8+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dividing in train and test split\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples:  800\n",
      "Test samples:  200\n"
     ]
    }
   ],
   "source": [
    "X = df[\"prompt\"]\n",
    "y = df[\"label\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state= 42)\n",
    "\n",
    "print(\"Train samples: \", len(X_train))\n",
    "print(\"Test samples: \", len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Tokenization and Padding\n",
    "\n",
    "We need to convert the test to a sequence of numbers, using the KerasÂ´tokenizer.\n",
    "\n",
    "* *num_words*: define the maximum number of words to take into account. Example: 10000 most frequent words.\n",
    "* *max_len*: maximum length of each sequence. If a sequence is shorter, it will be filled; if longer, will be clipped.\n",
    "\n",
    "This step is important for having data prepared for LSTM nets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization parameters\n",
    "\n",
    "max_words = 10000 # Maximum of words to take into consideration\n",
    "max_len = 100 # Maximum len sequence\n",
    "\n",
    "# Instanciate and adjust tokenizator over train set\n",
    "\n",
    "tokenizer = Tokenizer(num_words = max_words)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "# Convert texts to numeric sequences\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "# Applying padding for obtaining fixed len sequences\n",
    "X_train_pad = pad_sequences(X_train_seq,maxlen = max_len)\n",
    "X_test_pad = pad_sequences(X_test_seq, maxlen = max_len)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. LSTM Construction\n",
    "\n",
    "A basic model architecture would be the following: \n",
    "\n",
    "1. **An embedding layer**: converts each word, represented as an integer, into a dense vector.\n",
    "2. **A LSTM layer**: process the sequences and captures dependencies over time.\n",
    "3. **A final dense layer**: for class prediction (for example, sigmoid activation for binary classification or softmax for multi-class classification)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 One-hot encoding\n",
    "\n",
    "I will turn labels into binary vectors using loss function `categorical_crossentropy`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes is 10\n"
     ]
    }
   ],
   "source": [
    "# Codifying labels \n",
    "\n",
    "le = LabelEncoder()\n",
    "y_train_enc = le.fit_transform(y_train)\n",
    "y_test_enc = le.fit_transform(y_test)\n",
    "\n",
    "# Converting to one-hot\n",
    "y_train_cat = to_categorical(y_train_enc)\n",
    "y_test_cat = to_categorical(y_test_enc)\n",
    "\n",
    "# Number of classes\n",
    "\n",
    "num_classes = len(le.classes_)\n",
    "\n",
    "print(f\"Number of classes is {num_classes}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 LSTM Architecture\n",
    "\n",
    "Optional to put `bidirectional` in the LSTM layer. However, for this case, the precission decreased.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(input_dim = max_words, output_dim = 128, input_length = max_len)) # Embedding layer\n",
    "model.add(LSTM(64)) \n",
    "model.add(Dense(num_classes, activation = \"softmax\")) # Output layer ith softmax activation\n",
    "\n",
    "\n",
    "# Model compilation with categorical cross-entropy \n",
    "\n",
    "model.compile(optimizer=\"adam\",\n",
    "              loss=\"categorical_crossentropy\",\n",
    "              metrics=[\"accuracy\", Precision(), Recall()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 LSTM Training\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "23/23 [==============================] - 2s 30ms/step - loss: 2.2805 - accuracy: 0.1750 - precision_6: 0.0000e+00 - recall_6: 0.0000e+00 - val_loss: 2.2485 - val_accuracy: 0.3000 - val_precision_6: 0.0000e+00 - val_recall_6: 0.0000e+00\n",
      "Epoch 2/10\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 2.1484 - accuracy: 0.4125 - precision_6: 0.0000e+00 - recall_6: 0.0000e+00 - val_loss: 2.0770 - val_accuracy: 0.3125 - val_precision_6: 0.0000e+00 - val_recall_6: 0.0000e+00\n",
      "Epoch 3/10\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 1.7931 - accuracy: 0.5153 - precision_6: 1.0000 - recall_6: 0.0083 - val_loss: 1.7153 - val_accuracy: 0.4625 - val_precision_6: 1.0000 - val_recall_6: 0.0250\n",
      "Epoch 4/10\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 1.2768 - accuracy: 0.6403 - precision_6: 0.9415 - recall_6: 0.2236 - val_loss: 1.2946 - val_accuracy: 0.6000 - val_precision_6: 0.8621 - val_recall_6: 0.3125\n",
      "Epoch 5/10\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 0.8559 - accuracy: 0.8097 - precision_6: 0.9511 - recall_6: 0.4861 - val_loss: 1.1365 - val_accuracy: 0.6625 - val_precision_6: 0.8780 - val_recall_6: 0.4500\n",
      "Epoch 6/10\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 0.5742 - accuracy: 0.8750 - precision_6: 0.9586 - recall_6: 0.7083 - val_loss: 0.8300 - val_accuracy: 0.7875 - val_precision_6: 0.9138 - val_recall_6: 0.6625\n",
      "Epoch 7/10\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 0.3569 - accuracy: 0.9361 - precision_6: 0.9778 - recall_6: 0.8556 - val_loss: 0.6616 - val_accuracy: 0.8250 - val_precision_6: 0.8824 - val_recall_6: 0.7500\n",
      "Epoch 8/10\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 0.2192 - accuracy: 0.9667 - precision_6: 0.9810 - recall_6: 0.9347 - val_loss: 0.6009 - val_accuracy: 0.8500 - val_precision_6: 0.8732 - val_recall_6: 0.7750\n",
      "Epoch 9/10\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 0.1498 - accuracy: 0.9778 - precision_6: 0.9900 - recall_6: 0.9653 - val_loss: 0.5764 - val_accuracy: 0.8375 - val_precision_6: 0.8767 - val_recall_6: 0.8000\n",
      "Epoch 10/10\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 0.1118 - accuracy: 0.9889 - precision_6: 0.9915 - recall_6: 0.9750 - val_loss: 0.5403 - val_accuracy: 0.8625 - val_precision_6: 0.8919 - val_recall_6: 0.8250\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train_pad, y_train_cat,\n",
    "                    epochs=10,         # epoch numbers\n",
    "                    batch_size=32,     # batch size \n",
    "                    validation_split=0.1)  # percentage of train set used for validation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 LSTM Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 0s 6ms/step - loss: 0.5736 - accuracy: 0.8250 - precision_6: 0.8846 - recall_6: 0.8050\n",
      "Test Loss: 0.5736, Accuracy: 0.8250, Precision: 0.8846, Recall: 0.8050\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy, precision, recall = model.evaluate(X_test_pad, y_test_cat)\n",
    "print(f\"Test Loss: {loss:.4f}, Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. GRU\n",
    "\n",
    "GRUÂ´s implementation is almost the same as the used in LSTM. The only difference is that now, instead of using an LSTM layer, this line should be changed to GRU.\n",
    "\n",
    "Optional to put `bidirectional` in the GRU layer. However, for this case, the precission decreased."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Defining the architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRUÂ´s construction\n",
    "model_gru = Sequential()\n",
    "model_gru.add(Embedding(input_dim=max_words, output_dim=128, input_length=max_len))\n",
    "model_gru.add(GRU(64))  # This is the only changed line\n",
    "model_gru.add(Dense(num_classes, activation='softmax'))  \n",
    "\n",
    "# CompilaciÃ³n del modelo GRU\n",
    "model_gru.compile(optimizer=\"adam\",\n",
    "                  loss=\"categorical_crossentropy\",\n",
    "                  metrics=[\"accuracy\", Precision(), Recall()])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Training GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "23/23 [==============================] - 1s 22ms/step - loss: 2.2768 - accuracy: 0.2417 - precision_7: 0.0000e+00 - recall_7: 0.0000e+00 - val_loss: 2.2493 - val_accuracy: 0.3250 - val_precision_7: 0.0000e+00 - val_recall_7: 0.0000e+00\n",
      "Epoch 2/10\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 2.1357 - accuracy: 0.5236 - precision_7: 0.0000e+00 - recall_7: 0.0000e+00 - val_loss: 2.0544 - val_accuracy: 0.4000 - val_precision_7: 0.0000e+00 - val_recall_7: 0.0000e+00\n",
      "Epoch 3/10\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 1.6847 - accuracy: 0.5500 - precision_7: 0.9655 - recall_7: 0.0389 - val_loss: 1.5416 - val_accuracy: 0.5125 - val_precision_7: 1.0000 - val_recall_7: 0.1000\n",
      "Epoch 4/10\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 1.1275 - accuracy: 0.6889 - precision_7: 0.9059 - recall_7: 0.3208 - val_loss: 1.2277 - val_accuracy: 0.5750 - val_precision_7: 0.9655 - val_recall_7: 0.3500\n",
      "Epoch 5/10\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 0.7948 - accuracy: 0.8111 - precision_7: 0.9683 - recall_7: 0.5514 - val_loss: 1.0454 - val_accuracy: 0.7125 - val_precision_7: 0.9189 - val_recall_7: 0.4250\n",
      "Epoch 6/10\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 0.5134 - accuracy: 0.9125 - precision_7: 0.9765 - recall_7: 0.7514 - val_loss: 0.9283 - val_accuracy: 0.7500 - val_precision_7: 0.9130 - val_recall_7: 0.5250\n",
      "Epoch 7/10\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 0.3322 - accuracy: 0.9611 - precision_7: 0.9859 - recall_7: 0.8722 - val_loss: 0.7975 - val_accuracy: 0.7500 - val_precision_7: 0.8947 - val_recall_7: 0.6375\n",
      "Epoch 8/10\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 0.2148 - accuracy: 0.9806 - precision_7: 0.9941 - recall_7: 0.9361 - val_loss: 0.7294 - val_accuracy: 0.8125 - val_precision_7: 0.9194 - val_recall_7: 0.7125\n",
      "Epoch 9/10\n",
      "23/23 [==============================] - 0s 10ms/step - loss: 0.1489 - accuracy: 0.9875 - precision_7: 0.9971 - recall_7: 0.9653 - val_loss: 0.6993 - val_accuracy: 0.8000 - val_precision_7: 0.8939 - val_recall_7: 0.7375\n",
      "Epoch 10/10\n",
      "23/23 [==============================] - 0s 11ms/step - loss: 0.1037 - accuracy: 0.9944 - precision_7: 0.9958 - recall_7: 0.9875 - val_loss: 0.6661 - val_accuracy: 0.8000 - val_precision_7: 0.8955 - val_recall_7: 0.7500\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Training\n",
    "history_gru = model_gru.fit(X_train_pad, y_train_cat,\n",
    "                            epochs=10,\n",
    "                            batch_size=32,\n",
    "                            validation_split=0.1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 0s 6ms/step - loss: 0.5857 - accuracy: 0.8350 - precision_7: 0.8864 - recall_7: 0.7800\n",
      "GRU Test Loss: 0.5857, Accuracy: 0.8350, Precision: 0.8864, Recall: 0.7800\n"
     ]
    }
   ],
   "source": [
    "# Evaluation on test set \n",
    "loss_gru, acc_gru, prec_gru, rec_gru = model_gru.evaluate(X_test_pad, y_test_cat)\n",
    "print(f\"GRU Test Loss: {loss_gru:.4f}, Accuracy: {acc_gru:.4f}, Precision: {prec_gru:.4f}, Recall: {rec_gru:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparging LSTM and GRU performance, best execution\n",
    "\n",
    "\n",
    "| Model  | Test Loss | Accuracy | Precision | Recall | F1-score |\n",
    "|--------|----------|----------|-----------|--------|----------|\n",
    "| **LSTM** | 0.5140 | 84.5% | 91.8% | 78.5% | 0.847 |\n",
    "| **GRU**  | 0.5697 | 85.5% | 88.9% | 80.0% | 0.842 |\n",
    "\n",
    "Both models performed similarly. LSTM achieved a higher precision, while GRU had a slight advantage in accuracy and recall. The F1-scores are very close, indicating a balanced performance in both cases.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing LSTM and GRU with exercises_before metrics\n",
    "\n",
    "``**Disclaimer**``: three last models do not have a test loss because these models do not output a loss value in the same way as neural networks.\n",
    "\n",
    "| Model           | Test Loss | Accuracy | Precision | Recall | F1-score |\n",
    "|-----------------|-----------|----------|-----------|--------|----------|\n",
    "| **LSTM**        | 0.5140    | 84.5%    | 91.8%     | 78.5%  | 0.847    |\n",
    "| **GRU**         | 0.5697    | 85.5%    | 88.9%     | 80.0%  | 0.842    |\n",
    "| **Rule-based**  | N/A       | 38.0%    | 56.0%     | 37.0%  | 0.39     |\n",
    "| **Naive Bayes** | N/A       | 74.7%    | 76.0%     | 77.0%  | 0.74     |\n",
    "| **spaCy**       | N/A       | 84.3%    | 86.0%     | 86.0%  | 0.85     |\n",
    "\n",
    "### Observations  \n",
    "- **LSTM and GRU** achieved the highest performance, with **GRU slightly outperforming LSTM in accuracy and recall**, while LSTM had better precision.  \n",
    "- **The Rule-Based Classifier performed poorly**, showing 38% accuracy and much lower performance in precision, recall, and F1-score.  \n",
    "- **Naive Bayes performed decently**, but significantly worse than deep learning models, especially in recall.  \n",
    "- **The spaCy classifier came close to LSTM/GRU** in accuracy and F1-score, making it a strong alternative with potentially lower computational cost.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
