{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate a real classifier\n",
    "\n",
    "This code is an example of the use of VADER classifier from NLTK. It is a Naive-Bayes classifier that is trainded with a lexicon and dataset of movie reviews.\n",
    "\n",
    "Look in the example how the library SKLearn is used to evaulate the classifier.\n",
    "\n",
    "At the end you have an example on how to use the classifier en custom examples. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import movie_reviews\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.classify.util import accuracy\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     C:\\Users\\victo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\victo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\victo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\victo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download required NLTK datasets\n",
    "nltk.download('movie_reviews')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the data\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def extract_features(words):\n",
    "    return {word: True for word in words if word.lower() not in stop_words}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the dataset\n",
    "documents = [(list(movie_reviews.words(fileid)), category)\n",
    "             for category in movie_reviews.categories()\n",
    "             for fileid in movie_reviews.fileids(category)]\n",
    "random.shuffle(documents)  # Shuffle the dataset for better randomness\n",
    "\n",
    "# Feature extraction\n",
    "feature_sets = [(extract_features(words), category) for (words, category) in documents]\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_size = int(len(feature_sets) * 0.8)\n",
    "train_set, test_set = feature_sets[:train_size], feature_sets[train_size:]\n",
    "\n",
    "# Train a Naive Bayes Classifier\n",
    "classifier = NaiveBayesClassifier.train(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Naive Bayes Classifier Evaluation:\n",
      "Accuracy: 70.25%\n",
      "Most Informative Features\n",
      "               miserably = True              neg : pos    =     13.9 : 1.0\n",
      "               ludicrous = True              neg : pos    =     13.2 : 1.0\n",
      "                captures = True              pos : neg    =     11.2 : 1.0\n",
      "              astounding = True              pos : neg    =     10.8 : 1.0\n",
      "                  avoids = True              pos : neg    =     10.8 : 1.0\n",
      "                  debate = True              pos : neg    =     10.2 : 1.0\n",
      "               insulting = True              neg : pos    =     10.0 : 1.0\n",
      "               atrocious = True              neg : pos    =      9.8 : 1.0\n",
      "            manipulation = True              pos : neg    =      9.5 : 1.0\n",
      "          excruciatingly = True              neg : pos    =      9.2 : 1.0\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the classifier\n",
    "print(\"\\nNaive Bayes Classifier Evaluation:\")\n",
    "print(f\"Accuracy: {accuracy(classifier, test_set) * 100:.2f}%\")\n",
    "classifier.show_most_informative_features(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.93      0.46      0.61       207\n",
      "         pos       0.62      0.96      0.76       193\n",
      "\n",
      "    accuracy                           0.70       400\n",
      "   macro avg       0.78      0.71      0.69       400\n",
      "weighted avg       0.78      0.70      0.68       400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Prepare predictions and true labels for sklearn metrics\n",
    "y_true = [label for (_, label) in test_set]\n",
    "y_pred = [classifier.classify(features) for (features, _) in test_set]\n",
    "# Evaluate using sklearn metrics\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_true, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[ 95 112]\n",
      " [  7 186]]\n"
     ]
    }
   ],
   "source": [
    "# Confusion Matrix\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_true, y_pred))\n",
    "\n",
    "# VADER Sentiment Analysis on custom examples\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "example_sentences = [\n",
    "    \"I absolutely loved this movie! The acting was fantastic.\",\n",
    "    \"This was the worst film I have ever seen.\",\n",
    "    \"The plot was predictable, but the cinematography was beautiful.\",\n",
    "    \"I wouldn't recommend it. It was boring and too long.\"\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "VADER Sentiment Analysis:\n",
      "Sentence: I absolutely loved this movie! The acting was fantastic.\n",
      "Sentiment: positive (Score: 0.8436)\n",
      "\n",
      "Sentence: This was the worst film I have ever seen.\n",
      "Sentiment: negative (Score: -0.6249)\n",
      "\n",
      "Sentence: The plot was predictable, but the cinematography was beautiful.\n",
      "Sentiment: positive (Score: 0.7469)\n",
      "\n",
      "Sentence: I wouldn't recommend it. It was boring and too long.\n",
      "Sentiment: negative (Score: -0.5283)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nVADER Sentiment Analysis:\")\n",
    "for sentence in example_sentences:\n",
    "    score = sia.polarity_scores(sentence)\n",
    "    sentiment = \"positive\" if score['compound'] > 0 else \"negative\"\n",
    "    print(f\"Sentence: {sentence}\\nSentiment: {sentiment} (Score: {score['compound']})\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise:\n",
    "\n",
    "Create your own gold standard and measure Precission, Recall, and F1 manually and with SKLearn to check if the result is the same. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Creating the gold standard\n",
    "\n",
    "IÂ´ll create a list of tuples with 14 examples of phrases with positive and negative meanings, using the variable `gold_standard`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our gold standard data - manually labeled examples\n",
    "gold_standard = [\n",
    "    (\"The movie was absolutely fantastic, I loved every minute of it\", \"pos\"),\n",
    "    (\"The actors delivered terrible performances, complete waste of time\", \"neg\"),\n",
    "    (\"While the special effects where great, the story was boring\", \"neg\"),\n",
    "    (\"I found the plot somewhat predictable but still enjoyed it\", \"pos\"),\n",
    "    (\"The plot was interesting, but the performance of the actors made it hard to watch\", \"neg\"),\n",
    "    (\"One of the best movies I have seen this year\", \"pos\"),\n",
    "    (\"Terrible movie, I would love if I could ask for a refund after watching it\", \"neg\"),\n",
    "    (\"The director's vision really shines through in every scene\", \"pos\"),\n",
    "    (\"Despite the large budget, the film feels cheap and rushed\", \"neg\"),\n",
    "    (\"I was moved to tears by the powerful performances\", \"pos\"),\n",
    "    (\"The dialogue was so poorly written it became unintentionally funny\", \"neg\"),\n",
    "    (\"A masterpiece that will be remembered for generations\", \"pos\"),\n",
    "    (\"The cinematography was stunning even though the plot had some holes\", \"pos\"),\n",
    "    (\"Too long and drawn out, I found myself checking my watch repeatedly\", \"neg\"),\n",
    "    (\"An average film that neither impresses nor disappoints\", \"neg\")\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Separating the gold Standard Data into texts and labels\n",
    "\n",
    "I do not extract the stopwords as Vader knows them, so I can skip this step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the first element of the tuples for the texts\n",
    "texts = [value[0] for value in gold_standard]\n",
    "\n",
    "# Extract the second element of the tuples for pos neg labels\n",
    "true_labels = [value[1] for value in gold_standard]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The movie was absolutely fantastic, I loved every minute of it', 'The actors delivered terrible performances, complete waste of time', 'While the special effects where great, the story was boring', 'I found the plot somewhat predictable but still enjoyed it', 'The plot was interesting, but the performance of the actors made it hard to watch', 'One of the best movies I have seen this year', 'Terrible movie, I would love if I could ask for a refund after watching it', \"The director's vision really shines through in every scene\", 'Despite the large budget, the film feels cheap and rushed', 'I was moved to tears by the powerful performances', 'The dialogue was so poorly written it became unintentionally funny', 'A masterpiece that will be remembered for generations', 'The cinematography was stunning even though the plot had some holes', 'Too long and drawn out, I found myself checking my watch repeatedly', 'An average film that neither impresses nor disappoints']\n",
      "['pos', 'neg', 'neg', 'pos', 'neg', 'pos', 'neg', 'pos', 'neg', 'pos', 'neg', 'pos', 'pos', 'neg', 'neg']\n"
     ]
    }
   ],
   "source": [
    "print(texts)\n",
    "\n",
    "print(true_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Sentiment Analysis\n",
    "In the next step, I will make a sentiment analysys of our gold standard using VADER, and stroing these predicted labels in a list to compare the results after."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "VADER Sentiment Analysis in custom gold Standard:\n",
      "Sentence: The movie was absolutely fantastic, I loved every minute of it\n",
      "Sentiment: pos (Score: 0.8431)\n",
      "\n",
      "Sentence: The actors delivered terrible performances, complete waste of time\n",
      "Sentiment: neg (Score: -0.7096)\n",
      "\n",
      "Sentence: While the special effects where great, the story was boring\n",
      "Sentiment: pos (Score: 0.6705)\n",
      "\n",
      "Sentence: I found the plot somewhat predictable but still enjoyed it\n",
      "Sentiment: pos (Score: 0.6652)\n",
      "\n",
      "Sentence: The plot was interesting, but the performance of the actors made it hard to watch\n",
      "Sentiment: pos (Score: 0.0644)\n",
      "\n",
      "Sentence: One of the best movies I have seen this year\n",
      "Sentiment: pos (Score: 0.6369)\n",
      "\n",
      "Sentence: Terrible movie, I would love if I could ask for a refund after watching it\n",
      "Sentiment: pos (Score: 0.2732)\n",
      "\n",
      "Sentence: The director's vision really shines through in every scene\n",
      "Sentiment: pos (Score: 0.25)\n",
      "\n",
      "Sentence: Despite the large budget, the film feels cheap and rushed\n",
      "Sentiment: neg (Score: 0.0)\n",
      "\n",
      "Sentence: I was moved to tears by the powerful performances\n",
      "Sentiment: pos (Score: 0.2263)\n",
      "\n",
      "Sentence: The dialogue was so poorly written it became unintentionally funny\n",
      "Sentiment: pos (Score: 0.4404)\n",
      "\n",
      "Sentence: A masterpiece that will be remembered for generations\n",
      "Sentiment: pos (Score: 0.6249)\n",
      "\n",
      "Sentence: The cinematography was stunning even though the plot had some holes\n",
      "Sentiment: pos (Score: 0.3818)\n",
      "\n",
      "Sentence: Too long and drawn out, I found myself checking my watch repeatedly\n",
      "Sentiment: neg (Score: 0.0)\n",
      "\n",
      "Sentence: An average film that neither impresses nor disappoints\n",
      "Sentiment: neg (Score: -0.5315)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predicted_labels = []\n",
    "\n",
    "print(\"\\nVADER Sentiment Analysis in custom gold Standard:\")\n",
    "for sentence in texts:\n",
    "    score = sia.polarity_scores(sentence)\n",
    "    sentiment = \"pos\" if score['compound'] > 0 else \"neg\"\n",
    "    # Storing the prediction\n",
    "    predicted_labels.append(sentiment)\n",
    "    print(f\"Sentence: {sentence}\\nSentiment: {sentiment} (Score: {score['compound']})\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Next step: calculate precission, recall and F1 Score.\n",
    "\n",
    "The next part is calculating manually this 3 metrics. Before that, IÂ´ll make a short explanation of every one of this concepts\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"image.png\" alt=\"alt text\" width=\"60%\">\n",
    "</div>\n",
    "\n",
    "1. **Precission**: out of all instances predicted as positive, how many of them where actually positive?\n",
    "\n",
    "    $$\\frac{\\text{true\\_positives}}{\\text{true\\_positives} + \\text{false\\_positives}}$$\n",
    "\n",
    "2. **Recall**: out of all actual positive instances, how many were correctly predicted? \n",
    "\n",
    "    $$\\frac{\\text{true\\_positives}}{\\text{true\\_positives} + \\text{false\\_negatives}}$$\n",
    "\n",
    "3. **F1 Score:** harmonic mean of precission and recall.\n",
    "    $$\\frac{2*P*R}{P + R}$$\n",
    "\n",
    "#### Now, IÂ´ll define the true and false positives, true and false negatives, before proceeding to implement these metrics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize counters\n",
    "true_positives = 0\n",
    "false_positives = 0\n",
    "true_negatives = 0\n",
    "false_negatives = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through each pair of true and predicted labels\n",
    "for true, pred in zip(true_labels, predicted_labels):\n",
    "    if true == \"pos\" and pred == \"pos\":\n",
    "        true_positives += 1\n",
    "    elif true == \"neg\" and pred == \"pos\":\n",
    "        false_positives += 1\n",
    "    elif true == \"neg\" and pred == \"neg\":\n",
    "        true_negatives += 1\n",
    "    elif true == \"pos\" and pred == \"neg\":\n",
    "        false_negatives += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Positives: 7\n",
      "False Positives: 4\n",
      "True Negatives: 4\n",
      "False Negatives: 0\n"
     ]
    }
   ],
   "source": [
    "# Print the results\n",
    "print(f\"True Positives: {true_positives}\")\n",
    "print(f\"False Positives: {false_positives}\")\n",
    "print(f\"True Negatives: {true_negatives}\")\n",
    "print(f\"False Negatives: {false_negatives}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Defining the metrics\n",
    "\n",
    "For this, IÂ´ll use 3 simple functions that follows the equations of the last markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6363636363636364\n"
     ]
    }
   ],
   "source": [
    "def precission_metric(gold_standard):\n",
    "    precission = true_positives / (true_positives + false_positives)\n",
    "    return precission\n",
    "\n",
    "print(precission_metric(gold_standard))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "def recall_metric(gold_standard):\n",
    "    recall = true_positives / (true_positives + false_negatives)\n",
    "\n",
    "    return recall\n",
    "\n",
    "print(recall_metric(gold_standard))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1-score: 0.7777777777777778\n"
     ]
    }
   ],
   "source": [
    "def f1_score(gold_standard):\n",
    "    recall = float(recall_metric(gold_standard))\n",
    "    precission = float(precission_metric(gold_standard))\n",
    "\n",
    "    f1 = (2 * precission * recall) / (precission + recall)\n",
    "\n",
    "    return f\"f1-score: {f1}\"\n",
    "\n",
    "print(f1_score(gold_standard))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Comparison with Scikit-Learn metrics\n",
    "\n",
    "Last step is pass this output from vader to scikit-learn built-in metrics and compare the results given."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       1.00      0.50      0.67         8\n",
      "         pos       0.64      1.00      0.78         7\n",
      "\n",
      "    accuracy                           0.73        15\n",
      "   macro avg       0.82      0.75      0.72        15\n",
      "weighted avg       0.83      0.73      0.72        15\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(true_labels, predicted_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[4 4]\n",
      " [0 7]]\n"
     ]
    }
   ],
   "source": [
    "# Confusion Matrix\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(true_labels, predicted_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclussions about differences between metrics\n",
    "\n",
    "\n",
    "iinsert table\n",
    "\n",
    "1. Scikit-learn metrics.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "2. Custom metrics.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aprendizaje_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
