{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# S09 Lab Exercise \n",
    "\n",
    "## Víctor Vega Sobral\n",
    "\n",
    "### Explanations \n",
    "The attached files are a collection of tweets labelled with sentiment in 3 categories:\n",
    "\n",
    "```json\n",
    "sentiments = {\n",
    "    \"LABEL_0\": \"Bearish\", \n",
    "    \"LABEL_1\": \"Bullish\", \n",
    "    \"LABEL_2\": \"Neutral\"\n",
    "}  \n",
    "```\n",
    "\n",
    "1. Train a LSTM network to with the training file. \n",
    "\n",
    "2. Validate the trained model with the valid file. \n",
    "\n",
    "3. Comment what you are doing in each part of your code. As the better the code, comments and result validation as the better the grade.\n",
    "\n",
    "Remember that you have to send the final file in this exercise and the file must be in your digital portfolio with all the proper commits done.\n",
    "\n",
    "- ``sent_train.csv`` \n",
    "- ``sent_valid.csv`` \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "__author__ = \"Victor Vega Sobral\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Importing necessary Libraries and Constant Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional\n",
    "from tensorflow.keras.metrics import Precision, Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 10\n",
    "BATCH_SIZE = 32\n",
    "VAL_SPLIT = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Loading the Datasets\n",
    "\n",
    "Next step is to load the two different datasets in:\n",
    "\n",
    "- `train_df`: Dataframe with the training set.\n",
    "\n",
    "- `valid_df`: Dataframe with the validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9543 entries, 0 to 9542\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   text    9543 non-null   object\n",
      " 1   label   9543 non-null   int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 149.2+ KB\n"
     ]
    }
   ],
   "source": [
    "# Training dataset\n",
    "train_df = pd.read_csv(\"data/sent_train.csv\")\n",
    "train_df.info()\n",
    "\n",
    "X_train = train_df[\"text\"]\n",
    "y_train = train_df[\"label\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4 Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2388 entries, 0 to 2387\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   text    2388 non-null   object\n",
      " 1   label   2388 non-null   int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 37.4+ KB\n"
     ]
    }
   ],
   "source": [
    "# Valdiation dataset\n",
    "valid_df = pd.read_csv(\"data/sent_valid.csv\")\n",
    "valid_df.info()\n",
    "\n",
    "X_test = valid_df[\"text\"]\n",
    "y_test = valid_df[\"label\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Dividing in train and test split (explanation)\n",
    "\n",
    "As we already have two different csv files with the train and validation, this step is already done. In other cases, `train_test_split` scikit-learn method could be used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Tokenization and Paddding\n",
    "\n",
    "Before training the LSTM, we need to convert both test and training dataframes to a sequence of numbers using the Keras tokenizer.\n",
    "\n",
    "* `num_words`: defines the maximum number of words that the LSTM will take into account.\n",
    "* `max_len`: maximum length of each sequence. This is the *paddding* step. \n",
    "* `embedding_dim`: maximum dimmensions the embedding vector will have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "############ TOKENIZATION ############\n",
    "max_words = 10000\n",
    "max_len = 100\n",
    "embedding_dim = 64 # Number of dimensions the embedding vectors will have\n",
    "\n",
    "# Instanciating and adjusting tokenizator\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "# Converting texts to numeric sequences.\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "##################### PADDING ####################\n",
    "X_train_pad = pad_sequences(X_train_seq, maxlen = max_len)\n",
    "X_test_pad = pad_sequences(X_test_seq, maxlen = max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. LSTM Construction\n",
    "\n",
    "LSTMs have this basic model architecture:\n",
    "\n",
    "1. **Embedding layer**: converts each word, represented as an integer, into a dense vector.\n",
    "\n",
    "2. **LSTM layer**: where it processes the sequences and captures dependencies over time. It´s recommended to also put it to be `Bidirectional`, but deppending of the case, the noise added by this can produce worse results.\n",
    "\n",
    "3. **Final dense layer**: for class prediction, in this case, I´ll use `softmax`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1 Loss functions: categorical cross entropy.\n",
    "\n",
    "Categorical Cross Entropy is widely used for LSTMs. However, for using it, first we need to encode the labels into binary vectors. That is, **one-hot encoding**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes:  3\n"
     ]
    }
   ],
   "source": [
    "label_enc = LabelEncoder()\n",
    "\n",
    "y_train_encoded = label_enc.fit_transform(y_train)\n",
    "y_test_encoded = label_enc.fit_transform(y_test)\n",
    "\n",
    "# Converting to one-hot\n",
    "y_train_cat = to_categorical(y_train_encoded)\n",
    "y_test_cat = to_categorical(y_test_encoded)\n",
    "\n",
    "# Verify the number of classes using the classes_ method\n",
    "# of the label encoder\n",
    "num_clases = len(label_enc.classes_)\n",
    "\n",
    "print(\"Number of classes: \", num_clases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2 LSTM Architecture\n",
    "\n",
    "In this cell I will build the LSTM architecture mentioned in the previous ``cell 5``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### Model Architecture ############\n",
    "model = Sequential([\n",
    "    # Embedding layer\n",
    "    Embedding(input_dim = max_words, \n",
    "              output_dim = embedding_dim,\n",
    "              input_length = max_len),\n",
    "    #########\n",
    "\n",
    "    # LSTM Layer\n",
    "    Bidirectional(LSTM(embedding_dim)),\n",
    "    ###########\n",
    "\n",
    "    # Dense layer\n",
    "    Dense(num_clases, activation = \"softmax\")\n",
    "    ###########\n",
    "])\n",
    "\n",
    "############ Model Optimizer ###############\n",
    "\n",
    "### Adam Optimizer with more metrics added like Precision and Recall\n",
    "model.compile(optimizer =\"adam\",\n",
    "              loss = \"categorical_crossentropy\",\n",
    "              metrics = [\"accuracy\", Precision(), Recall()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3 LSTM Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "269/269 [==============================] - 19s 65ms/step - loss: 0.7208 - accuracy: 0.7041 - precision_4: 0.7614 - recall_4: 0.6214 - val_loss: 0.9648 - val_accuracy: 0.5654 - val_precision_4: 0.6222 - val_recall_4: 0.4398\n",
      "Epoch 2/10\n",
      "269/269 [==============================] - 17s 64ms/step - loss: 0.4778 - accuracy: 0.7882 - precision_4: 0.8670 - recall_4: 0.7168 - val_loss: 1.0030 - val_accuracy: 0.5550 - val_precision_4: 0.6096 - val_recall_4: 0.4398\n",
      "Epoch 3/10\n",
      "269/269 [==============================] - 17s 63ms/step - loss: 0.3143 - accuracy: 0.8781 - precision_4: 0.9142 - recall_4: 0.8378 - val_loss: 1.1947 - val_accuracy: 0.6262 - val_precision_4: 0.6500 - val_recall_4: 0.5853\n",
      "Epoch 4/10\n",
      "269/269 [==============================] - 16s 59ms/step - loss: 0.1691 - accuracy: 0.9429 - precision_4: 0.9549 - recall_4: 0.9311 - val_loss: 1.4217 - val_accuracy: 0.6199 - val_precision_4: 0.6352 - val_recall_4: 0.5979\n",
      "Epoch 5/10\n",
      "269/269 [==============================] - 16s 61ms/step - loss: 0.0942 - accuracy: 0.9709 - precision_4: 0.9750 - recall_4: 0.9658 - val_loss: 1.6035 - val_accuracy: 0.6366 - val_precision_4: 0.6484 - val_recall_4: 0.6178\n",
      "Epoch 6/10\n",
      "269/269 [==============================] - 16s 60ms/step - loss: 0.0550 - accuracy: 0.9837 - precision_4: 0.9858 - recall_4: 0.9816 - val_loss: 1.6808 - val_accuracy: 0.6377 - val_precision_4: 0.6452 - val_recall_4: 0.6283\n",
      "Epoch 7/10\n",
      "269/269 [==============================] - 16s 60ms/step - loss: 0.0439 - accuracy: 0.9881 - precision_4: 0.9895 - recall_4: 0.9875 - val_loss: 1.7838 - val_accuracy: 0.6471 - val_precision_4: 0.6562 - val_recall_4: 0.6377\n",
      "Epoch 8/10\n",
      "269/269 [==============================] - 16s 61ms/step - loss: 0.0306 - accuracy: 0.9903 - precision_4: 0.9924 - recall_4: 0.9896 - val_loss: 1.9976 - val_accuracy: 0.6471 - val_precision_4: 0.6517 - val_recall_4: 0.6366\n",
      "Epoch 9/10\n",
      "269/269 [==============================] - 18s 65ms/step - loss: 0.0290 - accuracy: 0.9928 - precision_4: 0.9936 - recall_4: 0.9918 - val_loss: 1.7309 - val_accuracy: 0.6251 - val_precision_4: 0.6314 - val_recall_4: 0.6188\n",
      "Epoch 10/10\n",
      "269/269 [==============================] - 17s 64ms/step - loss: 0.0163 - accuracy: 0.9957 - precision_4: 0.9960 - recall_4: 0.9957 - val_loss: 2.2249 - val_accuracy: 0.6419 - val_precision_4: 0.6474 - val_recall_4: 0.6325\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "history = model.fit(X_train_pad, y_train_cat,\n",
    "                    epochs = NUM_EPOCHS, # epoch numbers\n",
    "                    batch_size = BATCH_SIZE, # batch size\n",
    "                    validation_split = VAL_SPLIT, # percentage of training data \n",
    "                                                # used for validation\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 LSTM Performance on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75/75 [==============================] - 3s 44ms/step - loss: 1.3546 - accuracy: 0.7567 - precision_4: 0.7599 - recall_4: 0.7542\n",
      "Test Loss: 1.3546, Accuracy: 0.7567, Precision: 0.7599, Recall: 0.7542\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy, precision, recall = model.evaluate(X_test_pad, y_test_cat)\n",
    "print(f\"Test Loss: {loss:.4f}, Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aprendizaje_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
