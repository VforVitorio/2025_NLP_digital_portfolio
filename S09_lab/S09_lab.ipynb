{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# S09 Lab Exercise \n",
    "\n",
    "## Víctor Vega Sobral\n",
    "\n",
    "### Explanations \n",
    "The attached files are a collection of tweets labelled with sentiment in 3 categories:\n",
    "\n",
    "```json\n",
    "sentiments = {\n",
    "    \"LABEL_0\": \"Bearish\", \n",
    "    \"LABEL_1\": \"Bullish\", \n",
    "    \"LABEL_2\": \"Neutral\"\n",
    "}  \n",
    "```\n",
    "\n",
    "1. Train a LSTM network to with the training file. \n",
    "\n",
    "2. Validate the trained model with the valid file. \n",
    "\n",
    "3. Comment what you are doing in each part of your code. As the better the code, comments and result validation as the better the grade.\n",
    "\n",
    "Remember that you have to send the final file in this exercise and the file must be in your digital portfolio with all the proper commits done.\n",
    "\n",
    "- ``sent_train.csv`` \n",
    "- ``sent_valid.csv`` \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "__author__ = \"Victor Vega Sobral\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Importing necessary Libraries and Constant Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional, Dropout\n",
    "from tensorflow.keras.metrics import Precision, Recall\n",
    "from tensorflow.keras import regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 10\n",
    "BATCH_SIZE = 32\n",
    "VAL_SPLIT = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Loading the Datasets\n",
    "\n",
    "Next step is to load the two different datasets in:\n",
    "\n",
    "- `train_df`: Dataframe with the training set.\n",
    "\n",
    "- `valid_df`: Dataframe with the validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9543 entries, 0 to 9542\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   text    9543 non-null   object\n",
      " 1   label   9543 non-null   int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 149.2+ KB\n"
     ]
    }
   ],
   "source": [
    "# Training dataset\n",
    "train_df = pd.read_csv(\"data/sent_train.csv\")\n",
    "train_df.info()\n",
    "\n",
    "X_train = train_df[\"text\"]\n",
    "y_train = train_df[\"label\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4 Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2388 entries, 0 to 2387\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   text    2388 non-null   object\n",
      " 1   label   2388 non-null   int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 37.4+ KB\n"
     ]
    }
   ],
   "source": [
    "# Valdiation dataset\n",
    "valid_df = pd.read_csv(\"data/sent_valid.csv\")\n",
    "valid_df.info()\n",
    "\n",
    "X_test = valid_df[\"text\"]\n",
    "y_test = valid_df[\"label\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Dividing in train and test split (explanation)\n",
    "\n",
    "As we already have two different csv files with the train and validation, this step is already done. In other cases, `train_test_split` scikit-learn method could be used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Tokenization and Paddding\n",
    "\n",
    "Before training the LSTM, we need to convert both test and training dataframes to a sequence of numbers using the Keras tokenizer.\n",
    "\n",
    "* `num_words`: defines the maximum number of words that the LSTM will take into account.\n",
    "* `max_len`: maximum length of each sequence. This is the *paddding* step. \n",
    "* `embedding_dim`: maximum dimmensions the embedding vector will have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "############ TOKENIZATION ############\n",
    "max_words = 10000\n",
    "max_len = 100\n",
    "embedding_dim = 64 # Number of dimensions the embedding vectors will have\n",
    "\n",
    "# Instanciating and adjusting tokenizator\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "# Converting texts to numeric sequences.\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "##################### PADDING ####################\n",
    "X_train_pad = pad_sequences(X_train_seq, maxlen = max_len)\n",
    "X_test_pad = pad_sequences(X_test_seq, maxlen = max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. LSTM Construction\n",
    "\n",
    "LSTMs have this basic model architecture:\n",
    "\n",
    "1. **Embedding layer**: converts each word, represented as an integer, into a dense vector.\n",
    "\n",
    "2. **LSTM layer**: where it processes the sequences and captures dependencies over time. It´s recommended to also put it to be `Bidirectional`, but deppending of the case, the noise added by this can produce worse results.\n",
    "\n",
    "3. **Final dense layer**: for class prediction, in this case, I´ll use `softmax`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1 Loss functions: categorical cross entropy.\n",
    "\n",
    "Categorical Cross Entropy is widely used for LSTMs. However, for using it, first we need to encode the labels into binary vectors. That is, **one-hot encoding**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes:  3\n"
     ]
    }
   ],
   "source": [
    "label_enc = LabelEncoder()\n",
    "\n",
    "y_train_encoded = label_enc.fit_transform(y_train)\n",
    "y_test_encoded = label_enc.fit_transform(y_test)\n",
    "\n",
    "# Converting to one-hot\n",
    "y_train_cat = to_categorical(y_train_encoded)\n",
    "y_test_cat = to_categorical(y_test_encoded)\n",
    "\n",
    "# Verify the number of classes using the classes_ method\n",
    "# of the label encoder\n",
    "num_clases = len(label_enc.classes_)\n",
    "\n",
    "print(\"Number of classes: \", num_clases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2 LSTM Architecture\n",
    "\n",
    "In this cell I will build the LSTM architecture mentioned in the previous ``cell 5``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### Model Architecture ############\n",
    "model = Sequential([\n",
    "    # Embedding layer\n",
    "    Embedding(input_dim = max_words, \n",
    "              output_dim = embedding_dim,\n",
    "              input_length = max_len),\n",
    "    #########\n",
    "\n",
    "    # LSTM Layer\n",
    "    Bidirectional(LSTM(embedding_dim,\n",
    "                       kernel_regularizer=regularizers.l2(0.001))),\n",
    "    ###########\n",
    "\n",
    "    #### Dropout layer\n",
    "    Dropout(0.2),\n",
    "    # Dense layer\n",
    "    Dense(num_clases, \n",
    "          activation = \"softmax\",\n",
    "          kernel_regularizer=regularizers.l2(0.001))\n",
    "    ###########\n",
    "])\n",
    "\n",
    "############ Model Optimizer ###############\n",
    "\n",
    "### Adam Optimizer with more metrics added like Precision and Recall\n",
    "model.compile(optimizer =\"adam\",\n",
    "              loss = \"categorical_crossentropy\",\n",
    "              metrics = [\"accuracy\", Precision(), Recall()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3 LSTM Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 13/269 [>.............................] - ETA: 17s - loss: 1.2078 - accuracy: 0.5962 - precision_6: 0.5962 - recall_6: 0.1490  "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "history = model.fit(X_train_pad, y_train_cat,\n",
    "                    epochs = NUM_EPOCHS, # epoch numbers\n",
    "                    batch_size = BATCH_SIZE, # batch size\n",
    "                    validation_split = VAL_SPLIT, # percentage of training data \n",
    "                                                # used for validation\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 LSTM Performance on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75/75 [==============================] - 3s 44ms/step - loss: 1.3546 - accuracy: 0.7567 - precision_4: 0.7599 - recall_4: 0.7542\n",
      "Test Loss: 1.3546, Accuracy: 0.7567, Precision: 0.7599, Recall: 0.7542\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy, precision, recall = model.evaluate(X_test_pad, y_test_cat)\n",
    "print(f\"Test Loss: {loss:.4f}, Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Conclussions\n",
    "\n",
    "### 6.1 Possible Overfitting\n",
    "\n",
    "* Difference of performance between training and test set implies possible overfitting. \n",
    "\n",
    "* Some steps need to be made to overcome this problem.\n",
    "\n",
    "* I´ll add the `kernel_regularizer parameter` with l2 implemented to try to prevent this overfitting.\n",
    "\n",
    "* I´ll also add a ``Dropout`` layer to deactivate 20% of the ñayers to also prevent overfitting.\n",
    "### 6.2 Metrics without l2 and Dropout.\n",
    "\n",
    "* Test Loss: 1.3546\n",
    "\n",
    "* Accuracy: 0.7567\n",
    "\n",
    "* Precision: 0.7599\n",
    "\n",
    "* Recall: 0.7542\n",
    "\n",
    "### 6.3 Metrics with l2 implemented."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aprendizaje_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
