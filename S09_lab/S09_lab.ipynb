{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# S09 Lab Exercise \n",
    "\n",
    "## VÃ­ctor Vega Sobral\n",
    "\n",
    "### Explanations \n",
    "The attached files are a collection of tweets labelled with sentiment in 3 categories:\n",
    "\n",
    "```json\n",
    "sentiments = {\n",
    "    \"LABEL_0\": \"Bearish\", \n",
    "    \"LABEL_1\": \"Bullish\", \n",
    "    \"LABEL_2\": \"Neutral\"\n",
    "}  \n",
    "```\n",
    "\n",
    "1. Train a LSTM network to with the training file. \n",
    "\n",
    "2. Validate the trained model with the valid file. \n",
    "\n",
    "3. Comment what you are doing in each part of your code. As the better the code, comments and result validation as the better the grade.\n",
    "\n",
    "Remember that you have to send the final file in this exercise and the file must be in your digital portfolio with all the proper commits done.\n",
    "\n",
    "- ``sent_train.csv`` \n",
    "- ``sent_valid.csv`` \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "__author__ = \"Victor Vega Sobral\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Importing necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional\n",
    "from tensorflow.keras.metrics import Precision, Recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Loading the Datasets\n",
    "\n",
    "Next step is to load the two different datasets in:\n",
    "\n",
    "- `train_df`: Dataframe with the training set.\n",
    "\n",
    "- `valid_df`: Dataframe with the validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9543 entries, 0 to 9542\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   text    9543 non-null   object\n",
      " 1   label   9543 non-null   int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 149.2+ KB\n"
     ]
    }
   ],
   "source": [
    "# Training dataset\n",
    "train_df = pd.read_csv(\"data/sent_train.csv\")\n",
    "train_df.info()\n",
    "\n",
    "X_train = train_df[\"text\"]\n",
    "y_train = train_df[\"label\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4 Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2388 entries, 0 to 2387\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   text    2388 non-null   object\n",
      " 1   label   2388 non-null   int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 37.4+ KB\n"
     ]
    }
   ],
   "source": [
    "# Valdiation dataset\n",
    "valid_df = pd.read_csv(\"data/sent_valid.csv\")\n",
    "valid_df.info()\n",
    "\n",
    "X_test = valid_df[\"text\"]\n",
    "y_test = valid_df[\"label\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Dividing in train and test split (explanation)\n",
    "\n",
    "As we already have two different csv files with the train and validation, this step is already done. In other cases, `train_test_split` scikit-learn method could be used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Tokenization and Paddding\n",
    "\n",
    "Before training the LSTM, we need to convert both test and training dataframes to a sequence of numbers using the Keras tokenizer.\n",
    "\n",
    "* `num_words`: defines the maximum number of words that the LSTM will take into account.\n",
    "* `max_len`: maximum length of each sequence. This is the *paddding* step. \n",
    "* `embedding_dim`: maximum dimmensions the embedding vector will have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "pad_sequences() got an unexpected keyword argument 'max_len'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 15\u001b[0m\n\u001b[0;32m     12\u001b[0m X_test_seq \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mtexts_to_sequences(X_test)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m##################### PADDING ####################\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m X_train_pad \u001b[38;5;241m=\u001b[39m \u001b[43mpad_sequences\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_seq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_len\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmax_len\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m X_test_pad \u001b[38;5;241m=\u001b[39m pad_sequences(X_test_seq, max_len \u001b[38;5;241m=\u001b[39m max_len)\n",
      "\u001b[1;31mTypeError\u001b[0m: pad_sequences() got an unexpected keyword argument 'max_len'"
     ]
    }
   ],
   "source": [
    "############ TOKENIZATION ############\n",
    "max_words = 10000\n",
    "max_len = 100\n",
    "embedding_dim = 64 # Number of dimensions the embedding vectors will have\n",
    "\n",
    "# Instanciating and adjusting tokenizator\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "# Converting texts to numeric sequences.\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "##################### PADDING ####################\n",
    "X_train_pad = pad_sequences(X_train_seq, maxlen = max_len)\n",
    "X_test_pad = pad_sequences(X_test_seq, maxlen = max_len)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aprendizaje_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
