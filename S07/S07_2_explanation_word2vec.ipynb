{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XvNCXd9Dfqe1"
      },
      "source": [
        "# Word2vec with gensim\n",
        "\n",
        "In this Jupyter notebook you will use the [Gensim] library (https://radimrehurek.com/gensim/index.html) to experiment with Word2VEC.This notebook is focused on the intuition of the concepts and not on the implementation details.This notebook is inspired by this [Guide] (https://radicrehurek.com/gensim/auto_examples/ttorials/run_word2vec.html).\n",
        "\n",
        "## 1. Installation and loading the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "zKIqnDXXfpiz"
      },
      "outputs": [],
      "source": [
        "#!pip install --upgrade gensim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Sn7Q2jB3frOn"
      },
      "outputs": [],
      "source": [
        "import gensim.downloader as api"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "yBaT8djWkaZy"
      },
      "outputs": [],
      "source": [
        "model = api.load('word2vec-google-news-300')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nVZm7iTOoawW"
      },
      "source": [
        "## 2. Similarity of words\n",
        "\n",
        "In this section we will see how to achieve the similarity between two words using a Word Embedding already trained."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "kOZfaelLoi4c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.6510957"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.similarity(\"king\", \"queen\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "BX-Kk9HZofuF"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.22942671"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.similarity(\"king\", \"man\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "ypFK-pLrol3N"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.09978464"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.similarity(\"king\", \"potato\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "rBWzZySFormq"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.similarity(\"king\", \"king\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9GijWs_tx83W"
      },
      "source": [
        "Now we will see how to find the words with greater similarity to the set of specified words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "ytELAWBLk2-6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('monarch', 0.7042065858840942),\n",
              " ('kings', 0.6780862808227539),\n",
              " ('princess', 0.6731551885604858),\n",
              " ('queens', 0.6679496765136719),\n",
              " ('prince', 0.6435247659683228)]"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.most_similar([\"king\", \"queen\"], topn=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "7D4ZS7N3ovxB"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('carrots', 0.7536594867706299),\n",
              " ('tomatoes', 0.712963879108429),\n",
              " ('celery', 0.7025030851364136),\n",
              " ('broccoli', 0.6796349883079529),\n",
              " ('cherry_tomatoes', 0.662927508354187)]"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.most_similar([\"tomato\", \"carrot\"], topn=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WZFlxKjOyBpu"
      },
      "source": [
        "But you can even do interesting things such as seeing what word does not correspond to a list."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "8CrZdcBpn3pn"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'air'"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.doesnt_match([\"summer\", \"fall\", \"spring\", \"air\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ko09hZ3dqMZ1"
      },
      "source": [
        "## Exercises\n",
        "\n",
        "1. Use the Word2VEC model to make a ranking of the following 15 words according to its similarity with the words \"man\" and \"Woman\".For each pair, it prints its similarity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vzZ1eD3PpT-d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('wife', 'husband'), ('wife', 'child'), ('wife', 'queen'), ('wife', 'king'), ('wife', 'man'), ('wife', 'woman'), ('wife', 'birth'), ('wife', 'doctor'), ('wife', 'nurse'), ('wife', 'teacher'), ('wife', 'professor'), ('wife', 'engineer'), ('wife', 'scientist'), ('wife', 'president'), ('husband', 'child'), ('husband', 'queen'), ('husband', 'king'), ('husband', 'man'), ('husband', 'woman'), ('husband', 'birth'), ('husband', 'doctor'), ('husband', 'nurse'), ('husband', 'teacher'), ('husband', 'professor'), ('husband', 'engineer'), ('husband', 'scientist'), ('husband', 'president'), ('child', 'queen'), ('child', 'king'), ('child', 'man'), ('child', 'woman'), ('child', 'birth'), ('child', 'doctor'), ('child', 'nurse'), ('child', 'teacher'), ('child', 'professor'), ('child', 'engineer'), ('child', 'scientist'), ('child', 'president'), ('queen', 'king'), ('queen', 'man'), ('queen', 'woman'), ('queen', 'birth'), ('queen', 'doctor'), ('queen', 'nurse'), ('queen', 'teacher'), ('queen', 'professor'), ('queen', 'engineer'), ('queen', 'scientist'), ('queen', 'president'), ('king', 'man'), ('king', 'woman'), ('king', 'birth'), ('king', 'doctor'), ('king', 'nurse'), ('king', 'teacher'), ('king', 'professor'), ('king', 'engineer'), ('king', 'scientist'), ('king', 'president'), ('man', 'woman'), ('man', 'birth'), ('man', 'doctor'), ('man', 'nurse'), ('man', 'teacher'), ('man', 'professor'), ('man', 'engineer'), ('man', 'scientist'), ('man', 'president'), ('woman', 'birth'), ('woman', 'doctor'), ('woman', 'nurse'), ('woman', 'teacher'), ('woman', 'professor'), ('woman', 'engineer'), ('woman', 'scientist'), ('woman', 'president'), ('birth', 'doctor'), ('birth', 'nurse'), ('birth', 'teacher'), ('birth', 'professor'), ('birth', 'engineer'), ('birth', 'scientist'), ('birth', 'president'), ('doctor', 'nurse'), ('doctor', 'teacher'), ('doctor', 'professor'), ('doctor', 'engineer'), ('doctor', 'scientist'), ('doctor', 'president'), ('nurse', 'teacher'), ('nurse', 'professor'), ('nurse', 'engineer'), ('nurse', 'scientist'), ('nurse', 'president'), ('teacher', 'professor'), ('teacher', 'engineer'), ('teacher', 'scientist'), ('teacher', 'president'), ('professor', 'engineer'), ('professor', 'scientist'), ('professor', 'president'), ('engineer', 'scientist'), ('engineer', 'president'), ('scientist', 'president')]\n"
          ]
        }
      ],
      "source": [
        "words = [\n",
        "\"wife\",\n",
        "\"husband\",\n",
        "\"child\",\n",
        "\"queen\",\n",
        "\"king\",\n",
        "\"man\",\n",
        "\"woman\",\n",
        "\"birth\",\n",
        "\"doctor\",\n",
        "\"nurse\",\n",
        "\"teacher\",\n",
        "\"professor\",\n",
        "\"engineer\",\n",
        "\"scientist\",\n",
        "\"president\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('wife', 'husband'), ('wife', 'child'), ('wife', 'queen'), ('wife', 'king'), ('wife', 'man'), ('wife', 'woman'), ('wife', 'birth'), ('wife', 'doctor'), ('wife', 'nurse'), ('wife', 'teacher'), ('wife', 'professor'), ('wife', 'engineer'), ('wife', 'scientist'), ('wife', 'president'), ('husband', 'child'), ('husband', 'queen'), ('husband', 'king'), ('husband', 'man'), ('husband', 'woman'), ('husband', 'birth'), ('husband', 'doctor'), ('husband', 'nurse'), ('husband', 'teacher'), ('husband', 'professor'), ('husband', 'engineer'), ('husband', 'scientist'), ('husband', 'president'), ('child', 'queen'), ('child', 'king'), ('child', 'man'), ('child', 'woman'), ('child', 'birth'), ('child', 'doctor'), ('child', 'nurse'), ('child', 'teacher'), ('child', 'professor'), ('child', 'engineer'), ('child', 'scientist'), ('child', 'president'), ('queen', 'king'), ('queen', 'man'), ('queen', 'woman'), ('queen', 'birth'), ('queen', 'doctor'), ('queen', 'nurse'), ('queen', 'teacher'), ('queen', 'professor'), ('queen', 'engineer'), ('queen', 'scientist'), ('queen', 'president'), ('king', 'man'), ('king', 'woman'), ('king', 'birth'), ('king', 'doctor'), ('king', 'nurse'), ('king', 'teacher'), ('king', 'professor'), ('king', 'engineer'), ('king', 'scientist'), ('king', 'president'), ('man', 'woman'), ('man', 'birth'), ('man', 'doctor'), ('man', 'nurse'), ('man', 'teacher'), ('man', 'professor'), ('man', 'engineer'), ('man', 'scientist'), ('man', 'president'), ('woman', 'birth'), ('woman', 'doctor'), ('woman', 'nurse'), ('woman', 'teacher'), ('woman', 'professor'), ('woman', 'engineer'), ('woman', 'scientist'), ('woman', 'president'), ('birth', 'doctor'), ('birth', 'nurse'), ('birth', 'teacher'), ('birth', 'professor'), ('birth', 'engineer'), ('birth', 'scientist'), ('birth', 'president'), ('doctor', 'nurse'), ('doctor', 'teacher'), ('doctor', 'professor'), ('doctor', 'engineer'), ('doctor', 'scientist'), ('doctor', 'president'), ('nurse', 'teacher'), ('nurse', 'professor'), ('nurse', 'engineer'), ('nurse', 'scientist'), ('nurse', 'president'), ('teacher', 'professor'), ('teacher', 'engineer'), ('teacher', 'scientist'), ('teacher', 'president'), ('professor', 'engineer'), ('professor', 'scientist'), ('professor', 'president'), ('engineer', 'scientist'), ('engineer', 'president'), ('scientist', 'president')]\n"
          ]
        }
      ],
      "source": [
        "def make_pairs(words):\n",
        "    word_pairs = []\n",
        "    for first_word in range(len(words)):\n",
        "        for second_word in range(first_word + 1, len(words)):\n",
        "            word_pairs.append((words[first_word], words[second_word]))\n",
        "    return word_pairs\n",
        "\n",
        "print(make_pairs(words))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EKamywnxqxJJ"
      },
      "source": [
        "** 2. Complete the following analogies on your own (without using the model) **\n",
        "\n",
        "a. king is to throne as judge is to _\n",
        "\n",
        "b. giant is to dwarf as genius is to _\n",
        "\n",
        "c. French is to France as Spaniard is to _\n",
        "\n",
        "d. bad is to good as sad is to _\n",
        "\n",
        "e. nurse is to hospital as teacher is to _\n",
        "\n",
        "f. universe is to planet as house is to _"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VcNRxHuZrXAM"
      },
      "source": [
        "**3. Ahora completa las analogías usando un modelo word2vec**\n",
        "\n",
        "Aquí hay un ejemplo de cómo hacerlo. Puedes resolver analogías como \"A es a B como C es a _\" haciendo A + C - B. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "K4kF08h4qhxM"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('queen', 0.7118193507194519)]"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# man is to woman as king is to ___?\n",
        "model.most_similar(positive=[\"king\", \"woman\"], negative=[\"man\"], topn=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "DiPbbGsori48"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('taco', 0.6266060471534729)]"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# us is to burger as italy is to ___?\n",
        "model.most_similar(positive=[\"Mexico\", \"burger\"], negative=[\"USA\"], topn=1)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyOnghp0wIfCZSJ+lSjdKnPj",
      "collapsed_sections": [],
      "include_colab_link": true,
      "name": "Word2vec.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "tensorflow_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
