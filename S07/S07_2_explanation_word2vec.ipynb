{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XvNCXd9Dfqe1"
      },
      "source": [
        "# Word2vec with gensim\n",
        "\n",
        "In this Jupyter notebook you will use the [Gensim] library (https://radimrehurek.com/gensim/index.html) to experiment with Word2VEC.This notebook is focused on the intuition of the concepts and not on the implementation details.This notebook is inspired by this [Guide] (https://radicrehurek.com/gensim/auto_examples/ttorials/run_word2vec.html).\n",
        "\n",
        "## 1. Installation and loading the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "zKIqnDXXfpiz"
      },
      "outputs": [],
      "source": [
        "#!pip install --upgrade gensim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Sn7Q2jB3frOn"
      },
      "outputs": [],
      "source": [
        "import gensim.downloader as api"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "yBaT8djWkaZy"
      },
      "outputs": [],
      "source": [
        "model = api.load('word2vec-google-news-300')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nVZm7iTOoawW"
      },
      "source": [
        "## 2. Similarity of words\n",
        "\n",
        "In this section we will see how to achieve the similarity between two words using a Word Embedding already trained."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "kOZfaelLoi4c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.6510957"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.similarity(\"king\", \"queen\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "BX-Kk9HZofuF"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.22942671"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.similarity(\"king\", \"man\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "ypFK-pLrol3N"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.09978464"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.similarity(\"king\", \"potato\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "rBWzZySFormq"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.similarity(\"king\", \"king\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9GijWs_tx83W"
      },
      "source": [
        "Now we will see how to find the words with greater similarity to the set of specified words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "ytELAWBLk2-6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('monarch', 0.7042065858840942),\n",
              " ('kings', 0.6780862808227539),\n",
              " ('princess', 0.6731551885604858),\n",
              " ('queens', 0.6679496765136719),\n",
              " ('prince', 0.6435247659683228)]"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.most_similar([\"king\", \"queen\"], topn=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "7D4ZS7N3ovxB"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('carrots', 0.7536594867706299),\n",
              " ('tomatoes', 0.712963879108429),\n",
              " ('celery', 0.7025030851364136),\n",
              " ('broccoli', 0.6796349883079529),\n",
              " ('cherry_tomatoes', 0.662927508354187)]"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.most_similar([\"tomato\", \"carrot\"], topn=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WZFlxKjOyBpu"
      },
      "source": [
        "But you can even do interesting things such as seeing what word does not correspond to a list."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "8CrZdcBpn3pn"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'air'"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.doesnt_match([\"summer\", \"fall\", \"spring\", \"air\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ko09hZ3dqMZ1"
      },
      "source": [
        "## Exercises\n",
        "\n",
        "1. Use the Word2VEC model to make a ranking of the following 15 words according to its similarity with the words \"man\" and \"Woman\".For each pair, it prints its similarity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vzZ1eD3PpT-d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('wife', 'husband'), ('wife', 'child'), ('wife', 'queen'), ('wife', 'king'), ('wife', 'man'), ('wife', 'woman'), ('wife', 'birth'), ('wife', 'doctor'), ('wife', 'nurse'), ('wife', 'teacher'), ('wife', 'professor'), ('wife', 'engineer'), ('wife', 'scientist'), ('wife', 'president'), ('husband', 'child'), ('husband', 'queen'), ('husband', 'king'), ('husband', 'man'), ('husband', 'woman'), ('husband', 'birth'), ('husband', 'doctor'), ('husband', 'nurse'), ('husband', 'teacher'), ('husband', 'professor'), ('husband', 'engineer'), ('husband', 'scientist'), ('husband', 'president'), ('child', 'queen'), ('child', 'king'), ('child', 'man'), ('child', 'woman'), ('child', 'birth'), ('child', 'doctor'), ('child', 'nurse'), ('child', 'teacher'), ('child', 'professor'), ('child', 'engineer'), ('child', 'scientist'), ('child', 'president'), ('queen', 'king'), ('queen', 'man'), ('queen', 'woman'), ('queen', 'birth'), ('queen', 'doctor'), ('queen', 'nurse'), ('queen', 'teacher'), ('queen', 'professor'), ('queen', 'engineer'), ('queen', 'scientist'), ('queen', 'president'), ('king', 'man'), ('king', 'woman'), ('king', 'birth'), ('king', 'doctor'), ('king', 'nurse'), ('king', 'teacher'), ('king', 'professor'), ('king', 'engineer'), ('king', 'scientist'), ('king', 'president'), ('man', 'woman'), ('man', 'birth'), ('man', 'doctor'), ('man', 'nurse'), ('man', 'teacher'), ('man', 'professor'), ('man', 'engineer'), ('man', 'scientist'), ('man', 'president'), ('woman', 'birth'), ('woman', 'doctor'), ('woman', 'nurse'), ('woman', 'teacher'), ('woman', 'professor'), ('woman', 'engineer'), ('woman', 'scientist'), ('woman', 'president'), ('birth', 'doctor'), ('birth', 'nurse'), ('birth', 'teacher'), ('birth', 'professor'), ('birth', 'engineer'), ('birth', 'scientist'), ('birth', 'president'), ('doctor', 'nurse'), ('doctor', 'teacher'), ('doctor', 'professor'), ('doctor', 'engineer'), ('doctor', 'scientist'), ('doctor', 'president'), ('nurse', 'teacher'), ('nurse', 'professor'), ('nurse', 'engineer'), ('nurse', 'scientist'), ('nurse', 'president'), ('teacher', 'professor'), ('teacher', 'engineer'), ('teacher', 'scientist'), ('teacher', 'president'), ('professor', 'engineer'), ('professor', 'scientist'), ('professor', 'president'), ('engineer', 'scientist'), ('engineer', 'president'), ('scientist', 'president')]\n"
          ]
        }
      ],
      "source": [
        "words = [\n",
        "\"wife\",\n",
        "\"husband\",\n",
        "\"child\",\n",
        "\"queen\",\n",
        "\"king\",\n",
        "\"man\",\n",
        "\"woman\",\n",
        "\"birth\",\n",
        "\"doctor\",\n",
        "\"nurse\",\n",
        "\"teacher\",\n",
        "\"professor\",\n",
        "\"engineer\",\n",
        "\"scientist\",\n",
        "\"president\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Axuliar functions that I do not use anymore but that I want to keep"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('wife', 'husband'), ('wife', 'child'), ('wife', 'queen'), ('wife', 'king'), ('wife', 'man'), ('wife', 'woman'), ('wife', 'birth'), ('wife', 'doctor'), ('wife', 'nurse'), ('wife', 'teacher'), ('wife', 'professor'), ('wife', 'engineer'), ('wife', 'scientist'), ('wife', 'president'), ('husband', 'child'), ('husband', 'queen'), ('husband', 'king'), ('husband', 'man'), ('husband', 'woman'), ('husband', 'birth'), ('husband', 'doctor'), ('husband', 'nurse'), ('husband', 'teacher'), ('husband', 'professor'), ('husband', 'engineer'), ('husband', 'scientist'), ('husband', 'president'), ('child', 'queen'), ('child', 'king'), ('child', 'man'), ('child', 'woman'), ('child', 'birth'), ('child', 'doctor'), ('child', 'nurse'), ('child', 'teacher'), ('child', 'professor'), ('child', 'engineer'), ('child', 'scientist'), ('child', 'president'), ('queen', 'king'), ('queen', 'man'), ('queen', 'woman'), ('queen', 'birth'), ('queen', 'doctor'), ('queen', 'nurse'), ('queen', 'teacher'), ('queen', 'professor'), ('queen', 'engineer'), ('queen', 'scientist'), ('queen', 'president'), ('king', 'man'), ('king', 'woman'), ('king', 'birth'), ('king', 'doctor'), ('king', 'nurse'), ('king', 'teacher'), ('king', 'professor'), ('king', 'engineer'), ('king', 'scientist'), ('king', 'president'), ('man', 'woman'), ('man', 'birth'), ('man', 'doctor'), ('man', 'nurse'), ('man', 'teacher'), ('man', 'professor'), ('man', 'engineer'), ('man', 'scientist'), ('man', 'president'), ('woman', 'birth'), ('woman', 'doctor'), ('woman', 'nurse'), ('woman', 'teacher'), ('woman', 'professor'), ('woman', 'engineer'), ('woman', 'scientist'), ('woman', 'president'), ('birth', 'doctor'), ('birth', 'nurse'), ('birth', 'teacher'), ('birth', 'professor'), ('birth', 'engineer'), ('birth', 'scientist'), ('birth', 'president'), ('doctor', 'nurse'), ('doctor', 'teacher'), ('doctor', 'professor'), ('doctor', 'engineer'), ('doctor', 'scientist'), ('doctor', 'president'), ('nurse', 'teacher'), ('nurse', 'professor'), ('nurse', 'engineer'), ('nurse', 'scientist'), ('nurse', 'president'), ('teacher', 'professor'), ('teacher', 'engineer'), ('teacher', 'scientist'), ('teacher', 'president'), ('professor', 'engineer'), ('professor', 'scientist'), ('professor', 'president'), ('engineer', 'scientist'), ('engineer', 'president'), ('scientist', 'president')]\n"
          ]
        }
      ],
      "source": [
        "# # Making pairs from the words list\n",
        "# def make_pairs(words):\n",
        "#     word_pairs = []\n",
        "#     for first_word in range(len(words)):\n",
        "#         for second_word in range(first_word + 1, len(words)):\n",
        "#             word_pairs.append((words[first_word], words[second_word]))\n",
        "#     return word_pairs\n",
        "\n",
        "# print(make_pairs(words))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Similarity between wife and husband: 0.8294166922569275\n",
            "Similarity between wife and child: 0.3550868034362793\n",
            "Similarity between wife and queen: 0.20636820793151855\n",
            "Similarity between wife and king: 0.1500406712293625\n",
            "Similarity between wife and man: 0.3292091488838196\n",
            "Similarity between wife and woman: 0.4448239803314209\n",
            "Similarity between wife and birth: 0.2527046501636505\n",
            "Similarity between wife and doctor: 0.3103739619255066\n",
            "Similarity between wife and nurse: 0.3347511887550354\n",
            "Similarity between wife and teacher: 0.30123811960220337\n",
            "Similarity between wife and professor: 0.17416977882385254\n",
            "Similarity between wife and engineer: 0.15991957485675812\n",
            "Similarity between wife and scientist: 0.15480250120162964\n",
            "Similarity between wife and president: 0.16623905301094055\n",
            "Similarity between husband and child: 0.3832300305366516\n",
            "Similarity between husband and queen: 0.2445879429578781\n",
            "Similarity between husband and king: 0.12284289300441742\n",
            "Similarity between husband and man: 0.34499746561050415\n",
            "Similarity between husband and woman: 0.4928138256072998\n",
            "Similarity between husband and birth: 0.25081178545951843\n",
            "Similarity between husband and doctor: 0.3260897397994995\n",
            "Similarity between husband and nurse: 0.3321164846420288\n",
            "Similarity between husband and teacher: 0.276230126619339\n",
            "Similarity between husband and professor: 0.2528173327445984\n",
            "Similarity between husband and engineer: 0.23952431976795197\n",
            "Similarity between husband and scientist: 0.19585749506950378\n",
            "Similarity between husband and president: 0.22222386300563812\n",
            "Similarity between child and queen: 0.15617316961288452\n",
            "Similarity between child and king: 0.062251269817352295\n",
            "Similarity between child and man: 0.3163333833217621\n",
            "Similarity between child and woman: 0.475003719329834\n",
            "Similarity between child and birth: 0.4171780049800873\n",
            "Similarity between child and doctor: 0.3273963928222656\n",
            "Similarity between child and nurse: 0.3574445843696594\n",
            "Similarity between child and teacher: 0.4166046977043152\n",
            "Similarity between child and professor: 0.05856069549918175\n",
            "Similarity between child and engineer: 0.027358490973711014\n",
            "Similarity between child and scientist: 0.04166358709335327\n",
            "Similarity between child and president: -0.06373519450426102\n",
            "Similarity between queen and king: 0.6510956883430481\n",
            "Similarity between queen and man: 0.16658204793930054\n",
            "Similarity between queen and woman: 0.31618136167526245\n",
            "Similarity between queen and birth: 0.16190363466739655\n",
            "Similarity between queen and doctor: 0.2043769508600235\n",
            "Similarity between queen and nurse: 0.22716909646987915\n",
            "Similarity between queen and teacher: 0.167035311460495\n",
            "Similarity between queen and professor: 0.07806959748268127\n",
            "Similarity between queen and engineer: -0.039261870086193085\n",
            "Similarity between queen and scientist: 0.09491612762212753\n",
            "Similarity between queen and president: 0.17118436098098755\n",
            "Similarity between king and man: 0.22942671179771423\n",
            "Similarity between king and woman: 0.1284797340631485\n",
            "Similarity between king and birth: 0.11755013465881348\n",
            "Similarity between king and doctor: 0.19477394223213196\n",
            "Similarity between king and nurse: 0.026095394045114517\n",
            "Similarity between king and teacher: 0.11878164112567902\n",
            "Similarity between king and professor: 0.08975034207105637\n",
            "Similarity between king and engineer: 0.012417793273925781\n",
            "Similarity between king and scientist: 0.0370476171374321\n",
            "Similarity between king and president: 0.20158202946186066\n",
            "Similarity between man and woman: 0.7664012312889099\n",
            "Similarity between man and birth: 0.11078789830207825\n",
            "Similarity between man and doctor: 0.31448960304260254\n",
            "Similarity between man and nurse: 0.25472286343574524\n",
            "Similarity between man and teacher: 0.25000131130218506\n",
            "Similarity between man and professor: 0.09415861964225769\n",
            "Similarity between man and engineer: 0.15128928422927856\n",
            "Similarity between man and scientist: 0.1582496464252472\n",
            "Similarity between man and president: 0.028424618765711784\n",
            "Similarity between woman and birth: 0.2147129476070404\n",
            "Similarity between woman and doctor: 0.37945860624313354\n",
            "Similarity between woman and nurse: 0.44135597348213196\n",
            "Similarity between woman and teacher: 0.31357842683792114\n",
            "Similarity between woman and professor: 0.13077852129936218\n",
            "Similarity between woman and engineer: 0.09435376524925232\n",
            "Similarity between woman and scientist: 0.1548689752817154\n",
            "Similarity between woman and president: 0.06267671287059784\n",
            "Similarity between birth and doctor: 0.2566632032394409\n",
            "Similarity between birth and nurse: 0.20020928978919983\n",
            "Similarity between birth and teacher: 0.12560302019119263\n",
            "Similarity between birth and professor: 0.05203273519873619\n",
            "Similarity between birth and engineer: 0.055274706333875656\n",
            "Similarity between birth and scientist: 0.10486912727355957\n",
            "Similarity between birth and president: 0.011224426329135895\n",
            "Similarity between doctor and nurse: 0.6319522857666016\n",
            "Similarity between doctor and teacher: 0.3099597096443176\n",
            "Similarity between doctor and professor: 0.21336083114147186\n",
            "Similarity between doctor and engineer: 0.21366837620735168\n",
            "Similarity between doctor and scientist: 0.27712035179138184\n",
            "Similarity between doctor and president: 0.12481808662414551\n",
            "Similarity between nurse and teacher: 0.5016448497772217\n",
            "Similarity between nurse and professor: 0.21981285512447357\n",
            "Similarity between nurse and engineer: 0.24601878225803375\n",
            "Similarity between nurse and scientist: 0.26532018184661865\n",
            "Similarity between nurse and president: 0.09608262777328491\n",
            "Similarity between teacher and professor: 0.39003685116767883\n",
            "Similarity between teacher and engineer: 0.2732195258140564\n",
            "Similarity between teacher and scientist: 0.27760058641433716\n",
            "Similarity between teacher and president: 0.12480965256690979\n",
            "Similarity between professor and engineer: 0.37013930082321167\n",
            "Similarity between professor and scientist: 0.6541188955307007\n",
            "Similarity between professor and president: 0.31181517243385315\n",
            "Similarity between engineer and scientist: 0.5331616401672363\n",
            "Similarity between engineer and president: 0.23249566555023193\n",
            "Similarity between scientist and president: 0.2758055031299591\n",
            "0.2758055\n"
          ]
        }
      ],
      "source": [
        "# # Now I can calculate word similiratity \n",
        "\n",
        "# word_pairs = make_pairs(words)\n",
        "# def calculate_similarity(word_pairs):\n",
        "#     for word_1, word_2 in word_pairs:\n",
        "#         similarity = model.similarity(word_1, word_2)\n",
        "#         print(f\"Similarity between {word_1} and {word_2}: {similarity}\")\n",
        "\n",
        "#     return similarity\n",
        "\n",
        "# print(calculate_similarity(word_pairs))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EKamywnxqxJJ"
      },
      "source": [
        "** 2. Complete the following analogies on your own (without using the model) **\n",
        "\n",
        "a. king is to throne as judge is to _\n",
        "\n",
        "b. giant is to dwarf as genius is to _\n",
        "\n",
        "c. French is to France as Spaniard is to _\n",
        "\n",
        "d. bad is to good as sad is to _\n",
        "\n",
        "e. nurse is to hospital as teacher is to _\n",
        "\n",
        "f. universe is to planet as house is to _"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VcNRxHuZrXAM"
      },
      "source": [
        "**3. Ahora completa las analogías usando un modelo word2vec**\n",
        "\n",
        "Aquí hay un ejemplo de cómo hacerlo. Puedes resolver analogías como \"A es a B como C es a _\" haciendo A + C - B. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "K4kF08h4qhxM"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('queen', 0.7118193507194519)]"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# man is to woman as king is to ___?\n",
        "model.most_similar(positive=[\"king\", \"woman\"], negative=[\"man\"], topn=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "DiPbbGsori48"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('taco', 0.6266060471534729)]"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# us is to burger as italy is to ___?\n",
        "model.most_similar(positive=[\"Mexico\", \"burger\"], negative=[\"USA\"], topn=1)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyOnghp0wIfCZSJ+lSjdKnPj",
      "collapsed_sections": [],
      "include_colab_link": true,
      "name": "Word2vec.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "tensorflow_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
