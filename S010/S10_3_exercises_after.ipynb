{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hugging Face Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForTokenClassification, \n",
    "    Trainer, \n",
    "    TrainingArguments,\n",
    "    DataCollatorForTokenClassification,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "from datasets import Dataset as HFDataset\n",
    "from datasets import DatasetDict\n",
    "from sklearn.model_selection import train_test_split\n",
    "from seqeval.metrics import f1_score, precision_score, recall_score, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Constant Definition \n",
    "\n",
    "In this cell, IÂ´ll document the type of entities and their correspondant colors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Types of defined entities:\n",
      "  - ACTION: Direct commands or actions mentioned in the message\n",
      "  - SITUATION: Racing context or circumstance descriptions\n",
      "  - INCIDENT: Accidents or on-track events\n",
      "  - STRATEGY_INSTRUCTION: Strategic directives\n",
      "  - POSITION_CHANGE: References to overtakes or positions\n",
      "  - PIT_CALL: Specific calls for pit stops\n",
      "  - TRACK_CONDITION: Mentions of the track's state\n",
      "  - TECHNICAL_ISSUE: Mechanical or car-related problems\n",
      "  - WEATHER: References to weather conditions\n"
     ]
    }
   ],
   "source": [
    "# Define entity types and their descriptions\n",
    "ENTITY_TYPES = {\n",
    "    \"ACTION\": \"Direct commands or actions mentioned in the message\",\n",
    "    \"SITUATION\": \"Racing context or circumstance descriptions\",\n",
    "    \"INCIDENT\": \"Accidents or on-track events\",\n",
    "    \"STRATEGY_INSTRUCTION\": \"Strategic directives\",\n",
    "    \"POSITION_CHANGE\": \"References to overtakes or positions\",\n",
    "    \"PIT_CALL\": \"Specific calls for pit stops\",\n",
    "    \"TRACK_CONDITION\": \"Mentions of the track's state\",\n",
    "    \"TECHNICAL_ISSUE\": \"Mechanical or car-related problems\",\n",
    "    \"WEATHER\": \"References to weather conditions\"\n",
    "}\n",
    "\n",
    "# Color scheme for entity visualization\n",
    "ENTITY_COLORS = {\n",
    "    \"ACTION\": \"#4e79a7\",           # Blue\n",
    "    \"SITUATION\": \"#f28e2c\",         # Orange\n",
    "    \"INCIDENT\": \"#e15759\",          # Red\n",
    "    \"STRATEGY_INSTRUCTION\": \"#76b7b2\", # Teal\n",
    "    \"POSITION_CHANGE\": \"#59a14f\",   # Green\n",
    "    \"PIT_CALL\": \"#edc949\",          # Yellow\n",
    "    \"TRACK_CONDITION\": \"#af7aa1\",   # Purple\n",
    "    \"TECHNICAL_ISSUE\": \"#ff9da7\",   # Pink\n",
    "    \"WEATHER\": \"#9c755f\"            # Brown\n",
    "}\n",
    "\n",
    "print(\"Entity types defined:\")\n",
    "for entity, description in ENTITY_TYPES.items():\n",
    "    print(f\"  - {entity}: {description}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load and Explore Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load F1 radio data from JSON file\n",
    "def load_f1_radio_data(json_file):\n",
    "    \"\"\"Load and explore F1 radio data from JSON file\"\"\"\n",
    "    with open(json_file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    print(f\"Loaded {len(data)} messages from {json_file}\")\n",
    "    \n",
    "    # Show sample structure\n",
    "    if len(data) > 0:\n",
    "        print(\"\\nSample record structure:\")\n",
    "        sample = data[0]\n",
    "        print(f\"  Driver: {sample.get('driver', 'N/A')}\")\n",
    "        print(f\"  Radio message: {sample.get('radio_message', 'N/A')[:100]}...\")\n",
    "        \n",
    "        if 'annotations' in sample and len(sample['annotations']) > 1:\n",
    "            if isinstance(sample['annotations'][1], dict) and 'entities' in sample['annotations'][1]:\n",
    "                entities = sample['annotations'][1]['entities']\n",
    "                print(f\"  Number of entities: {len(entities)}\")\n",
    "                if len(entities) > 0:\n",
    "                    entity = entities[0]\n",
    "                    entity_text = sample['radio_message'][entity[0]:entity[1]]\n",
    "                    print(f\"  Sample entity: [{entity[0]}, {entity[1]}, '{entity_text}', '{entity[2]}']\")\n",
    "    \n",
    "    return data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 529 messages from f1_radio_entity_annotations.json\n",
      "\n",
      "Sample record structure:\n",
      "  Driver: 1\n",
      "  Radio message: So don't forget Max, use your head please. Are we both doing it or what? You just follow my instruct...\n",
      "  Number of entities: 3\n",
      "  Sample entity: [82, 103, 'follow my instruction', 'ACTION']\n",
      "\n",
      "Entity type distribution in dataset:\n",
      "  - SITUATION: 255\n",
      "  - ACTION: 165\n",
      "  - STRATEGY_INSTRUCTION: 137\n",
      "  - TECHNICAL_ISSUE: 137\n",
      "  - WEATHER: 112\n",
      "  - POSITION_CHANGE: 83\n",
      "  - INCIDENT: 78\n",
      "  - TRACK_CONDITION: 62\n",
      "  - PIT_CALL: 42\n"
     ]
    }
   ],
   "source": [
    "# Load the JSON data\n",
    "json_file_path = \"f1_radio_entity_annotations.json\"\n",
    "f1_data = load_f1_radio_data(json_file_path)\n",
    "\n",
    "# Count entity types in the dataset\n",
    "entity_counts = {}\n",
    "for item in f1_data:\n",
    "    if 'annotations' in item and len(item['annotations']) > 1:\n",
    "        if isinstance(item['annotations'][1], dict) and 'entities' in item['annotations'][1]:\n",
    "            for _, _, entity_type in item['annotations'][1]['entities']:\n",
    "                entity_counts[entity_type] = entity_counts.get(entity_type, 0) + 1\n",
    "\n",
    "print(\"\\nEntity type distribution in dataset:\")\n",
    "for entity_type, count in sorted(entity_counts.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"  - {entity_type}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Preprocessing F1 Radio Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_f1_data(data):\n",
    "    \"\"\"Extract and preprocess F1 radio data with valid annotations\"\"\"\n",
    "    processed_data = []\n",
    "    skipped_count = 0\n",
    "    \n",
    "    for item in data:\n",
    "        if 'radio_message' not in item or 'annotations' not in item:\n",
    "            skipped_count += 1\n",
    "            continue\n",
    "            \n",
    "        text = item['radio_message']\n",
    "        \n",
    "        # Skip items with empty or null text\n",
    "        if not text or text.strip() == \"\":\n",
    "            skipped_count += 1\n",
    "            continue\n",
    "            \n",
    "        # Extract entities if they exist in expected format\n",
    "        if len(item['annotations']) > 1 and isinstance(item['annotations'][1], dict):\n",
    "            annotations = item['annotations'][1]\n",
    "            if 'entities' in annotations and annotations['entities']:\n",
    "                entities = annotations['entities']\n",
    "                \n",
    "                # Add to processed data\n",
    "                processed_data.append({\n",
    "                    'text': text,\n",
    "                    'entities': entities,\n",
    "                    'driver': item.get('driver', None)\n",
    "                })\n",
    "            else:\n",
    "                skipped_count += 1\n",
    "        else:\n",
    "            skipped_count += 1\n",
    "    \n",
    "    print(f\"Processed {len(processed_data)} messages with valid annotations\")\n",
    "    print(f\"Skipped {skipped_count} messages with missing or invalid annotations\")\n",
    "    \n",
    "    # Show a sample of processed data\n",
    "    if processed_data:\n",
    "        sample = processed_data[10]\n",
    "        print(\"\\nSample processed message:\")\n",
    "        print(f\"Text: {sample['text']}\")\n",
    "        print(\"Entities:\")\n",
    "        for start, end, entity_type in sample['entities']:\n",
    "            entity_text = sample['text'][start:end]\n",
    "            print(f\"  - [{start}, {end}] '{entity_text}' ({entity_type})\")\n",
    "    \n",
    "    return processed_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 399 messages with valid annotations\n",
      "Skipped 130 messages with missing or invalid annotations\n",
      "\n",
      "Sample processed message:\n",
      "Text: Max, we've currently got yellows in turn 7. Ferrari in the wall, no? Yes, that's Charles stopped. We are expecting the potential of an aborted start, but just keep to your protocol at the moment.\n",
      "Entities:\n",
      "  - [159, 194] 'keep to your protocol at the moment' (ACTION)\n",
      "  - [5, 42] 'we've currently got yellows in turn 7' (SITUATION)\n",
      "  - [98, 148] 'We are expecting the potential of an aborted start' (SITUATION)\n",
      "  - [44, 63] 'Ferrari in the wall' (INCIDENT)\n",
      "  - [74, 96] 'that's Charles stopped' (INCIDENT)\n"
     ]
    }
   ],
   "source": [
    "# Preprocess the loaded data\n",
    "processed_f1_data = preprocess_f1_data(f1_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Covert to BIO tagging format\n",
    "\n",
    "Deeper BIO tagging format information can be searched [here](https://en.wikipedia.org/wiki/Insideâoutsideâbeginning_(tagging)).\n",
    "\n",
    "### BIO Format Explanation\n",
    "\n",
    "The **BIO format** is a way to label words in a sentence to indicate if they are part of a named entity, and if so, where in the entity they belong. It uses three types of labels:\n",
    "\n",
    "- **B- (Beginning)**: The first word in an entity.\n",
    "- **I- (Inside)**: Any word inside the entity that isn't the first one.\n",
    "- **O (Outside)**: Words that are not part of any entity.\n",
    "\n",
    "---\n",
    "\n",
    "### Example Radio\n",
    "\n",
    "Here is an example of a radio message from Max VerstappenÂ´s track engineer: \n",
    "\n",
    "**Text:**  \n",
    "*\"Max, we've currently got yellows in turn 7. Ferrari in the wall, no? Yes, that's Charles stopped. We are expecting the potential of an aborted start, but just keep to your protocol at the moment.\"*\n",
    "\n",
    "Here are the entities mentioned in the message:\n",
    "\n",
    "1. **'keep to your protocol at the moment'** (ACTION)\n",
    "2. **'we've currently got yellows in turn 7'** (SITUATION)\n",
    "3. **'We are expecting the potential of an aborted start'** (SITUATION)\n",
    "4. **'Ferrari in the wall'** (INCIDENT)\n",
    "5. **'that's Charles stopped'** (INCIDENT)\n",
    "\n",
    "---\n",
    "\n",
    "### Breaking the Sentence\n",
    "\n",
    "We break the sentence into words and then tag them as follows:\n",
    "\n",
    "| Word            | BIO Tag          |\n",
    "|-----------------|------------------|\n",
    "| Max,            | O                |\n",
    "| we've           | O                |\n",
    "| currently       | O                |\n",
    "| got             | O                |\n",
    "| yellows         | O                |\n",
    "| in              | O                |\n",
    "| turn            | O                |\n",
    "| 7.              | O                |\n",
    "| Ferrari         | B-INCIDENT       |\n",
    "| in              | I-INCIDENT       |\n",
    "| the             | I-INCIDENT       |\n",
    "| wall,           | I-INCIDENT       |\n",
    "| no?             | O                |\n",
    "| Yes,            | O                |\n",
    "| that's          | B-INCIDENT       |\n",
    "| Charles         | I-INCIDENT       |\n",
    "| stopped.        | I-INCIDENT       |\n",
    "| We              | B-SITUATION      |\n",
    "| are             | I-SITUATION      |\n",
    "| expecting       | I-SITUATION      |\n",
    "| the             | I-SITUATION      |\n",
    "| potential       | I-SITUATION      |\n",
    "| of              | I-SITUATION      |\n",
    "| an              | I-SITUATION      |\n",
    "| aborted         | I-SITUATION      |\n",
    "| start,          | I-SITUATION      |\n",
    "| but             | O                |\n",
    "| just            | O                |\n",
    "| keep            | B-ACTION         |\n",
    "| to              | I-ACTION         |\n",
    "| your            | I-ACTION         |\n",
    "| protocol        | I-ACTION         |\n",
    "| at              | I-ACTION         |\n",
    "| the             | I-ACTION         |\n",
    "| moment.         | I-ACTION         |\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ner_tags(text, entities):\n",
    "    \"\"\"Convert character-based entity spans to token-based BIO tags\"\"\"\n",
    "    words = text.split()\n",
    "    tags = [\"O\"] * len(words)\n",
    "    char_to_word = {}\n",
    "    \n",
    "    # Create mapping from character positions to word indices\n",
    "    char_idx = 0\n",
    "    for word_idx, word in enumerate(words):\n",
    "        # Account for spaces\n",
    "        if char_idx > 0:\n",
    "            char_idx += 1  # Space\n",
    "        \n",
    "        # Map each character position to its word index\n",
    "        for char_pos in range(char_idx, char_idx + len(word)):\n",
    "            char_to_word[char_pos] = word_idx\n",
    "        \n",
    "        char_idx += len(word)\n",
    "    \n",
    "    # Apply entity tags\n",
    "    for start_char, end_char, entity_type in entities:\n",
    "        # Skip invalid spans\n",
    "        if start_char >= len(text) or end_char > len(text) or start_char >= end_char:\n",
    "            continue\n",
    "            \n",
    "        # Find word indices for start and end characters\n",
    "        if start_char in char_to_word:\n",
    "            start_word = char_to_word[start_char]\n",
    "            # Find the last word of the entity\n",
    "            end_word = char_to_word.get(end_char - 1, start_word)\n",
    "            \n",
    "            # Tag the first word as B-entity\n",
    "            tags[start_word] = f\"B-{entity_type}\"\n",
    "            \n",
    "            # Tag subsequent words as I-entity\n",
    "            for word_idx in range(start_word + 1, end_word + 1):\n",
    "                tags[word_idx] = f\"I-{entity_type}\"\n",
    "    \n",
    "    return words, tags\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_bio_format(processed_data):\n",
    "    \"\"\"Convert processed data to BIO tagging format\"\"\"\n",
    "    bio_data = []\n",
    "    mapping_errors = 0\n",
    "    \n",
    "    for item in processed_data:\n",
    "        text = item['text']\n",
    "        entities = item['entities']\n",
    "        \n",
    "        # Convert to BIO tags\n",
    "        words, tags = create_ner_tags(text, entities)\n",
    "        \n",
    "        # Check if we mapped any entities\n",
    "        if all(tag == \"O\" for tag in tags) and len(entities) > 0:\n",
    "            mapping_errors += 1\n",
    "        \n",
    "        bio_data.append({\n",
    "            \"tokens\": words,\n",
    "            \"ner_tags\": tags,\n",
    "            \"driver\": item.get('driver', None)\n",
    "        })\n",
    "    \n",
    "    print(f\"Converted {len(bio_data)} messages to BIO format\")\n",
    "    print(f\"Mapping errors: {mapping_errors} (messages where no entities were mapped)\")\n",
    "    \n",
    "    # Show an example\n",
    "    if bio_data:\n",
    "        sample = bio_data[10]\n",
    "        print(\"\\nSample BIO tagging:\")\n",
    "        print(f\"Original text: {' '.join(sample['tokens'])}\")\n",
    "        for token, tag in zip(sample['tokens'], sample['ner_tags']):\n",
    "            print(f\"  {token} -> {tag}\")\n",
    "    \n",
    "    return bio_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 399 messages to BIO format\n",
      "Mapping errors: 0 (messages where no entities were mapped)\n",
      "\n",
      "Sample BIO tagging:\n",
      "Original text: Max, we've currently got yellows in turn 7. Ferrari in the wall, no? Yes, that's Charles stopped. We are expecting the potential of an aborted start, but just keep to your protocol at the moment.\n",
      "  Max, -> O\n",
      "  we've -> B-SITUATION\n",
      "  currently -> I-SITUATION\n",
      "  got -> I-SITUATION\n",
      "  yellows -> I-SITUATION\n",
      "  in -> I-SITUATION\n",
      "  turn -> I-SITUATION\n",
      "  7. -> I-SITUATION\n",
      "  Ferrari -> B-INCIDENT\n",
      "  in -> I-INCIDENT\n",
      "  the -> I-INCIDENT\n",
      "  wall, -> I-INCIDENT\n",
      "  no? -> O\n",
      "  Yes, -> O\n",
      "  that's -> B-INCIDENT\n",
      "  Charles -> I-INCIDENT\n",
      "  stopped. -> I-INCIDENT\n",
      "  We -> B-SITUATION\n",
      "  are -> I-SITUATION\n",
      "  expecting -> I-SITUATION\n",
      "  the -> I-SITUATION\n",
      "  potential -> I-SITUATION\n",
      "  of -> I-SITUATION\n",
      "  an -> I-SITUATION\n",
      "  aborted -> I-SITUATION\n",
      "  start, -> I-SITUATION\n",
      "  but -> O\n",
      "  just -> O\n",
      "  keep -> B-ACTION\n",
      "  to -> I-ACTION\n",
      "  your -> I-ACTION\n",
      "  protocol -> I-ACTION\n",
      "  at -> I-ACTION\n",
      "  the -> I-ACTION\n",
      "  moment. -> I-ACTION\n"
     ]
    }
   ],
   "source": [
    "# Convert processed data to BIO format\n",
    "bio_data = convert_to_bio_format(processed_f1_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What the Function Does\n",
    "\n",
    "The function `create_ner_tags` takes the text and entities and converts them into BIO format. It starts by splitting the text into words. \n",
    "\n",
    "Then, it maps each word to a tag: \"O\" for words that are not part of an entity, \"B-\" for the first word of an entity, and \"I-\" for subsequent words inside the entity. \n",
    "\n",
    "The function also uses the character positions of the entities to determine which words they correspond to. Once the tags are assigned, the function returns the words and their BIO tags, ready for use in training a Named Entity Recognition (NER) model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Create tag mappings and prepare datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 `create_tag_mappings`\n",
    "\n",
    "This function creates mappings between NER (Named Entity Recognition) tags and unique IDs. It does this by:\n",
    "\n",
    "1. Collecting all unique NER tags from the `bio_data`.\n",
    "2. Sorting and assigning each unique tag an ID.\n",
    "3. Creating two mappings:\n",
    "   - `tag2id`: Maps each tag to its corresponding ID.\n",
    "   - `id2tag`: Maps each ID back to its corresponding tag.\n",
    "\n",
    "It then prints out the mappings and returns the two dictionaries: `tag2id` and `id2tag`.\n",
    "\n",
    "**What it does:**\n",
    "- Converts NER tags into unique IDs for easier processing in machine learning models.\n",
    "- Helps with transforming the tags when working with model inputs and outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tag_mappings(bio_data):\n",
    "    \"\"\"Create mappings between NER tags and IDs\"\"\"\n",
    "    unique_tags = set()\n",
    "    for item in bio_data:\n",
    "        unique_tags.update(item[\"ner_tags\"])\n",
    "    \n",
    "    tag2id = {tag: id for id, tag in enumerate(sorted(list(unique_tags)))}\n",
    "    id2tag = {id: tag for tag, id in tag2id.items()}\n",
    "    \n",
    "    print(f\"Created mappings for {len(tag2id)} unique tags:\")\n",
    "    for tag, idx in tag2id.items():\n",
    "        print(f\"  {tag}: {idx}\")\n",
    "    \n",
    "    return tag2id, id2tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created mappings for 19 unique tags:\n",
      "  B-ACTION: 0\n",
      "  B-INCIDENT: 1\n",
      "  B-PIT_CALL: 2\n",
      "  B-POSITION_CHANGE: 3\n",
      "  B-SITUATION: 4\n",
      "  B-STRATEGY_INSTRUCTION: 5\n",
      "  B-TECHNICAL_ISSUE: 6\n",
      "  B-TRACK_CONDITION: 7\n",
      "  B-WEATHER: 8\n",
      "  I-ACTION: 9\n",
      "  I-INCIDENT: 10\n",
      "  I-PIT_CALL: 11\n",
      "  I-POSITION_CHANGE: 12\n",
      "  I-SITUATION: 13\n",
      "  I-STRATEGY_INSTRUCTION: 14\n",
      "  I-TECHNICAL_ISSUE: 15\n",
      "  I-TRACK_CONDITION: 16\n",
      "  I-WEATHER: 17\n",
      "  O: 18\n"
     ]
    }
   ],
   "source": [
    "# Create tag mappings\n",
    "tag2id, id2tag = create_tag_mappings(bio_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 `prepare_datasets`\n",
    "\n",
    "This function prepares the dataset for training a model by splitting it into training, validation, and test sets using the Hugging Face library. Here's what it does:\n",
    "\n",
    "1. Converts the input `bio_data` into a Hugging Face `Dataset`.\n",
    "2. Splits the data into two parts: training + validation, and test.\n",
    "3. Further splits the training data into training and validation sets based on the specified sizes (`test_size` and `val_size`).\n",
    "4. Returns a `DatasetDict` containing the `train`, `validation`, and `test` sets.\n",
    "\n",
    "**What it does:**\n",
    "- Converts the data into a format suitable for machine learning.\n",
    "- Splits the data into three parts: training, validation, and test sets for model evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_datasets(bio_data, test_size=0.1, val_size=0.1, seed=42):\n",
    "    \"\"\"Convert to Hugging Face Dataset and split into train/val/test\"\"\"\n",
    "    # Convert to Hugging Face dataset\n",
    "    hf_dataset = HFDataset.from_list(bio_data)\n",
    "    \n",
    "    # First split: train + validation vs test\n",
    "    train_val_test = hf_dataset.train_test_split(test_size=test_size, seed=seed)\n",
    "    \n",
    "    # Second split: train vs validation (validation is val_size/(1-test_size) of the train set)\n",
    "    val_fraction = val_size / (1 - test_size)\n",
    "    train_val = train_val_test[\"train\"].train_test_split(test_size=val_fraction, seed=seed)\n",
    "    \n",
    "    # Combine into DatasetDict\n",
    "    datasets = DatasetDict({\n",
    "        \"train\": train_val[\"train\"],\n",
    "        \"validation\": train_val[\"test\"],\n",
    "        \"test\": train_val_test[\"test\"]\n",
    "    })\n",
    "    \n",
    "    print(f\"Prepared datasets with:\")\n",
    "    print(f\"  - Train: {len(datasets['train'])} examples\")\n",
    "    print(f\"  - Validation: {len(datasets['validation'])} examples\")\n",
    "    print(f\"  - Test: {len(datasets['test'])} examples\")\n",
    "    \n",
    "    return datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared datasets with:\n",
      "  - Train: 319 examples\n",
      "  - Validation: 40 examples\n",
      "  - Test: 40 examples\n"
     ]
    }
   ],
   "source": [
    "datasets = prepare_datasets(bio_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Tokenize and Align Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples, tokenizer, tag2id, max_length=128):\n",
    "    \"\"\"Tokenize text and align labels with model's tokens\"\"\"\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"tokens\"],\n",
    "        truncation=True,\n",
    "        is_split_into_words=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=max_length\n",
    "    )\n",
    "\n",
    "    labels = []\n",
    "    \n",
    "    for i, label in enumerate(examples[\"ner_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        \n",
    "        for word_idx in word_ids:\n",
    "            # Special tokens\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            # First token of a word\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(tag2id[label[word_idx]])\n",
    "            # Subsequent tokens of a word\n",
    "            else:\n",
    "                label_ids.append(-100)\n",
    "                \n",
    "            previous_word_idx = word_idx\n",
    "        \n",
    "        labels.append(label_ids)\n",
    "    \n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60c40454022b4222bed839c1c2369f6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03dda37958d04f6c95205d1d16c429cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/579 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cb9af58b9d040f4b0d1ec91aac51cdc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spm.model:   0%|          | 0.00/2.46M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ImportError",
     "evalue": "\n requires the protobuf library but it was not found in your environment. Checkout the instructions on the\ninstallation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones\nthat match your environment. Please note that you may need to restart your runtime after installation.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\victo\\miniconda3\\envs\\f1_strat_manager\\lib\\site-packages\\transformers\\convert_slow_tokenizer.py:1587\u001b[0m, in \u001b[0;36mTikTokenConverter.extract_vocab_merges_from_model\u001b[1;34m(self, tiktoken_url)\u001b[0m\n\u001b[0;32m   1586\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1587\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtiktoken\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mload\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_tiktoken_bpe\n\u001b[0;32m   1588\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tiktoken'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\victo\\miniconda3\\envs\\f1_strat_manager\\lib\\site-packages\\transformers\\convert_slow_tokenizer.py:1730\u001b[0m, in \u001b[0;36mconvert_slow_tokenizer\u001b[1;34m(transformer_tokenizer, from_tiktoken)\u001b[0m\n\u001b[0;32m   1726\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConverting from Tiktoken\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1727\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mTikTokenConverter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1728\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvocab_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransformer_tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvocab_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1729\u001b[0m \u001b[43m        \u001b[49m\u001b[43madditional_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransformer_tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madditional_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m-> 1730\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverted\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1731\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\victo\\miniconda3\\envs\\f1_strat_manager\\lib\\site-packages\\transformers\\convert_slow_tokenizer.py:1624\u001b[0m, in \u001b[0;36mTikTokenConverter.converted\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1623\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mconverted\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tokenizer:\n\u001b[1;32m-> 1624\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1625\u001b[0m     tokenizer\u001b[38;5;241m.\u001b[39mpre_tokenizer \u001b[38;5;241m=\u001b[39m pre_tokenizers\u001b[38;5;241m.\u001b[39mSequence(\n\u001b[0;32m   1626\u001b[0m         [\n\u001b[0;32m   1627\u001b[0m             pre_tokenizers\u001b[38;5;241m.\u001b[39mSplit(Regex(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpattern), behavior\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124misolated\u001b[39m\u001b[38;5;124m\"\u001b[39m, invert\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[0;32m   1628\u001b[0m             pre_tokenizers\u001b[38;5;241m.\u001b[39mByteLevel(add_prefix_space\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_prefix_space, use_regex\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[0;32m   1629\u001b[0m         ]\n\u001b[0;32m   1630\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\victo\\miniconda3\\envs\\f1_strat_manager\\lib\\site-packages\\transformers\\convert_slow_tokenizer.py:1617\u001b[0m, in \u001b[0;36mTikTokenConverter.tokenizer\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1616\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mtokenizer\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m-> 1617\u001b[0m     vocab_scores, merges \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_vocab_merges_from_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvocab_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1618\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m Tokenizer(BPE(vocab_scores, merges, fuse_unk\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m))\n",
      "File \u001b[1;32mc:\\Users\\victo\\miniconda3\\envs\\f1_strat_manager\\lib\\site-packages\\transformers\\convert_slow_tokenizer.py:1589\u001b[0m, in \u001b[0;36mTikTokenConverter.extract_vocab_merges_from_model\u001b[1;34m(self, tiktoken_url)\u001b[0m\n\u001b[0;32m   1588\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m-> 1589\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1590\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`tiktoken` is required to read a `tiktoken` file. Install it with \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`pip install tiktoken`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1591\u001b[0m     )\n\u001b[0;32m   1593\u001b[0m bpe_ranks \u001b[38;5;241m=\u001b[39m load_tiktoken_bpe(tiktoken_url)\n",
      "\u001b[1;31mValueError\u001b[0m: `tiktoken` is required to read a `tiktoken` file. Install it with `pip install tiktoken`.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\victo\\miniconda3\\envs\\f1_strat_manager\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2302\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._from_pretrained\u001b[1;34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, token, cache_dir, local_files_only, _commit_hash, _is_local, trust_remote_code, *init_inputs, **kwargs)\u001b[0m\n\u001b[0;32m   2301\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 2302\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m(\u001b[38;5;241m*\u001b[39minit_inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minit_kwargs)\n\u001b[0;32m   2303\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m import_protobuf_decode_error():\n",
      "File \u001b[1;32mc:\\Users\\victo\\miniconda3\\envs\\f1_strat_manager\\lib\\site-packages\\transformers\\models\\deberta_v2\\tokenization_deberta_v2_fast.py:103\u001b[0m, in \u001b[0;36mDebertaV2TokenizerFast.__init__\u001b[1;34m(self, vocab_file, tokenizer_file, do_lower_case, split_by_punct, bos_token, eos_token, unk_token, sep_token, pad_token, cls_token, mask_token, **kwargs)\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m     89\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m     90\u001b[0m     vocab_file\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    101\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    102\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 103\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m    104\u001b[0m         vocab_file,\n\u001b[0;32m    105\u001b[0m         tokenizer_file\u001b[38;5;241m=\u001b[39mtokenizer_file,\n\u001b[0;32m    106\u001b[0m         do_lower_case\u001b[38;5;241m=\u001b[39mdo_lower_case,\n\u001b[0;32m    107\u001b[0m         bos_token\u001b[38;5;241m=\u001b[39mbos_token,\n\u001b[0;32m    108\u001b[0m         eos_token\u001b[38;5;241m=\u001b[39meos_token,\n\u001b[0;32m    109\u001b[0m         unk_token\u001b[38;5;241m=\u001b[39munk_token,\n\u001b[0;32m    110\u001b[0m         sep_token\u001b[38;5;241m=\u001b[39msep_token,\n\u001b[0;32m    111\u001b[0m         pad_token\u001b[38;5;241m=\u001b[39mpad_token,\n\u001b[0;32m    112\u001b[0m         cls_token\u001b[38;5;241m=\u001b[39mcls_token,\n\u001b[0;32m    113\u001b[0m         mask_token\u001b[38;5;241m=\u001b[39mmask_token,\n\u001b[0;32m    114\u001b[0m         split_by_punct\u001b[38;5;241m=\u001b[39msplit_by_punct,\n\u001b[0;32m    115\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    116\u001b[0m     )\n\u001b[0;32m    118\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdo_lower_case \u001b[38;5;241m=\u001b[39m do_lower_case\n",
      "File \u001b[1;32mc:\\Users\\victo\\miniconda3\\envs\\f1_strat_manager\\lib\\site-packages\\transformers\\tokenization_utils_fast.py:139\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast.__init__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madditional_special_tokens \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madditional_special_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m, [])\n\u001b[1;32m--> 139\u001b[0m fast_tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_slow_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_tiktoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    140\u001b[0m slow_tokenizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\victo\\miniconda3\\envs\\f1_strat_manager\\lib\\site-packages\\transformers\\convert_slow_tokenizer.py:1732\u001b[0m, in \u001b[0;36mconvert_slow_tokenizer\u001b[1;34m(transformer_tokenizer, from_tiktoken)\u001b[0m\n\u001b[0;32m   1731\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m-> 1732\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1733\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConverting from Tiktoken failed, if a converter for SentencePiece is available, provide a model path \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1734\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwith a SentencePiece tokenizer.model file.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1735\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCurrently available slow->fast converters: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(SLOW_TO_FAST_CONVERTERS\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1736\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Converting from Tiktoken failed, if a converter for SentencePiece is available, provide a model path with a SentencePiece tokenizer.model file.Currently available slow->fast converters: ['AlbertTokenizer', 'BartTokenizer', 'BarthezTokenizer', 'BertTokenizer', 'BigBirdTokenizer', 'BlenderbotTokenizer', 'CamembertTokenizer', 'CLIPTokenizer', 'CodeGenTokenizer', 'ConvBertTokenizer', 'DebertaTokenizer', 'DebertaV2Tokenizer', 'DistilBertTokenizer', 'DPRReaderTokenizer', 'DPRQuestionEncoderTokenizer', 'DPRContextEncoderTokenizer', 'ElectraTokenizer', 'FNetTokenizer', 'FunnelTokenizer', 'GPT2Tokenizer', 'HerbertTokenizer', 'LayoutLMTokenizer', 'LayoutLMv2Tokenizer', 'LayoutLMv3Tokenizer', 'LayoutXLMTokenizer', 'LongformerTokenizer', 'LEDTokenizer', 'LxmertTokenizer', 'MarkupLMTokenizer', 'MBartTokenizer', 'MBart50Tokenizer', 'MPNetTokenizer', 'MobileBertTokenizer', 'MvpTokenizer', 'NllbTokenizer', 'OpenAIGPTTokenizer', 'PegasusTokenizer', 'Qwen2Tokenizer', 'RealmTokenizer', 'ReformerTokenizer', 'RemBertTokenizer', 'RetriBertTokenizer', 'RobertaTokenizer', 'RoFormerTokenizer', 'SeamlessM4TTokenizer', 'SqueezeBertTokenizer', 'T5Tokenizer', 'UdopTokenizer', 'WhisperTokenizer', 'XLMRobertaTokenizer', 'XLNetTokenizer', 'SplinterTokenizer', 'XGLMTokenizer', 'LlamaTokenizer', 'CodeLlamaTokenizer', 'GemmaTokenizer', 'Phi3Tokenizer']",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[39], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Initialize tokenizer (smaller model for demonstration purposes)\u001b[39;00m\n\u001b[0;32m      2\u001b[0m model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmicrosoft/deberta-v3-base\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 3\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mAutoTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\victo\\miniconda3\\envs\\f1_strat_manager\\lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:992\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[0;32m    989\u001b[0m tokenizer_class_py, tokenizer_class_fast \u001b[38;5;241m=\u001b[39m TOKENIZER_MAPPING[\u001b[38;5;28mtype\u001b[39m(config)]\n\u001b[0;32m    991\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tokenizer_class_fast \u001b[38;5;129;01mand\u001b[39;00m (use_fast \u001b[38;5;129;01mor\u001b[39;00m tokenizer_class_py \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m--> 992\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer_class_fast\u001b[38;5;241m.\u001b[39mfrom_pretrained(pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    993\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    994\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tokenizer_class_py \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\victo\\miniconda3\\envs\\f1_strat_manager\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2062\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, trust_remote_code, *init_inputs, **kwargs)\u001b[0m\n\u001b[0;32m   2059\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2060\u001b[0m         logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloading file \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m from cache at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresolved_vocab_files[file_id]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 2062\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_from_pretrained(\n\u001b[0;32m   2063\u001b[0m     resolved_vocab_files,\n\u001b[0;32m   2064\u001b[0m     pretrained_model_name_or_path,\n\u001b[0;32m   2065\u001b[0m     init_configuration,\n\u001b[0;32m   2066\u001b[0m     \u001b[38;5;241m*\u001b[39minit_inputs,\n\u001b[0;32m   2067\u001b[0m     token\u001b[38;5;241m=\u001b[39mtoken,\n\u001b[0;32m   2068\u001b[0m     cache_dir\u001b[38;5;241m=\u001b[39mcache_dir,\n\u001b[0;32m   2069\u001b[0m     local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[0;32m   2070\u001b[0m     _commit_hash\u001b[38;5;241m=\u001b[39mcommit_hash,\n\u001b[0;32m   2071\u001b[0m     _is_local\u001b[38;5;241m=\u001b[39mis_local,\n\u001b[0;32m   2072\u001b[0m     trust_remote_code\u001b[38;5;241m=\u001b[39mtrust_remote_code,\n\u001b[0;32m   2073\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   2074\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\victo\\miniconda3\\envs\\f1_strat_manager\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2303\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._from_pretrained\u001b[1;34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, token, cache_dir, local_files_only, _commit_hash, _is_local, trust_remote_code, *init_inputs, **kwargs)\u001b[0m\n\u001b[0;32m   2301\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   2302\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m(\u001b[38;5;241m*\u001b[39minit_inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minit_kwargs)\n\u001b[1;32m-> 2303\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[43mimport_protobuf_decode_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m   2304\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\n\u001b[0;32m   2305\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to load tokenizer model from SPM, loading from TikToken will be attempted instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2306\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(Google protobuf error: Tried to load SPM model with non-SPM vocab file).\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   2307\u001b[0m     )\n\u001b[0;32m   2308\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\victo\\miniconda3\\envs\\f1_strat_manager\\lib\\site-packages\\transformers\\tokenization_utils_base.py:87\u001b[0m, in \u001b[0;36mimport_protobuf_decode_error\u001b[1;34m(error_message)\u001b[0m\n\u001b[0;32m     85\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DecodeError\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 87\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(PROTOBUF_IMPORT_ERROR\u001b[38;5;241m.\u001b[39mformat(error_message))\n",
      "\u001b[1;31mImportError\u001b[0m: \n requires the protobuf library but it was not found in your environment. Checkout the instructions on the\ninstallation page of its repo: https://github.com/protocolbuffers/protobuf/tree/master/python#installation and follow the ones\nthat match your environment. Please note that you may need to restart your runtime after installation.\n"
     ]
    }
   ],
   "source": [
    "# Initialize tokenizer (smaller model for demonstration purposes)\n",
    "model_name = \"microsoft/deberta-v3-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize a sample to demonstrate alignment\n",
    "if bio_data:\n",
    "    sample = {\"tokens\": [bio_data[0][\"tokens\"]], \"ner_tags\": [bio_data[0][\"ner_tags\"]]}\n",
    "    tokenized_sample = tokenize_and_align_labels(sample, tokenizer, tag2id)\n",
    "    \n",
    "    print(\"Sample tokenization and label alignment:\")\n",
    "    print(f\"Original tokens: {sample['tokens'][0][:5]}...\")\n",
    "    print(f\"Tokenized: {tokenizer.convert_ids_to_tokens(tokenized_sample['input_ids'][0])[:10]}...\")\n",
    "    \n",
    "    # Show how labels are aligned\n",
    "    print(\"\\nLabel alignment example (first few tokens):\")\n",
    "    for i in range(10):  # Show first 10 tokens\n",
    "        token = tokenizer.convert_ids_to_tokens(tokenized_sample['input_ids'][0][i])\n",
    "        label_id = tokenized_sample['labels'][0][i]\n",
    "        label = id2tag.get(label_id, \"IGNORE\") if label_id != -100 else \"IGNORE\"\n",
    "        print(f\"  {token} -> {label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. Defining Evaluation Metrics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "f1_strat_manager",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
