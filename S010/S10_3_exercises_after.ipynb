{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hugging Face Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForTokenClassification, \n",
    "    Trainer, \n",
    "    TrainingArguments,\n",
    "    DataCollatorForTokenClassification,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "from datasets import Dataset as HFDataset\n",
    "from datasets import DatasetDict\n",
    "from sklearn.model_selection import train_test_split\n",
    "from seqeval.metrics import f1_score, precision_score, recall_score, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Constant Definition \n",
    "\n",
    "In this cell, IÂ´ll document the type of entities and their correspondant colors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Types of defined entities:\n",
      "  - ACTION: Direct commands or actions mentioned in the message\n",
      "  - SITUATION: Racing context or circumstance descriptions\n",
      "  - INCIDENT: Accidents or on-track events\n",
      "  - STRATEGY_INSTRUCTION: Strategic directives\n",
      "  - POSITION_CHANGE: References to overtakes or positions\n",
      "  - PIT_CALL: Specific calls for pit stops\n",
      "  - TRACK_CONDITION: Mentions of the track's state\n",
      "  - TECHNICAL_ISSUE: Mechanical or car-related problems\n",
      "  - WEATHER: References to weather conditions\n"
     ]
    }
   ],
   "source": [
    "# Define entity types and their descriptions\n",
    "ENTITY_TYPES = {\n",
    "    \"ACTION\": \"Direct commands or actions mentioned in the message\",\n",
    "    \"SITUATION\": \"Racing context or circumstance descriptions\",\n",
    "    \"INCIDENT\": \"Accidents or on-track events\",\n",
    "    \"STRATEGY_INSTRUCTION\": \"Strategic directives\",\n",
    "    \"POSITION_CHANGE\": \"References to overtakes or positions\",\n",
    "    \"PIT_CALL\": \"Specific calls for pit stops\",\n",
    "    \"TRACK_CONDITION\": \"Mentions of the track's state\",\n",
    "    \"TECHNICAL_ISSUE\": \"Mechanical or car-related problems\",\n",
    "    \"WEATHER\": \"References to weather conditions\"\n",
    "}\n",
    "\n",
    "# Color scheme for entity visualization\n",
    "ENTITY_COLORS = {\n",
    "    \"ACTION\": \"#4e79a7\",           # Blue\n",
    "    \"SITUATION\": \"#f28e2c\",         # Orange\n",
    "    \"INCIDENT\": \"#e15759\",          # Red\n",
    "    \"STRATEGY_INSTRUCTION\": \"#76b7b2\", # Teal\n",
    "    \"POSITION_CHANGE\": \"#59a14f\",   # Green\n",
    "    \"PIT_CALL\": \"#edc949\",          # Yellow\n",
    "    \"TRACK_CONDITION\": \"#af7aa1\",   # Purple\n",
    "    \"TECHNICAL_ISSUE\": \"#ff9da7\",   # Pink\n",
    "    \"WEATHER\": \"#9c755f\"            # Brown\n",
    "}\n",
    "\n",
    "print(\"Entity types defined:\")\n",
    "for entity, description in ENTITY_TYPES.items():\n",
    "    print(f\"  - {entity}: {description}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load and Explore Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load F1 radio data from JSON file\n",
    "def load_f1_radio_data(json_file):\n",
    "    \"\"\"Load and explore F1 radio data from JSON file\"\"\"\n",
    "    with open(json_file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    print(f\"Loaded {len(data)} messages from {json_file}\")\n",
    "    \n",
    "    # Show sample structure\n",
    "    if len(data) > 0:\n",
    "        print(\"\\nSample record structure:\")\n",
    "        sample = data[0]\n",
    "        print(f\"  Driver: {sample.get('driver', 'N/A')}\")\n",
    "        print(f\"  Radio message: {sample.get('radio_message', 'N/A')[:100]}...\")\n",
    "        \n",
    "        if 'annotations' in sample and len(sample['annotations']) > 1:\n",
    "            if isinstance(sample['annotations'][1], dict) and 'entities' in sample['annotations'][1]:\n",
    "                entities = sample['annotations'][1]['entities']\n",
    "                print(f\"  Number of entities: {len(entities)}\")\n",
    "                if len(entities) > 0:\n",
    "                    entity = entities[0]\n",
    "                    entity_text = sample['radio_message'][entity[0]:entity[1]]\n",
    "                    print(f\"  Sample entity: [{entity[0]}, {entity[1]}, '{entity_text}', '{entity[2]}']\")\n",
    "    \n",
    "    return data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 529 messages from f1_radio_entity_annotations.json\n",
      "\n",
      "Sample record structure:\n",
      "  Driver: 1\n",
      "  Radio message: So don't forget Max, use your head please. Are we both doing it or what? You just follow my instruct...\n",
      "  Number of entities: 3\n",
      "  Sample entity: [82, 103, 'follow my instruction', 'ACTION']\n",
      "\n",
      "Entity type distribution in dataset:\n",
      "  - SITUATION: 255\n",
      "  - ACTION: 165\n",
      "  - STRATEGY_INSTRUCTION: 137\n",
      "  - TECHNICAL_ISSUE: 137\n",
      "  - WEATHER: 112\n",
      "  - POSITION_CHANGE: 83\n",
      "  - INCIDENT: 78\n",
      "  - TRACK_CONDITION: 62\n",
      "  - PIT_CALL: 42\n"
     ]
    }
   ],
   "source": [
    "# Load the JSON data\n",
    "json_file_path = \"f1_radio_entity_annotations.json\"\n",
    "f1_data = load_f1_radio_data(json_file_path)\n",
    "\n",
    "# Count entity types in the dataset\n",
    "entity_counts = {}\n",
    "for item in f1_data:\n",
    "    if 'annotations' in item and len(item['annotations']) > 1:\n",
    "        if isinstance(item['annotations'][1], dict) and 'entities' in item['annotations'][1]:\n",
    "            for _, _, entity_type in item['annotations'][1]['entities']:\n",
    "                entity_counts[entity_type] = entity_counts.get(entity_type, 0) + 1\n",
    "\n",
    "print(\"\\nEntity type distribution in dataset:\")\n",
    "for entity_type, count in sorted(entity_counts.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"  - {entity_type}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Preprocessing F1 Radio Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_f1_data(data):\n",
    "    \"\"\"Extract and preprocess F1 radio data with valid annotations\"\"\"\n",
    "    processed_data = []\n",
    "    skipped_count = 0\n",
    "    \n",
    "    for item in data:\n",
    "        if 'radio_message' not in item or 'annotations' not in item:\n",
    "            skipped_count += 1\n",
    "            continue\n",
    "            \n",
    "        text = item['radio_message']\n",
    "        \n",
    "        # Skip items with empty or null text\n",
    "        if not text or text.strip() == \"\":\n",
    "            skipped_count += 1\n",
    "            continue\n",
    "            \n",
    "        # Extract entities if they exist in expected format\n",
    "        if len(item['annotations']) > 1 and isinstance(item['annotations'][1], dict):\n",
    "            annotations = item['annotations'][1]\n",
    "            if 'entities' in annotations and annotations['entities']:\n",
    "                entities = annotations['entities']\n",
    "                \n",
    "                # Add to processed data\n",
    "                processed_data.append({\n",
    "                    'text': text,\n",
    "                    'entities': entities,\n",
    "                    'driver': item.get('driver', None)\n",
    "                })\n",
    "            else:\n",
    "                skipped_count += 1\n",
    "        else:\n",
    "            skipped_count += 1\n",
    "    \n",
    "    print(f\"Processed {len(processed_data)} messages with valid annotations\")\n",
    "    print(f\"Skipped {skipped_count} messages with missing or invalid annotations\")\n",
    "    \n",
    "    # Show a sample of processed data\n",
    "    if processed_data:\n",
    "        sample = processed_data[10]\n",
    "        print(\"\\nSample processed message:\")\n",
    "        print(f\"Text: {sample['text']}\")\n",
    "        print(\"Entities:\")\n",
    "        for start, end, entity_type in sample['entities']:\n",
    "            entity_text = sample['text'][start:end]\n",
    "            print(f\"  - [{start}, {end}] '{entity_text}' ({entity_type})\")\n",
    "    \n",
    "    return processed_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 399 messages with valid annotations\n",
      "Skipped 130 messages with missing or invalid annotations\n",
      "\n",
      "Sample processed message:\n",
      "Text: Max, we've currently got yellows in turn 7. Ferrari in the wall, no? Yes, that's Charles stopped. We are expecting the potential of an aborted start, but just keep to your protocol at the moment.\n",
      "Entities:\n",
      "  - [159, 194] 'keep to your protocol at the moment' (ACTION)\n",
      "  - [5, 42] 'we've currently got yellows in turn 7' (SITUATION)\n",
      "  - [98, 148] 'We are expecting the potential of an aborted start' (SITUATION)\n",
      "  - [44, 63] 'Ferrari in the wall' (INCIDENT)\n",
      "  - [74, 96] 'that's Charles stopped' (INCIDENT)\n"
     ]
    }
   ],
   "source": [
    "# Preprocess the loaded data\n",
    "processed_f1_data = preprocess_f1_data(f1_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Covert to BIO tagging format\n",
    "\n",
    "Deeper BIO tagging format information can be searched [here](https://en.wikipedia.org/wiki/Insideâoutsideâbeginning_(tagging)).\n",
    "\n",
    "### BIO Format Explanation\n",
    "\n",
    "The **BIO format** is a way to label words in a sentence to indicate if they are part of a named entity, and if so, where in the entity they belong. It uses three types of labels:\n",
    "\n",
    "- **B- (Beginning)**: The first word in an entity.\n",
    "- **I- (Inside)**: Any word inside the entity that isn't the first one.\n",
    "- **O (Outside)**: Words that are not part of any entity.\n",
    "\n",
    "---\n",
    "\n",
    "### Example Radio\n",
    "\n",
    "Here is an example of a radio message from Max VerstappenÂ´s track engineer: \n",
    "\n",
    "**Text:**  \n",
    "*\"Max, we've currently got yellows in turn 7. Ferrari in the wall, no? Yes, that's Charles stopped. We are expecting the potential of an aborted start, but just keep to your protocol at the moment.\"*\n",
    "\n",
    "Here are the entities mentioned in the message:\n",
    "\n",
    "1. **'keep to your protocol at the moment'** (ACTION)\n",
    "2. **'we've currently got yellows in turn 7'** (SITUATION)\n",
    "3. **'We are expecting the potential of an aborted start'** (SITUATION)\n",
    "4. **'Ferrari in the wall'** (INCIDENT)\n",
    "5. **'that's Charles stopped'** (INCIDENT)\n",
    "\n",
    "---\n",
    "\n",
    "### Breaking the Sentence\n",
    "\n",
    "We break the sentence into words and then tag them as follows:\n",
    "\n",
    "| Word            | BIO Tag          |\n",
    "|-----------------|------------------|\n",
    "| Max,            | O                |\n",
    "| we've           | O                |\n",
    "| currently       | O                |\n",
    "| got             | O                |\n",
    "| yellows         | O                |\n",
    "| in              | O                |\n",
    "| turn            | O                |\n",
    "| 7.              | O                |\n",
    "| Ferrari         | B-INCIDENT       |\n",
    "| in              | I-INCIDENT       |\n",
    "| the             | I-INCIDENT       |\n",
    "| wall,           | I-INCIDENT       |\n",
    "| no?             | O                |\n",
    "| Yes,            | O                |\n",
    "| that's          | B-INCIDENT       |\n",
    "| Charles         | I-INCIDENT       |\n",
    "| stopped.        | I-INCIDENT       |\n",
    "| We              | B-SITUATION      |\n",
    "| are             | I-SITUATION      |\n",
    "| expecting       | I-SITUATION      |\n",
    "| the             | I-SITUATION      |\n",
    "| potential       | I-SITUATION      |\n",
    "| of              | I-SITUATION      |\n",
    "| an              | I-SITUATION      |\n",
    "| aborted         | I-SITUATION      |\n",
    "| start,          | I-SITUATION      |\n",
    "| but             | O                |\n",
    "| just            | O                |\n",
    "| keep            | B-ACTION         |\n",
    "| to              | I-ACTION         |\n",
    "| your            | I-ACTION         |\n",
    "| protocol        | I-ACTION         |\n",
    "| at              | I-ACTION         |\n",
    "| the             | I-ACTION         |\n",
    "| moment.         | I-ACTION         |\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ner_tags(text, entities):\n",
    "    \"\"\"Convert character-based entity spans to token-based BIO tags\"\"\"\n",
    "    words = text.split()\n",
    "    tags = [\"O\"] * len(words)\n",
    "    char_to_word = {}\n",
    "    \n",
    "    # Create mapping from character positions to word indices\n",
    "    char_idx = 0\n",
    "    for word_idx, word in enumerate(words):\n",
    "        # Account for spaces\n",
    "        if char_idx > 0:\n",
    "            char_idx += 1  # Space\n",
    "        \n",
    "        # Map each character position to its word index\n",
    "        for char_pos in range(char_idx, char_idx + len(word)):\n",
    "            char_to_word[char_pos] = word_idx\n",
    "        \n",
    "        char_idx += len(word)\n",
    "    \n",
    "    # Apply entity tags\n",
    "    for start_char, end_char, entity_type in entities:\n",
    "        # Skip invalid spans\n",
    "        if start_char >= len(text) or end_char > len(text) or start_char >= end_char:\n",
    "            continue\n",
    "            \n",
    "        # Find word indices for start and end characters\n",
    "        if start_char in char_to_word:\n",
    "            start_word = char_to_word[start_char]\n",
    "            # Find the last word of the entity\n",
    "            end_word = char_to_word.get(end_char - 1, start_word)\n",
    "            \n",
    "            # Tag the first word as B-entity\n",
    "            tags[start_word] = f\"B-{entity_type}\"\n",
    "            \n",
    "            # Tag subsequent words as I-entity\n",
    "            for word_idx in range(start_word + 1, end_word + 1):\n",
    "                tags[word_idx] = f\"I-{entity_type}\"\n",
    "    \n",
    "    return words, tags\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_bio_format(processed_data):\n",
    "    \"\"\"Convert processed data to BIO tagging format\"\"\"\n",
    "    bio_data = []\n",
    "    mapping_errors = 0\n",
    "    \n",
    "    for item in processed_data:\n",
    "        text = item['text']\n",
    "        entities = item['entities']\n",
    "        \n",
    "        # Convert to BIO tags\n",
    "        words, tags = create_ner_tags(text, entities)\n",
    "        \n",
    "        # Check if we mapped any entities\n",
    "        if all(tag == \"O\" for tag in tags) and len(entities) > 0:\n",
    "            mapping_errors += 1\n",
    "        \n",
    "        bio_data.append({\n",
    "            \"tokens\": words,\n",
    "            \"ner_tags\": tags,\n",
    "            \"driver\": item.get('driver', None)\n",
    "        })\n",
    "    \n",
    "    print(f\"Converted {len(bio_data)} messages to BIO format\")\n",
    "    print(f\"Mapping errors: {mapping_errors} (messages where no entities were mapped)\")\n",
    "    \n",
    "    # Show an example\n",
    "    if bio_data:\n",
    "        sample = bio_data[10]\n",
    "        print(\"\\nSample BIO tagging:\")\n",
    "        print(f\"Original text: {' '.join(sample['tokens'])}\")\n",
    "        for token, tag in zip(sample['tokens'], sample['ner_tags']):\n",
    "            print(f\"  {token} -> {tag}\")\n",
    "    \n",
    "    return bio_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 399 messages to BIO format\n",
      "Mapping errors: 0 (messages where no entities were mapped)\n",
      "\n",
      "Sample BIO tagging:\n",
      "Original text: Max, we've currently got yellows in turn 7. Ferrari in the wall, no? Yes, that's Charles stopped. We are expecting the potential of an aborted start, but just keep to your protocol at the moment.\n",
      "  Max, -> O\n",
      "  we've -> B-SITUATION\n",
      "  currently -> I-SITUATION\n",
      "  got -> I-SITUATION\n",
      "  yellows -> I-SITUATION\n",
      "  in -> I-SITUATION\n",
      "  turn -> I-SITUATION\n",
      "  7. -> I-SITUATION\n",
      "  Ferrari -> B-INCIDENT\n",
      "  in -> I-INCIDENT\n",
      "  the -> I-INCIDENT\n",
      "  wall, -> I-INCIDENT\n",
      "  no? -> O\n",
      "  Yes, -> O\n",
      "  that's -> B-INCIDENT\n",
      "  Charles -> I-INCIDENT\n",
      "  stopped. -> I-INCIDENT\n",
      "  We -> B-SITUATION\n",
      "  are -> I-SITUATION\n",
      "  expecting -> I-SITUATION\n",
      "  the -> I-SITUATION\n",
      "  potential -> I-SITUATION\n",
      "  of -> I-SITUATION\n",
      "  an -> I-SITUATION\n",
      "  aborted -> I-SITUATION\n",
      "  start, -> I-SITUATION\n",
      "  but -> O\n",
      "  just -> O\n",
      "  keep -> B-ACTION\n",
      "  to -> I-ACTION\n",
      "  your -> I-ACTION\n",
      "  protocol -> I-ACTION\n",
      "  at -> I-ACTION\n",
      "  the -> I-ACTION\n",
      "  moment. -> I-ACTION\n"
     ]
    }
   ],
   "source": [
    "# Convert processed data to BIO format\n",
    "bio_data = convert_to_bio_format(processed_f1_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What the Function Does\n",
    "\n",
    "The function `create_ner_tags` takes the text and entities and converts them into BIO format. It starts by splitting the text into words. \n",
    "\n",
    "Then, it maps each word to a tag: \"O\" for words that are not part of an entity, \"B-\" for the first word of an entity, and \"I-\" for subsequent words inside the entity. \n",
    "\n",
    "The function also uses the character positions of the entities to determine which words they correspond to. Once the tags are assigned, the function returns the words and their BIO tags, ready for use in training a Named Entity Recognition (NER) model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "f1_strat_manager",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
