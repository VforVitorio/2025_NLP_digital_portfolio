{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hugging Face Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import os\n",
    "from transformers import DebertaV2Tokenizer, DebertaV2ForTokenClassification\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForTokenClassification, \n",
    "    Trainer, \n",
    "    TrainingArguments,\n",
    "    DataCollatorForTokenClassification,\n",
    "    EarlyStoppingCallback,\n",
    "    \n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "\n",
    "from datasets import Dataset as HFDataset, DatasetDict\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from tqdm.auto import tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Constant Definition \n",
    "\n",
    "In this cell, I´ll document the type of entities and their correspondant colors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define entity types and their descriptions\n",
    "ENTITY_TYPES = {\n",
    "    \"ACTION\": \"Direct commands or actions mentioned in the message\",\n",
    "    \"SITUATION\": \"Racing context or circumstance descriptions\",\n",
    "    \"INCIDENT\": \"Accidents or on-track events\",\n",
    "    \"STRATEGY_INSTRUCTION\": \"Strategic directives\",\n",
    "    \"POSITION_CHANGE\": \"References to overtakes or positions\",\n",
    "    \"PIT_CALL\": \"Specific calls for pit stops\",\n",
    "    \"TRACK_CONDITION\": \"Mentions of the track's state\",\n",
    "    \"TECHNICAL_ISSUE\": \"Mechanical or car-related problems\",\n",
    "    \"WEATHER\": \"References to weather conditions\"\n",
    "}\n",
    "\n",
    "# Color scheme for entity visualization\n",
    "ENTITY_COLORS = {\n",
    "    \"ACTION\": \"#4e79a7\",           # Blue\n",
    "    \"SITUATION\": \"#f28e2c\",         # Orange\n",
    "    \"INCIDENT\": \"#e15759\",          # Red\n",
    "    \"STRATEGY_INSTRUCTION\": \"#76b7b2\", # Teal\n",
    "    \"POSITION_CHANGE\": \"#59a14f\",   # Green\n",
    "    \"PIT_CALL\": \"#edc949\",          # Yellow\n",
    "    \"TRACK_CONDITION\": \"#af7aa1\",   # Purple\n",
    "    \"TECHNICAL_ISSUE\": \"#ff9da7\",   # Pink\n",
    "    \"WEATHER\": \"#9c755f\"            # Brown\n",
    "}\n",
    "\n",
    "print(\"Entity types defined:\")\n",
    "for entity, description in ENTITY_TYPES.items():\n",
    "    print(f\"  - {entity}: {description}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load and Explore Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load F1 radio data from JSON file\n",
    "def load_f1_radio_data(json_file):\n",
    "    \"\"\"Load and explore F1 radio data from JSON file\"\"\"\n",
    "    with open(json_file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    print(f\"Loaded {len(data)} messages from {json_file}\")\n",
    "    \n",
    "    # Show sample structure\n",
    "    if len(data) > 0:\n",
    "        print(\"\\nSample record structure:\")\n",
    "        sample = data[0]\n",
    "        print(f\"  Driver: {sample.get('driver', 'N/A')}\")\n",
    "        print(f\"  Radio message: {sample.get('radio_message', 'N/A')[:100]}...\")\n",
    "        \n",
    "        if 'annotations' in sample and len(sample['annotations']) > 1:\n",
    "            if isinstance(sample['annotations'][1], dict) and 'entities' in sample['annotations'][1]:\n",
    "                entities = sample['annotations'][1]['entities']\n",
    "                print(f\"  Number of entities: {len(entities)}\")\n",
    "                if len(entities) > 0:\n",
    "                    entity = entities[0]\n",
    "                    entity_text = sample['radio_message'][entity[0]:entity[1]]\n",
    "                    print(f\"  Sample entity: [{entity[0]}, {entity[1]}, '{entity_text}', '{entity[2]}']\")\n",
    "    \n",
    "    return data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the JSON data\n",
    "json_file_path = \"f1_radio_entity_annotations.json\"\n",
    "f1_data = load_f1_radio_data(json_file_path)\n",
    "\n",
    "# Count entity types in the dataset\n",
    "entity_counts = {}\n",
    "for item in f1_data:\n",
    "    if 'annotations' in item and len(item['annotations']) > 1:\n",
    "        if isinstance(item['annotations'][1], dict) and 'entities' in item['annotations'][1]:\n",
    "            for _, _, entity_type in item['annotations'][1]['entities']:\n",
    "                entity_counts[entity_type] = entity_counts.get(entity_type, 0) + 1\n",
    "\n",
    "print(\"\\nEntity type distribution in dataset:\")\n",
    "for entity_type, count in sorted(entity_counts.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"  - {entity_type}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Preprocessing F1 Radio Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_f1_data(data):\n",
    "    \"\"\"Extract and preprocess F1 radio data with valid annotations\"\"\"\n",
    "    processed_data = []\n",
    "    skipped_count = 0\n",
    "    \n",
    "    for item in data:\n",
    "        if 'radio_message' not in item or 'annotations' not in item:\n",
    "            skipped_count += 1\n",
    "            continue\n",
    "            \n",
    "        text = item['radio_message']\n",
    "        \n",
    "        # Skip items with empty or null text\n",
    "        if not text or text.strip() == \"\":\n",
    "            skipped_count += 1\n",
    "            continue\n",
    "            \n",
    "        # Extract entities if they exist in expected format\n",
    "        if len(item['annotations']) > 1 and isinstance(item['annotations'][1], dict):\n",
    "            annotations = item['annotations'][1]\n",
    "            if 'entities' in annotations and annotations['entities']:\n",
    "                entities = annotations['entities']\n",
    "                \n",
    "                # Add to processed data\n",
    "                processed_data.append({\n",
    "                    'text': text,\n",
    "                    'entities': entities,\n",
    "                    'driver': item.get('driver', None)\n",
    "                })\n",
    "            else:\n",
    "                skipped_count += 1\n",
    "        else:\n",
    "            skipped_count += 1\n",
    "    \n",
    "    print(f\"Processed {len(processed_data)} messages with valid annotations\")\n",
    "    print(f\"Skipped {skipped_count} messages with missing or invalid annotations\")\n",
    "    \n",
    "    # Show a sample of processed data\n",
    "    if processed_data:\n",
    "        sample = processed_data[10]\n",
    "        print(\"\\nSample processed message:\")\n",
    "        print(f\"Text: {sample['text']}\")\n",
    "        print(\"Entities:\")\n",
    "        for start, end, entity_type in sample['entities']:\n",
    "            entity_text = sample['text'][start:end]\n",
    "            print(f\"  - [{start}, {end}] '{entity_text}' ({entity_type})\")\n",
    "    \n",
    "    return processed_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the loaded data\n",
    "processed_f1_data = preprocess_f1_data(f1_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Covert to BIO tagging format\n",
    "\n",
    "Deeper BIO tagging format information can be searched [here](https://en.wikipedia.org/wiki/Inside–outside–beginning_(tagging)).\n",
    "\n",
    "### BIO Format Explanation\n",
    "\n",
    "The **BIO format** is a way to label words in a sentence to indicate if they are part of a named entity, and if so, where in the entity they belong. It uses three types of labels:\n",
    "\n",
    "- **B- (Beginning)**: The first word in an entity.\n",
    "- **I- (Inside)**: Any word inside the entity that isn't the first one.\n",
    "- **O (Outside)**: Words that are not part of any entity.\n",
    "\n",
    "---\n",
    "\n",
    "### Example Radio\n",
    "\n",
    "Here is an example of a radio message from Max Verstappen´s track engineer: \n",
    "\n",
    "**Text:**  \n",
    "*\"Max, we've currently got yellows in turn 7. Ferrari in the wall, no? Yes, that's Charles stopped. We are expecting the potential of an aborted start, but just keep to your protocol at the moment.\"*\n",
    "\n",
    "Here are the entities mentioned in the message:\n",
    "\n",
    "1. **'keep to your protocol at the moment'** (ACTION)\n",
    "2. **'we've currently got yellows in turn 7'** (SITUATION)\n",
    "3. **'We are expecting the potential of an aborted start'** (SITUATION)\n",
    "4. **'Ferrari in the wall'** (INCIDENT)\n",
    "5. **'that's Charles stopped'** (INCIDENT)\n",
    "\n",
    "---\n",
    "\n",
    "### Breaking the Sentence\n",
    "\n",
    "We break the sentence into words and then tag them as follows:\n",
    "\n",
    "| Word            | BIO Tag          |\n",
    "|-----------------|------------------|\n",
    "| Max,            | O                |\n",
    "| we've           | O                |\n",
    "| currently       | O                |\n",
    "| got             | O                |\n",
    "| yellows         | O                |\n",
    "| in              | O                |\n",
    "| turn            | O                |\n",
    "| 7.              | O                |\n",
    "| Ferrari         | B-INCIDENT       |\n",
    "| in              | I-INCIDENT       |\n",
    "| the             | I-INCIDENT       |\n",
    "| wall,           | I-INCIDENT       |\n",
    "| no?             | O                |\n",
    "| Yes,            | O                |\n",
    "| that's          | B-INCIDENT       |\n",
    "| Charles         | I-INCIDENT       |\n",
    "| stopped.        | I-INCIDENT       |\n",
    "| We              | B-SITUATION      |\n",
    "| are             | I-SITUATION      |\n",
    "| expecting       | I-SITUATION      |\n",
    "| the             | I-SITUATION      |\n",
    "| potential       | I-SITUATION      |\n",
    "| of              | I-SITUATION      |\n",
    "| an              | I-SITUATION      |\n",
    "| aborted         | I-SITUATION      |\n",
    "| start,          | I-SITUATION      |\n",
    "| but             | O                |\n",
    "| just            | O                |\n",
    "| keep            | B-ACTION         |\n",
    "| to              | I-ACTION         |\n",
    "| your            | I-ACTION         |\n",
    "| protocol        | I-ACTION         |\n",
    "| at              | I-ACTION         |\n",
    "| the             | I-ACTION         |\n",
    "| moment.         | I-ACTION         |\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ner_tags(text, entities):\n",
    "    \"\"\"Convert character-based entity spans to token-based BIO tags\"\"\"\n",
    "    words = text.split()\n",
    "    tags = [\"O\"] * len(words)\n",
    "    char_to_word = {}\n",
    "    \n",
    "    # Create mapping from character positions to word indices\n",
    "    char_idx = 0\n",
    "    for word_idx, word in enumerate(words):\n",
    "        # Account for spaces\n",
    "        if char_idx > 0:\n",
    "            char_idx += 1  # Space\n",
    "        \n",
    "        # Map each character position to its word index\n",
    "        for char_pos in range(char_idx, char_idx + len(word)):\n",
    "            char_to_word[char_pos] = word_idx\n",
    "        \n",
    "        char_idx += len(word)\n",
    "    \n",
    "    # Apply entity tags\n",
    "    for start_char, end_char, entity_type in entities:\n",
    "        # Skip invalid spans\n",
    "        if start_char >= len(text) or end_char > len(text) or start_char >= end_char:\n",
    "            continue\n",
    "            \n",
    "        # Find word indices for start and end characters\n",
    "        if start_char in char_to_word:\n",
    "            start_word = char_to_word[start_char]\n",
    "            # Find the last word of the entity\n",
    "            end_word = char_to_word.get(end_char - 1, start_word)\n",
    "            \n",
    "            # Tag the first word as B-entity\n",
    "            tags[start_word] = f\"B-{entity_type}\"\n",
    "            \n",
    "            # Tag subsequent words as I-entity\n",
    "            for word_idx in range(start_word + 1, end_word + 1):\n",
    "                tags[word_idx] = f\"I-{entity_type}\"\n",
    "    \n",
    "    return words, tags\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_bio_format(processed_data):\n",
    "    \"\"\"Convert processed data to BIO tagging format\"\"\"\n",
    "    bio_data = []\n",
    "    mapping_errors = 0\n",
    "    \n",
    "    for item in processed_data:\n",
    "        text = item['text']\n",
    "        entities = item['entities']\n",
    "        \n",
    "        # Convert to BIO tags\n",
    "        words, tags = create_ner_tags(text, entities)\n",
    "        \n",
    "        # Check if we mapped any entities\n",
    "        if all(tag == \"O\" for tag in tags) and len(entities) > 0:\n",
    "            mapping_errors += 1\n",
    "        \n",
    "        bio_data.append({\n",
    "            \"tokens\": words,\n",
    "            \"ner_tags\": tags,\n",
    "            \"driver\": item.get('driver', None)\n",
    "        })\n",
    "    \n",
    "    print(f\"Converted {len(bio_data)} messages to BIO format\")\n",
    "    print(f\"Mapping errors: {mapping_errors} (messages where no entities were mapped)\")\n",
    "    \n",
    "    # Show an example\n",
    "    if bio_data:\n",
    "        sample = bio_data[10]\n",
    "        print(\"\\nSample BIO tagging:\")\n",
    "        print(f\"Original text: {' '.join(sample['tokens'])}\")\n",
    "        for token, tag in zip(sample['tokens'], sample['ner_tags']):\n",
    "            print(f\"  {token} -> {tag}\")\n",
    "    \n",
    "    return bio_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert processed data to BIO format\n",
    "bio_data = convert_to_bio_format(processed_f1_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What the Function Does\n",
    "\n",
    "The function `create_ner_tags` takes the text and entities and converts them into BIO format. It starts by splitting the text into words. \n",
    "\n",
    "Then, it maps each word to a tag: \"O\" for words that are not part of an entity, \"B-\" for the first word of an entity, and \"I-\" for subsequent words inside the entity. \n",
    "\n",
    "The function also uses the character positions of the entities to determine which words they correspond to. Once the tags are assigned, the function returns the words and their BIO tags, ready for use in training a Named Entity Recognition (NER) model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Create tag mappings and prepare datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 `create_tag_mappings`\n",
    "\n",
    "This function creates mappings between NER (Named Entity Recognition) tags and unique IDs. It does this by:\n",
    "\n",
    "1. Collecting all unique NER tags from the `bio_data`.\n",
    "2. Sorting and assigning each unique tag an ID.\n",
    "3. Creating two mappings:\n",
    "   - `tag2id`: Maps each tag to its corresponding ID.\n",
    "   - `id2tag`: Maps each ID back to its corresponding tag.\n",
    "\n",
    "It then prints out the mappings and returns the two dictionaries: `tag2id` and `id2tag`.\n",
    "\n",
    "**What it does:**\n",
    "- Converts NER tags into unique IDs for easier processing in machine learning models.\n",
    "- Helps with transforming the tags when working with model inputs and outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tag_mappings(bio_data):\n",
    "    \"\"\"Create mappings between NER tags and IDs\"\"\"\n",
    "    unique_tags = set()\n",
    "    for item in bio_data:\n",
    "        unique_tags.update(item[\"ner_tags\"])\n",
    "    \n",
    "    tag2id = {tag: id for id, tag in enumerate(sorted(list(unique_tags)))}\n",
    "    id2tag = {id: tag for tag, id in tag2id.items()}\n",
    "    \n",
    "    print(f\"Created mappings for {len(tag2id)} unique tags:\")\n",
    "    for tag, idx in tag2id.items():\n",
    "        print(f\"  {tag}: {idx}\")\n",
    "    \n",
    "    return tag2id, id2tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tag mappings\n",
    "tag2id, id2tag = create_tag_mappings(bio_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 `prepare_datasets`\n",
    "\n",
    "This function prepares the dataset for training a model by splitting it into training, validation, and test sets using the Hugging Face library. Here's what it does:\n",
    "\n",
    "1. Converts the input `bio_data` into a Hugging Face `Dataset`.\n",
    "2. Splits the data into two parts: training + validation, and test.\n",
    "3. Further splits the training data into training and validation sets based on the specified sizes (`test_size` and `val_size`).\n",
    "4. Returns a `DatasetDict` containing the `train`, `validation`, and `test` sets.\n",
    "\n",
    "**What it does:**\n",
    "- Converts the data into a format suitable for machine learning.\n",
    "- Splits the data into three parts: training, validation, and test sets for model evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_datasets(bio_data, test_size=0.1, val_size=0.1, seed=42):\n",
    "    \"\"\"Convert to Hugging Face Dataset and split into train/val/test\"\"\"\n",
    "    # Convert to Hugging Face dataset\n",
    "    hf_dataset = HFDataset.from_list(bio_data)\n",
    "    \n",
    "    # First split: train + validation vs test\n",
    "    train_val_test = hf_dataset.train_test_split(test_size=test_size, seed=seed)\n",
    "    \n",
    "    # Second split: train vs validation (validation is val_size/(1-test_size) of the train set)\n",
    "    val_fraction = val_size / (1 - test_size)\n",
    "    train_val = train_val_test[\"train\"].train_test_split(test_size=val_fraction, seed=seed)\n",
    "    \n",
    "    # Combine into DatasetDict\n",
    "    datasets = DatasetDict({\n",
    "        \"train\": train_val[\"train\"],\n",
    "        \"validation\": train_val[\"test\"],\n",
    "        \"test\": train_val_test[\"test\"]\n",
    "    })\n",
    "    \n",
    "    print(f\"Prepared datasets with:\")\n",
    "    print(f\"  - Train: {len(datasets['train'])} examples\")\n",
    "    print(f\"  - Validation: {len(datasets['validation'])} examples\")\n",
    "    print(f\"  - Test: {len(datasets['test'])} examples\")\n",
    "    \n",
    "    return datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = prepare_datasets(bio_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Calling Up the Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "# Cell 2: Initialize the tokenizer for DeBERTa v3 large\n",
    "model_name = \"microsoft/deberta-v3-large\"\n",
    "tokenizer = DebertaV2Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Check if it loaded correctly\n",
    "print(f\"Tokenizer loaded: {tokenizer.__class__.__name__}\")\n",
    "print(f\"Vocabulary size: {len(tokenizer)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. Custom Dataset for Deberta-v3 Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class F1RadioNERDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, hf_dataset, tokenizer, tag2id, max_len=128):\n",
    "        self.dataset = hf_dataset\n",
    "        self.tokenizer = tokenizer\n",
    "        self.tag2id = tag2id  # Add tag2id mapping\n",
    "        self.max_len = max_len\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataset[idx]\n",
    "        tokens = item[\"tokens\"]\n",
    "        tags = item[\"ner_tags\"]\n",
    "        \n",
    "        # Create a mapping from token index to word index\n",
    "        word_ids = []\n",
    "        all_tokens = []\n",
    "        \n",
    "        for word_idx, word in enumerate(tokens):\n",
    "            # Tokenize each word and keep track of word indices\n",
    "            word_tokens = self.tokenizer.tokenize(word)\n",
    "            if not word_tokens:\n",
    "                # Handle empty tokenization\n",
    "                word_tokens = [self.tokenizer.unk_token]\n",
    "            \n",
    "            for _ in word_tokens:\n",
    "                word_ids.append(word_idx)\n",
    "                \n",
    "            all_tokens.extend(word_tokens)\n",
    "        \n",
    "        # Truncate if necessary (leave room for special tokens)\n",
    "        if len(all_tokens) > self.max_len - 2:  # -2 for [CLS] and [SEP]\n",
    "            all_tokens = all_tokens[:self.max_len - 2]\n",
    "            word_ids = word_ids[:self.max_len - 2]\n",
    "        \n",
    "        # Add special tokens\n",
    "        encoded_input = self.tokenizer.encode_plus(\n",
    "            all_tokens,\n",
    "            is_split_into_words=False,  # We're passing already tokenized input\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        # Initialize labels with ignore index (-100)\n",
    "        labels = torch.ones(self.max_len, dtype=torch.long) * -100\n",
    "        \n",
    "        # Set labels based on word_ids\n",
    "        # First token ([CLS]) is already -100\n",
    "        for i, word_idx in enumerate(word_ids):\n",
    "            if i + 1 < self.max_len - 1:  # +1 for [CLS], leave room for [SEP]\n",
    "                # Convert string tag to numeric ID if needed\n",
    "                if isinstance(tags[word_idx], str):\n",
    "                    tag_id = self.tag2id.get(tags[word_idx], 0)  # Default to 0 (typically 'O')\n",
    "                else:\n",
    "                    tag_id = tags[word_idx]  # Already a numeric ID\n",
    "                    \n",
    "                labels[i + 1] = tag_id\n",
    "        \n",
    "        return {\n",
    "            \"input_ids\": encoded_input[\"input_ids\"].flatten(),\n",
    "            \"attention_mask\": encoded_input[\"attention_mask\"].flatten(),\n",
    "            \"labels\": labels\n",
    "        }\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 10. Pytorch Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.1 Creating Pytorch Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create PyTorch datasets - now pass the tag2id mapping\n",
    "train_dataset = F1RadioNERDataset(datasets[\"train\"], tokenizer, tag2id)\n",
    "val_dataset = F1RadioNERDataset(datasets[\"validation\"], tokenizer, tag2id)\n",
    "test_dataset = F1RadioNERDataset(datasets[\"test\"], tokenizer, tag2id)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.2 Creating Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoaders\n",
    "batch_size = 8  # Reduced batch size due to model size\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.3 Validating Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Validation samples: {len(val_dataset)}\")\n",
    "print(f\"Test samples: {len(test_dataset)}\")\n",
    "\n",
    "# Optional: Check a sample to verify everything is working\n",
    "sample = train_dataset[0]\n",
    "print(f\"Sample input shape: {sample['input_ids'].shape}\")\n",
    "print(f\"Sample attention mask shape: {sample['attention_mask'].shape}\")\n",
    "print(f\"Sample labels shape: {sample['labels'].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 11. Initializing Deberta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Initialize the DeBERTa v3 large model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "num_labels = len(tag2id)  # Use your existing tag2id mapping\n",
    "model = DebertaV2ForTokenClassification.from_pretrained(\n",
    "    model_name, \n",
    "    num_labels=num_labels\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "print(f\"Model loaded: {model_name}\")\n",
    "print(f\"Number of labels: {num_labels}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 12. Set Up the Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Training configuration\n",
    "from sklearn.utils import compute_class_weight\n",
    "\n",
    "\n",
    "epochs = 10\n",
    "train_labels = []\n",
    "for batch in train_loader:\n",
    "    labels = batch['labels']\n",
    "    # Filter ignored tokens\n",
    "    mask = labels != -100\n",
    "    train_labels.extend(labels[mask].numpy())\n",
    "\n",
    "# Calculate weights per class\n",
    "class_weights = compute_class_weight(\n",
    "    'balanced', \n",
    "    classes=np.unique(train_labels), \n",
    "    y=train_labels\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Training configuration\n",
    "from sklearn.utils import compute_class_weight\n",
    "\n",
    "\n",
    "epochs = 10\n",
    "train_labels = []\n",
    "for batch in train_loader:\n",
    "    labels = batch['labels']\n",
    "    # Filter ignored tokens\n",
    "    mask = labels != -100\n",
    "    train_labels.extend(labels[mask].numpy())\n",
    "\n",
    "# Calculate weights per class\n",
    "class_weights = compute_class_weight(\n",
    "    'balanced', \n",
    "    classes=np.unique(train_labels), \n",
    "    y=train_labels\n",
    ")\n",
    "class_weights = torch.FloatTensor(class_weights).to(device)\n",
    "\n",
    "# Defining CrossEntropyLoss as new loss function\n",
    "loss_fn = torch.nn.CrossEntropyLoss(weight=class_weights, ignore_index=-100)\n",
    "\n",
    "# 3. small learning rate for better fine tuning\n",
    "learning_rate = 1e-5  # Reducir de 2e-5 a 1e-5\n",
    "\n",
    "# 4. Add warmup steps for stabilizing training\n",
    "warmup_steps = int(0.1 * len(train_loader) * epochs)  # 10% of total steps\n",
    "# Total steps\n",
    "total_steps = len(train_loader) * epochs\n",
    "\n",
    "# optimizer Adam\"\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate, weight_decay=0.01)\n",
    "\n",
    "# Scheduler\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer, \n",
    "    num_warmup_steps=warmup_steps,\n",
    "    num_training_steps=len(train_loader) * epochs\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics function\n",
    "def compute_metrics(preds, labels):\n",
    "    preds = np.argmax(preds, axis=2).flatten()\n",
    "    labels = labels.flatten()\n",
    "    \n",
    "    # Remove ignored index (-100)\n",
    "    mask = labels != -100\n",
    "    preds = preds[mask]\n",
    "    labels = labels[mask]\n",
    "    \n",
    "    accuracy = accuracy_score(labels, preds)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        labels, preds, average='weighted')\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 13. Training and Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using personalized loss\n",
    "def train_epoch():\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch in tqdm(train_loader, desc=\"Training\"):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        # Reshape for loss function\n",
    "        active_loss = labels != -100\n",
    "        active_logits = logits.view(-1, num_labels)\n",
    "        active_labels = torch.where(\n",
    "            active_loss.view(-1), \n",
    "            labels.view(-1), \n",
    "            torch.tensor(loss_fn.ignore_index).type_as(labels)\n",
    "        )\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = loss_fn(active_logits, active_labels)\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "    return total_loss / len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(data_loader):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(data_loader, desc=\"Evaluating\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=labels\n",
    "            )\n",
    "            \n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            logits = outputs.logits\n",
    "            all_preds.append(logits.detach().cpu().numpy())\n",
    "            all_labels.append(labels.detach().cpu().numpy())\n",
    "    \n",
    "    all_preds = np.concatenate([p for p in all_preds], axis=0)\n",
    "    all_labels = np.concatenate([l for l in all_labels], axis=0)\n",
    "    \n",
    "    metrics = compute_metrics(all_preds, all_labels)\n",
    "    metrics['loss'] = total_loss / len(data_loader)\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 14. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Main training loop\n",
    "best_f1 = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    train_loss = train_epoch()\n",
    "    print(f\"Training loss: {train_loss:.4f}\")\n",
    "    \n",
    "    val_metrics = evaluate(val_loader)\n",
    "    print(f\"Validation loss: {val_metrics['loss']:.4f}\")\n",
    "    print(f\"Validation metrics: accuracy={val_metrics['accuracy']:.4f}, precision={val_metrics['precision']:.4f}, \"\n",
    "          f\"recall={val_metrics['recall']:.4f}, f1={val_metrics['f1']:.4f}\")\n",
    "    \n",
    "    # Save best model\n",
    "    if val_metrics['f1'] > best_f1:\n",
    "        best_f1 = val_metrics['f1']\n",
    "        torch.save(model.state_dict(), 'best_deberta_ner_model.pt')\n",
    "        print(f\"New best model saved with F1: {best_f1:.4f}\")\n",
    "\n",
    "print(\"\\nTraining complete!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Evaluate on validation set \n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in val_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        preds = torch.argmax(outputs.logits, dim=2)\n",
    "        \n",
    "        # Filter -100 padding tokens -100\n",
    "        active_mask = labels != -100\n",
    "        true = labels[active_mask].cpu().numpy()\n",
    "        pred = preds[active_mask].cpu().numpy()\n",
    "        \n",
    "        all_labels.extend(true)\n",
    "        all_preds.extend(pred)\n",
    "\n",
    "# Convertir índices a etiquetas\n",
    "true_tags = [id2tag[l] for l in all_labels]\n",
    "pred_tags = [id2tag[p] for p in all_preds]\n",
    "\n",
    "# Imprimir el classification report\n",
    "print(classification_report(true_tags, pred_tags))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 14.1 Test Set Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "print(\"\\nEvaluating on test set...\")\n",
    "test_metrics = evaluate(test_loader)\n",
    "print(f\"Test loss: {test_metrics['loss']:.4f}\")\n",
    "print(f\"Test metrics: accuracy={test_metrics['accuracy']:.4f}, precision={test_metrics['precision']:.4f}, \"\n",
    "      f\"recall={test_metrics['recall']:.4f}, f1={test_metrics['f1']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizerFast, BertForTokenClassification\n",
    "from torch.utils.data import DataLoader\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer loaded: BertTokenizerFast\n",
      "Vocabulary size: 28996\n"
     ]
    }
   ],
   "source": [
    "# Inicialización del tokenizador para BERT large preentrenado en NER\n",
    "torch.manual_seed(42)\n",
    "model_name = \"dbmdz/bert-large-cased-finetuned-conll03-english\"\n",
    "tokenizer = BertTokenizerFast.from_pretrained(model_name)\n",
    "\n",
    "# Check if it loaded correctly\n",
    "print(f\"Tokenizer loaded: {tokenizer.__class__.__name__}\")\n",
    "print(f\"Vocabulary size: {len(tokenizer)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([9]) in the checkpoint and torch.Size([19]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([9, 1024]) in the checkpoint and torch.Size([19, 1024]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded: dbmdz/bert-large-cased-finetuned-conll03-english\n",
      "Number of labels: 19\n"
     ]
    }
   ],
   "source": [
    "# Inicialización del modelo BERT large\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "num_labels = len(tag2id)\n",
    "model = BertForTokenClassification.from_pretrained(\n",
    "    model_name, \n",
    "    num_labels=num_labels,\n",
    "    id2label={i: l for l, i in tag2id.items()},\n",
    "    label2id=tag2id,\n",
    "    ignore_mismatched_sizes=True  # Para manejar la diferencia en la capa final\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "print(f\"Model loaded: {model_name}\")\n",
    "print(f\"Number of labels: {num_labels}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "class F1RadioNERDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, hf_dataset, tokenizer, tag2id, max_len=128):\n",
    "        self.dataset = hf_dataset\n",
    "        self.tokenizer = tokenizer\n",
    "        self.tag2id = tag2id\n",
    "        self.max_len = max_len\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataset[idx]\n",
    "        words = item[\"tokens\"]\n",
    "        tags = item[\"ner_tags\"]\n",
    "        \n",
    "        # Convertir tags de string a ID si es necesario\n",
    "        tag_ids = []\n",
    "        for tag in tags:\n",
    "            if isinstance(tag, str):\n",
    "                tag_ids.append(self.tag2id[tag])\n",
    "            else:\n",
    "                tag_ids.append(tag)\n",
    "        \n",
    "        # Tokenizar el texto y alinear etiquetas\n",
    "        tokenized_inputs = self.tokenizer(\n",
    "            words,\n",
    "            is_split_into_words=True,\n",
    "            max_length=self.max_len,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        # Inicializar etiquetas con -100\n",
    "        labels = torch.ones(self.max_len, dtype=torch.long) * -100\n",
    "        \n",
    "        # Obtener word_ids para alinear etiquetas\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=0)\n",
    "        \n",
    "        # Establecer etiquetas para tokens no especiales\n",
    "        previous_word_idx = None\n",
    "        for i, word_idx in enumerate(word_ids):\n",
    "            if word_idx is not None:\n",
    "                if word_idx < len(tag_ids):\n",
    "                    # Si primera subpalabra, asignar etiqueta\n",
    "                    # Si no (token continuación), asignar -100 o misma etiqueta según lo que prefieras\n",
    "                    if word_idx != previous_word_idx:  # Nueva palabra\n",
    "                        labels[i] = tag_ids[word_idx]\n",
    "                    else:  # Continuación de palabra\n",
    "                        # Opción 1: Usar -100 para continuaciones\n",
    "                        # labels[i] = -100\n",
    "                        # Opción 2: Usar misma etiqueta para subpalabras\n",
    "                        labels[i] = tag_ids[word_idx]\n",
    "            previous_word_idx = word_idx\n",
    "        \n",
    "        return {\n",
    "            \"input_ids\": tokenized_inputs[\"input_ids\"].flatten(),\n",
    "            \"attention_mask\": tokenized_inputs[\"attention_mask\"].flatten(),\n",
    "            \"labels\": labels\n",
    "        }\n",
    "\n",
    "# Crear los datasets\n",
    "train_dataset = F1RadioNERDataset(datasets[\"train\"], tokenizer, tag2id)\n",
    "val_dataset = F1RadioNERDataset(datasets[\"validation\"], tokenizer, tag2id)\n",
    "test_dataset = F1RadioNERDataset(datasets[\"test\"], tokenizer, tag2id)\n",
    "\n",
    "# Crear los dataloaders\n",
    "batch_size = 8\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mantener la misma configuración de entrenamiento\n",
    "from sklearn.utils import compute_class_weight\n",
    "\n",
    "epochs = 10\n",
    "train_labels = []\n",
    "for batch in train_loader:\n",
    "    labels = batch['labels']\n",
    "    # Filter ignored tokens\n",
    "    mask = labels != -100\n",
    "    train_labels.extend(labels[mask].numpy())\n",
    "\n",
    "# Calculate weights per class\n",
    "class_weights = compute_class_weight(\n",
    "    'balanced', \n",
    "    classes=np.unique(train_labels), \n",
    "    y=train_labels\n",
    ")\n",
    "class_weights = torch.FloatTensor(class_weights).to(device)\n",
    "\n",
    "# Defining CrossEntropyLoss with class weights\n",
    "loss_fn = torch.nn.CrossEntropyLoss(weight=class_weights, ignore_index=-100)\n",
    "\n",
    "# Learning rate reducido\n",
    "# 2e-5\n",
    "learning_rate = 1e-5\n",
    "\n",
    "# Add warmup steps\n",
    "warmup_steps = int(0.1 * len(train_loader) * epochs)  # 10% of total steps\n",
    "total_steps = len(train_loader) * epochs\n",
    "\n",
    "# Optimizer Adam\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate, weight_decay=0.01)\n",
    "\n",
    "# Scheduler\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer, \n",
    "    num_warmup_steps=warmup_steps,\n",
    "    num_training_steps=total_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Epoch 1/10\n",
      "==================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8dfc42998d14e8bb4349d3e1f9d059b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 3.0243\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a7de612f84b47acb502a1f20fa5a604",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 2.8536\n",
      "Validation metrics: accuracy=0.0814, precision=0.2006, recall=0.0814, f1=0.0800\n",
      "New best model saved with F1: 0.0800\n",
      "\n",
      "==================================================\n",
      "Epoch 2/10\n",
      "==================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec01d842af864148ab712f896baa3267",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 2.7945\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e30ae517a494f28960d5ec026a91a1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 2.8052\n",
      "Validation metrics: accuracy=0.1265, precision=0.3278, recall=0.1265, f1=0.1148\n",
      "New best model saved with F1: 0.1148\n",
      "\n",
      "==================================================\n",
      "Epoch 3/10\n",
      "==================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2918bc13c07f4130aa2b11e842c7cfe8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 2.4414\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a16fb6376e4c4193b34dcbd457ae6620",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 2.6126\n",
      "Validation metrics: accuracy=0.1823, precision=0.4099, recall=0.1823, f1=0.1751\n",
      "New best model saved with F1: 0.1751\n",
      "\n",
      "==================================================\n",
      "Epoch 4/10\n",
      "==================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4d759f3a03947a09664473cdcef7dd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 2.0613\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed1ee97b9e04422fa93bad488e4cac94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 2.5291\n",
      "Validation metrics: accuracy=0.1796, precision=0.3414, recall=0.1796, f1=0.1577\n",
      "\n",
      "==================================================\n",
      "Epoch 5/10\n",
      "==================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf77af37305f454ea5bb16a1a9bef402",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Usar el mismo ciclo de entrenamiento\n",
    "best_f1 = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    train_loss = train_epoch()  # La función train_epoch() ya definida\n",
    "    print(f\"Training loss: {train_loss:.4f}\")\n",
    "    \n",
    "    val_metrics = evaluate(val_loader)  # La función evaluate() ya definida\n",
    "    print(f\"Validation loss: {val_metrics['loss']:.4f}\")\n",
    "    print(f\"Validation metrics: accuracy={val_metrics['accuracy']:.4f}, precision={val_metrics['precision']:.4f}, \"\n",
    "          f\"recall={val_metrics['recall']:.4f}, f1={val_metrics['f1']:.4f}\")\n",
    "    \n",
    "    # Save best model\n",
    "    if val_metrics['f1'] > best_f1:\n",
    "        best_f1 = val_metrics['f1']\n",
    "        torch.save(model.state_dict(), 'best_bert_large_ner_model.pt')\n",
    "        print(f\"New best model saved with F1: {best_f1:.4f}\")\n",
    "\n",
    "print(\"\\nTraining complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluación final con classification report\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Evaluate on test set\n",
    "print(\"\\nEvaluating on test set...\")\n",
    "test_metrics = evaluate(test_loader)\n",
    "print(f\"Test loss: {test_metrics['loss']:.4f}\")\n",
    "print(f\"Test metrics: accuracy={test_metrics['accuracy']:.4f}, precision={test_metrics['precision']:.4f}, \"\n",
    "      f\"recall={test_metrics['recall']:.4f}, f1={test_metrics['f1']:.4f}\")\n",
    "\n",
    "# Classification report detallado\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        preds = torch.argmax(outputs.logits, dim=2)\n",
    "        \n",
    "        # Filter -100 padding tokens\n",
    "        active_mask = labels != -100\n",
    "        true = labels[active_mask].cpu().numpy()\n",
    "        pred = preds[active_mask].cpu().numpy()\n",
    "        \n",
    "        all_labels.extend(true)\n",
    "        all_preds.extend(pred)\n",
    "\n",
    "# Convertir índices a etiquetas\n",
    "true_tags = [id2tag[l] for l in all_labels]\n",
    "pred_tags = [id2tag[p] for p in all_preds]\n",
    "\n",
    "# Imprimir el classification report\n",
    "print(classification_report(true_tags, pred_tags))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "f1_strat_manager",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
