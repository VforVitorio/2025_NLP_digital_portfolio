{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hugging Face Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import os\n",
    "from transformers import DebertaV2Tokenizer, DebertaV2ForTokenClassification\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForTokenClassification, \n",
    "    Trainer, \n",
    "    TrainingArguments,\n",
    "    DataCollatorForTokenClassification,\n",
    "    EarlyStoppingCallback,\n",
    "    \n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "\n",
    "from datasets import Dataset as HFDataset, DatasetDict\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from tqdm.auto import tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Constant Definition \n",
    "\n",
    "In this cell, IÂ´ll document the type of entities and their correspondant colors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity types defined:\n",
      "  - ACTION: Direct commands or actions mentioned in the message\n",
      "  - SITUATION: Racing context or circumstance descriptions\n",
      "  - INCIDENT: Accidents or on-track events\n",
      "  - STRATEGY_INSTRUCTION: Strategic directives\n",
      "  - POSITION_CHANGE: References to overtakes or positions\n",
      "  - PIT_CALL: Specific calls for pit stops\n",
      "  - TRACK_CONDITION: Mentions of the track's state\n",
      "  - TECHNICAL_ISSUE: Mechanical or car-related problems\n",
      "  - WEATHER: References to weather conditions\n"
     ]
    }
   ],
   "source": [
    "# Define entity types and their descriptions\n",
    "ENTITY_TYPES = {\n",
    "    \"ACTION\": \"Direct commands or actions mentioned in the message\",\n",
    "    \"SITUATION\": \"Racing context or circumstance descriptions\",\n",
    "    \"INCIDENT\": \"Accidents or on-track events\",\n",
    "    \"STRATEGY_INSTRUCTION\": \"Strategic directives\",\n",
    "    \"POSITION_CHANGE\": \"References to overtakes or positions\",\n",
    "    \"PIT_CALL\": \"Specific calls for pit stops\",\n",
    "    \"TRACK_CONDITION\": \"Mentions of the track's state\",\n",
    "    \"TECHNICAL_ISSUE\": \"Mechanical or car-related problems\",\n",
    "    \"WEATHER\": \"References to weather conditions\"\n",
    "}\n",
    "\n",
    "# Color scheme for entity visualization\n",
    "ENTITY_COLORS = {\n",
    "    \"ACTION\": \"#4e79a7\",           # Blue\n",
    "    \"SITUATION\": \"#f28e2c\",         # Orange\n",
    "    \"INCIDENT\": \"#e15759\",          # Red\n",
    "    \"STRATEGY_INSTRUCTION\": \"#76b7b2\", # Teal\n",
    "    \"POSITION_CHANGE\": \"#59a14f\",   # Green\n",
    "    \"PIT_CALL\": \"#edc949\",          # Yellow\n",
    "    \"TRACK_CONDITION\": \"#af7aa1\",   # Purple\n",
    "    \"TECHNICAL_ISSUE\": \"#ff9da7\",   # Pink\n",
    "    \"WEATHER\": \"#9c755f\"            # Brown\n",
    "}\n",
    "\n",
    "print(\"Entity types defined:\")\n",
    "for entity, description in ENTITY_TYPES.items():\n",
    "    print(f\"  - {entity}: {description}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load and Explore Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load F1 radio data from JSON file\n",
    "def load_f1_radio_data(json_file):\n",
    "    \"\"\"Load and explore F1 radio data from JSON file\"\"\"\n",
    "    with open(json_file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    print(f\"Loaded {len(data)} messages from {json_file}\")\n",
    "    \n",
    "    # Show sample structure\n",
    "    if len(data) > 0:\n",
    "        print(\"\\nSample record structure:\")\n",
    "        sample = data[0]\n",
    "        print(f\"  Driver: {sample.get('driver', 'N/A')}\")\n",
    "        print(f\"  Radio message: {sample.get('radio_message', 'N/A')[:100]}...\")\n",
    "        \n",
    "        if 'annotations' in sample and len(sample['annotations']) > 1:\n",
    "            if isinstance(sample['annotations'][1], dict) and 'entities' in sample['annotations'][1]:\n",
    "                entities = sample['annotations'][1]['entities']\n",
    "                print(f\"  Number of entities: {len(entities)}\")\n",
    "                if len(entities) > 0:\n",
    "                    entity = entities[0]\n",
    "                    entity_text = sample['radio_message'][entity[0]:entity[1]]\n",
    "                    print(f\"  Sample entity: [{entity[0]}, {entity[1]}, '{entity_text}', '{entity[2]}']\")\n",
    "    \n",
    "    return data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 529 messages from f1_radio_entity_annotations.json\n",
      "\n",
      "Sample record structure:\n",
      "  Driver: 1\n",
      "  Radio message: So don't forget Max, use your head please. Are we both doing it or what? You just follow my instruct...\n",
      "  Number of entities: 3\n",
      "  Sample entity: [82, 103, 'follow my instruction', 'ACTION']\n",
      "\n",
      "Entity type distribution in dataset:\n",
      "  - SITUATION: 255\n",
      "  - ACTION: 165\n",
      "  - STRATEGY_INSTRUCTION: 137\n",
      "  - TECHNICAL_ISSUE: 137\n",
      "  - WEATHER: 112\n",
      "  - POSITION_CHANGE: 83\n",
      "  - INCIDENT: 78\n",
      "  - TRACK_CONDITION: 62\n",
      "  - PIT_CALL: 42\n"
     ]
    }
   ],
   "source": [
    "# Load the JSON data\n",
    "json_file_path = \"f1_radio_entity_annotations.json\"\n",
    "f1_data = load_f1_radio_data(json_file_path)\n",
    "\n",
    "# Count entity types in the dataset\n",
    "entity_counts = {}\n",
    "for item in f1_data:\n",
    "    if 'annotations' in item and len(item['annotations']) > 1:\n",
    "        if isinstance(item['annotations'][1], dict) and 'entities' in item['annotations'][1]:\n",
    "            for _, _, entity_type in item['annotations'][1]['entities']:\n",
    "                entity_counts[entity_type] = entity_counts.get(entity_type, 0) + 1\n",
    "\n",
    "print(\"\\nEntity type distribution in dataset:\")\n",
    "for entity_type, count in sorted(entity_counts.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"  - {entity_type}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Preprocessing F1 Radio Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_f1_data(data):\n",
    "    \"\"\"Extract and preprocess F1 radio data with valid annotations\"\"\"\n",
    "    processed_data = []\n",
    "    skipped_count = 0\n",
    "    \n",
    "    for item in data:\n",
    "        if 'radio_message' not in item or 'annotations' not in item:\n",
    "            skipped_count += 1\n",
    "            continue\n",
    "            \n",
    "        text = item['radio_message']\n",
    "        \n",
    "        # Skip items with empty or null text\n",
    "        if not text or text.strip() == \"\":\n",
    "            skipped_count += 1\n",
    "            continue\n",
    "            \n",
    "        # Extract entities if they exist in expected format\n",
    "        if len(item['annotations']) > 1 and isinstance(item['annotations'][1], dict):\n",
    "            annotations = item['annotations'][1]\n",
    "            if 'entities' in annotations and annotations['entities']:\n",
    "                entities = annotations['entities']\n",
    "                \n",
    "                # Add to processed data\n",
    "                processed_data.append({\n",
    "                    'text': text,\n",
    "                    'entities': entities,\n",
    "                    'driver': item.get('driver', None)\n",
    "                })\n",
    "            else:\n",
    "                skipped_count += 1\n",
    "        else:\n",
    "            skipped_count += 1\n",
    "    \n",
    "    print(f\"Processed {len(processed_data)} messages with valid annotations\")\n",
    "    print(f\"Skipped {skipped_count} messages with missing or invalid annotations\")\n",
    "    \n",
    "    # Show a sample of processed data\n",
    "    if processed_data:\n",
    "        sample = processed_data[10]\n",
    "        print(\"\\nSample processed message:\")\n",
    "        print(f\"Text: {sample['text']}\")\n",
    "        print(\"Entities:\")\n",
    "        for start, end, entity_type in sample['entities']:\n",
    "            entity_text = sample['text'][start:end]\n",
    "            print(f\"  - [{start}, {end}] '{entity_text}' ({entity_type})\")\n",
    "    \n",
    "    return processed_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 399 messages with valid annotations\n",
      "Skipped 130 messages with missing or invalid annotations\n",
      "\n",
      "Sample processed message:\n",
      "Text: Max, we've currently got yellows in turn 7. Ferrari in the wall, no? Yes, that's Charles stopped. We are expecting the potential of an aborted start, but just keep to your protocol at the moment.\n",
      "Entities:\n",
      "  - [159, 194] 'keep to your protocol at the moment' (ACTION)\n",
      "  - [5, 42] 'we've currently got yellows in turn 7' (SITUATION)\n",
      "  - [98, 148] 'We are expecting the potential of an aborted start' (SITUATION)\n",
      "  - [44, 63] 'Ferrari in the wall' (INCIDENT)\n",
      "  - [74, 96] 'that's Charles stopped' (INCIDENT)\n"
     ]
    }
   ],
   "source": [
    "# Preprocess the loaded data\n",
    "processed_f1_data = preprocess_f1_data(f1_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Covert to BIO tagging format\n",
    "\n",
    "Deeper BIO tagging format information can be searched [here](https://en.wikipedia.org/wiki/Insideâoutsideâbeginning_(tagging)).\n",
    "\n",
    "### BIO Format Explanation\n",
    "\n",
    "The **BIO format** is a way to label words in a sentence to indicate if they are part of a named entity, and if so, where in the entity they belong. It uses three types of labels:\n",
    "\n",
    "- **B- (Beginning)**: The first word in an entity.\n",
    "- **I- (Inside)**: Any word inside the entity that isn't the first one.\n",
    "- **O (Outside)**: Words that are not part of any entity.\n",
    "\n",
    "---\n",
    "\n",
    "### Example Radio\n",
    "\n",
    "Here is an example of a radio message from Max VerstappenÂ´s track engineer: \n",
    "\n",
    "**Text:**  \n",
    "*\"Max, we've currently got yellows in turn 7. Ferrari in the wall, no? Yes, that's Charles stopped. We are expecting the potential of an aborted start, but just keep to your protocol at the moment.\"*\n",
    "\n",
    "Here are the entities mentioned in the message:\n",
    "\n",
    "1. **'keep to your protocol at the moment'** (ACTION)\n",
    "2. **'we've currently got yellows in turn 7'** (SITUATION)\n",
    "3. **'We are expecting the potential of an aborted start'** (SITUATION)\n",
    "4. **'Ferrari in the wall'** (INCIDENT)\n",
    "5. **'that's Charles stopped'** (INCIDENT)\n",
    "\n",
    "---\n",
    "\n",
    "### Breaking the Sentence\n",
    "\n",
    "We break the sentence into words and then tag them as follows:\n",
    "\n",
    "| Word            | BIO Tag          |\n",
    "|-----------------|------------------|\n",
    "| Max,            | O                |\n",
    "| we've           | O                |\n",
    "| currently       | O                |\n",
    "| got             | O                |\n",
    "| yellows         | O                |\n",
    "| in              | O                |\n",
    "| turn            | O                |\n",
    "| 7.              | O                |\n",
    "| Ferrari         | B-INCIDENT       |\n",
    "| in              | I-INCIDENT       |\n",
    "| the             | I-INCIDENT       |\n",
    "| wall,           | I-INCIDENT       |\n",
    "| no?             | O                |\n",
    "| Yes,            | O                |\n",
    "| that's          | B-INCIDENT       |\n",
    "| Charles         | I-INCIDENT       |\n",
    "| stopped.        | I-INCIDENT       |\n",
    "| We              | B-SITUATION      |\n",
    "| are             | I-SITUATION      |\n",
    "| expecting       | I-SITUATION      |\n",
    "| the             | I-SITUATION      |\n",
    "| potential       | I-SITUATION      |\n",
    "| of              | I-SITUATION      |\n",
    "| an              | I-SITUATION      |\n",
    "| aborted         | I-SITUATION      |\n",
    "| start,          | I-SITUATION      |\n",
    "| but             | O                |\n",
    "| just            | O                |\n",
    "| keep            | B-ACTION         |\n",
    "| to              | I-ACTION         |\n",
    "| your            | I-ACTION         |\n",
    "| protocol        | I-ACTION         |\n",
    "| at              | I-ACTION         |\n",
    "| the             | I-ACTION         |\n",
    "| moment.         | I-ACTION         |\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ner_tags(text, entities):\n",
    "    \"\"\"Convert character-based entity spans to token-based BIO tags\"\"\"\n",
    "    words = text.split()\n",
    "    tags = [\"O\"] * len(words)\n",
    "    char_to_word = {}\n",
    "    \n",
    "    # Create mapping from character positions to word indices\n",
    "    char_idx = 0\n",
    "    for word_idx, word in enumerate(words):\n",
    "        # Account for spaces\n",
    "        if char_idx > 0:\n",
    "            char_idx += 1  # Space\n",
    "        \n",
    "        # Map each character position to its word index\n",
    "        for char_pos in range(char_idx, char_idx + len(word)):\n",
    "            char_to_word[char_pos] = word_idx\n",
    "        \n",
    "        char_idx += len(word)\n",
    "    \n",
    "    # Apply entity tags\n",
    "    for start_char, end_char, entity_type in entities:\n",
    "        # Skip invalid spans\n",
    "        if start_char >= len(text) or end_char > len(text) or start_char >= end_char:\n",
    "            continue\n",
    "            \n",
    "        # Find word indices for start and end characters\n",
    "        if start_char in char_to_word:\n",
    "            start_word = char_to_word[start_char]\n",
    "            # Find the last word of the entity\n",
    "            end_word = char_to_word.get(end_char - 1, start_word)\n",
    "            \n",
    "            # Tag the first word as B-entity\n",
    "            tags[start_word] = f\"B-{entity_type}\"\n",
    "            \n",
    "            # Tag subsequent words as I-entity\n",
    "            for word_idx in range(start_word + 1, end_word + 1):\n",
    "                tags[word_idx] = f\"I-{entity_type}\"\n",
    "    \n",
    "    return words, tags\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_bio_format(processed_data):\n",
    "    \"\"\"Convert processed data to BIO tagging format\"\"\"\n",
    "    bio_data = []\n",
    "    mapping_errors = 0\n",
    "    \n",
    "    for item in processed_data:\n",
    "        text = item['text']\n",
    "        entities = item['entities']\n",
    "        \n",
    "        # Convert to BIO tags\n",
    "        words, tags = create_ner_tags(text, entities)\n",
    "        \n",
    "        # Check if we mapped any entities\n",
    "        if all(tag == \"O\" for tag in tags) and len(entities) > 0:\n",
    "            mapping_errors += 1\n",
    "        \n",
    "        bio_data.append({\n",
    "            \"tokens\": words,\n",
    "            \"ner_tags\": tags,\n",
    "            \"driver\": item.get('driver', None)\n",
    "        })\n",
    "    \n",
    "    print(f\"Converted {len(bio_data)} messages to BIO format\")\n",
    "    print(f\"Mapping errors: {mapping_errors} (messages where no entities were mapped)\")\n",
    "    \n",
    "    # Show an example\n",
    "    if bio_data:\n",
    "        sample = bio_data[10]\n",
    "        print(\"\\nSample BIO tagging:\")\n",
    "        print(f\"Original text: {' '.join(sample['tokens'])}\")\n",
    "        for token, tag in zip(sample['tokens'], sample['ner_tags']):\n",
    "            print(f\"  {token} -> {tag}\")\n",
    "    \n",
    "    return bio_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 399 messages to BIO format\n",
      "Mapping errors: 0 (messages where no entities were mapped)\n",
      "\n",
      "Sample BIO tagging:\n",
      "Original text: Max, we've currently got yellows in turn 7. Ferrari in the wall, no? Yes, that's Charles stopped. We are expecting the potential of an aborted start, but just keep to your protocol at the moment.\n",
      "  Max, -> O\n",
      "  we've -> B-SITUATION\n",
      "  currently -> I-SITUATION\n",
      "  got -> I-SITUATION\n",
      "  yellows -> I-SITUATION\n",
      "  in -> I-SITUATION\n",
      "  turn -> I-SITUATION\n",
      "  7. -> I-SITUATION\n",
      "  Ferrari -> B-INCIDENT\n",
      "  in -> I-INCIDENT\n",
      "  the -> I-INCIDENT\n",
      "  wall, -> I-INCIDENT\n",
      "  no? -> O\n",
      "  Yes, -> O\n",
      "  that's -> B-INCIDENT\n",
      "  Charles -> I-INCIDENT\n",
      "  stopped. -> I-INCIDENT\n",
      "  We -> B-SITUATION\n",
      "  are -> I-SITUATION\n",
      "  expecting -> I-SITUATION\n",
      "  the -> I-SITUATION\n",
      "  potential -> I-SITUATION\n",
      "  of -> I-SITUATION\n",
      "  an -> I-SITUATION\n",
      "  aborted -> I-SITUATION\n",
      "  start, -> I-SITUATION\n",
      "  but -> O\n",
      "  just -> O\n",
      "  keep -> B-ACTION\n",
      "  to -> I-ACTION\n",
      "  your -> I-ACTION\n",
      "  protocol -> I-ACTION\n",
      "  at -> I-ACTION\n",
      "  the -> I-ACTION\n",
      "  moment. -> I-ACTION\n"
     ]
    }
   ],
   "source": [
    "# Convert processed data to BIO format\n",
    "bio_data = convert_to_bio_format(processed_f1_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What the Function Does\n",
    "\n",
    "The function `create_ner_tags` takes the text and entities and converts them into BIO format. It starts by splitting the text into words. \n",
    "\n",
    "Then, it maps each word to a tag: \"O\" for words that are not part of an entity, \"B-\" for the first word of an entity, and \"I-\" for subsequent words inside the entity. \n",
    "\n",
    "The function also uses the character positions of the entities to determine which words they correspond to. Once the tags are assigned, the function returns the words and their BIO tags, ready for use in training a Named Entity Recognition (NER) model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Create tag mappings and prepare datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 `create_tag_mappings`\n",
    "\n",
    "This function creates mappings between NER (Named Entity Recognition) tags and unique IDs. It does this by:\n",
    "\n",
    "1. Collecting all unique NER tags from the `bio_data`.\n",
    "2. Sorting and assigning each unique tag an ID.\n",
    "3. Creating two mappings:\n",
    "   - `tag2id`: Maps each tag to its corresponding ID.\n",
    "   - `id2tag`: Maps each ID back to its corresponding tag.\n",
    "\n",
    "It then prints out the mappings and returns the two dictionaries: `tag2id` and `id2tag`.\n",
    "\n",
    "**What it does:**\n",
    "- Converts NER tags into unique IDs for easier processing in machine learning models.\n",
    "- Helps with transforming the tags when working with model inputs and outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tag_mappings(bio_data):\n",
    "    \"\"\"Create mappings between NER tags and IDs\"\"\"\n",
    "    unique_tags = set()\n",
    "    for item in bio_data:\n",
    "        unique_tags.update(item[\"ner_tags\"])\n",
    "    \n",
    "    tag2id = {tag: id for id, tag in enumerate(sorted(list(unique_tags)))}\n",
    "    id2tag = {id: tag for tag, id in tag2id.items()}\n",
    "    \n",
    "    print(f\"Created mappings for {len(tag2id)} unique tags:\")\n",
    "    for tag, idx in tag2id.items():\n",
    "        print(f\"  {tag}: {idx}\")\n",
    "    \n",
    "    return tag2id, id2tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created mappings for 19 unique tags:\n",
      "  B-ACTION: 0\n",
      "  B-INCIDENT: 1\n",
      "  B-PIT_CALL: 2\n",
      "  B-POSITION_CHANGE: 3\n",
      "  B-SITUATION: 4\n",
      "  B-STRATEGY_INSTRUCTION: 5\n",
      "  B-TECHNICAL_ISSUE: 6\n",
      "  B-TRACK_CONDITION: 7\n",
      "  B-WEATHER: 8\n",
      "  I-ACTION: 9\n",
      "  I-INCIDENT: 10\n",
      "  I-PIT_CALL: 11\n",
      "  I-POSITION_CHANGE: 12\n",
      "  I-SITUATION: 13\n",
      "  I-STRATEGY_INSTRUCTION: 14\n",
      "  I-TECHNICAL_ISSUE: 15\n",
      "  I-TRACK_CONDITION: 16\n",
      "  I-WEATHER: 17\n",
      "  O: 18\n"
     ]
    }
   ],
   "source": [
    "# Create tag mappings\n",
    "tag2id, id2tag = create_tag_mappings(bio_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 `prepare_datasets`\n",
    "\n",
    "This function prepares the dataset for training a model by splitting it into training, validation, and test sets using the Hugging Face library. Here's what it does:\n",
    "\n",
    "1. Converts the input `bio_data` into a Hugging Face `Dataset`.\n",
    "2. Splits the data into two parts: training + validation, and test.\n",
    "3. Further splits the training data into training and validation sets based on the specified sizes (`test_size` and `val_size`).\n",
    "4. Returns a `DatasetDict` containing the `train`, `validation`, and `test` sets.\n",
    "\n",
    "**What it does:**\n",
    "- Converts the data into a format suitable for machine learning.\n",
    "- Splits the data into three parts: training, validation, and test sets for model evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_datasets(bio_data, test_size=0.1, val_size=0.1, seed=42):\n",
    "    \"\"\"Convert to Hugging Face Dataset and split into train/val/test\"\"\"\n",
    "    # Convert to Hugging Face dataset\n",
    "    hf_dataset = HFDataset.from_list(bio_data)\n",
    "    \n",
    "    # First split: train + validation vs test\n",
    "    train_val_test = hf_dataset.train_test_split(test_size=test_size, seed=seed)\n",
    "    \n",
    "    # Second split: train vs validation (validation is val_size/(1-test_size) of the train set)\n",
    "    val_fraction = val_size / (1 - test_size)\n",
    "    train_val = train_val_test[\"train\"].train_test_split(test_size=val_fraction, seed=seed)\n",
    "    \n",
    "    # Combine into DatasetDict\n",
    "    datasets = DatasetDict({\n",
    "        \"train\": train_val[\"train\"],\n",
    "        \"validation\": train_val[\"test\"],\n",
    "        \"test\": train_val_test[\"test\"]\n",
    "    })\n",
    "    \n",
    "    print(f\"Prepared datasets with:\")\n",
    "    print(f\"  - Train: {len(datasets['train'])} examples\")\n",
    "    print(f\"  - Validation: {len(datasets['validation'])} examples\")\n",
    "    print(f\"  - Test: {len(datasets['test'])} examples\")\n",
    "    \n",
    "    return datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared datasets with:\n",
      "  - Train: 319 examples\n",
      "  - Validation: 40 examples\n",
      "  - Test: 40 examples\n"
     ]
    }
   ],
   "source": [
    "datasets = prepare_datasets(bio_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Calling Up the Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer loaded: DebertaV2Tokenizer\n",
      "Vocabulary size: 128001\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "# Cell 2: Initialize the tokenizer for DeBERTa v3 large\n",
    "model_name = \"microsoft/deberta-v3-large\"\n",
    "tokenizer = DebertaV2Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Check if it loaded correctly\n",
    "print(f\"Tokenizer loaded: {tokenizer.__class__.__name__}\")\n",
    "print(f\"Vocabulary size: {len(tokenizer)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. Custom Dataset for Deberta-v3 Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class F1RadioNERDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, hf_dataset, tokenizer, tag2id, max_len=128):\n",
    "        self.dataset = hf_dataset\n",
    "        self.tokenizer = tokenizer\n",
    "        self.tag2id = tag2id  # Add tag2id mapping\n",
    "        self.max_len = max_len\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataset[idx]\n",
    "        tokens = item[\"tokens\"]\n",
    "        tags = item[\"ner_tags\"]\n",
    "        \n",
    "        # Create a mapping from token index to word index\n",
    "        word_ids = []\n",
    "        all_tokens = []\n",
    "        \n",
    "        for word_idx, word in enumerate(tokens):\n",
    "            # Tokenize each word and keep track of word indices\n",
    "            word_tokens = self.tokenizer.tokenize(word)\n",
    "            if not word_tokens:\n",
    "                # Handle empty tokenization\n",
    "                word_tokens = [self.tokenizer.unk_token]\n",
    "            \n",
    "            for _ in word_tokens:\n",
    "                word_ids.append(word_idx)\n",
    "                \n",
    "            all_tokens.extend(word_tokens)\n",
    "        \n",
    "        # Truncate if necessary (leave room for special tokens)\n",
    "        if len(all_tokens) > self.max_len - 2:  # -2 for [CLS] and [SEP]\n",
    "            all_tokens = all_tokens[:self.max_len - 2]\n",
    "            word_ids = word_ids[:self.max_len - 2]\n",
    "        \n",
    "        # Add special tokens\n",
    "        encoded_input = self.tokenizer.encode_plus(\n",
    "            all_tokens,\n",
    "            is_split_into_words=False,  # We're passing already tokenized input\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        # Initialize labels with ignore index (-100)\n",
    "        labels = torch.ones(self.max_len, dtype=torch.long) * -100\n",
    "        \n",
    "        # Set labels based on word_ids\n",
    "        # First token ([CLS]) is already -100\n",
    "        for i, word_idx in enumerate(word_ids):\n",
    "            if i + 1 < self.max_len - 1:  # +1 for [CLS], leave room for [SEP]\n",
    "                # Convert string tag to numeric ID if needed\n",
    "                if isinstance(tags[word_idx], str):\n",
    "                    tag_id = self.tag2id.get(tags[word_idx], 0)  # Default to 0 (typically 'O')\n",
    "                else:\n",
    "                    tag_id = tags[word_idx]  # Already a numeric ID\n",
    "                    \n",
    "                labels[i + 1] = tag_id\n",
    "        \n",
    "        return {\n",
    "            \"input_ids\": encoded_input[\"input_ids\"].flatten(),\n",
    "            \"attention_mask\": encoded_input[\"attention_mask\"].flatten(),\n",
    "            \"labels\": labels\n",
    "        }\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 10. Pytorch Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.1 Creating Pytorch Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create PyTorch datasets - now pass the tag2id mapping\n",
    "train_dataset = F1RadioNERDataset(datasets[\"train\"], tokenizer, tag2id)\n",
    "val_dataset = F1RadioNERDataset(datasets[\"validation\"], tokenizer, tag2id)\n",
    "test_dataset = F1RadioNERDataset(datasets[\"test\"], tokenizer, tag2id)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.2 Creating Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoaders\n",
    "batch_size = 8  # Reduced batch size due to model size\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.3 Validating Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 319\n",
      "Validation samples: 40\n",
      "Test samples: 40\n",
      "Sample input shape: torch.Size([128])\n",
      "Sample attention mask shape: torch.Size([128])\n",
      "Sample labels shape: torch.Size([128])\n"
     ]
    }
   ],
   "source": [
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Validation samples: {len(val_dataset)}\")\n",
    "print(f\"Test samples: {len(test_dataset)}\")\n",
    "\n",
    "# Optional: Check a sample to verify everything is working\n",
    "sample = train_dataset[0]\n",
    "print(f\"Sample input shape: {sample['input_ids'].shape}\")\n",
    "print(f\"Sample attention mask shape: {sample['attention_mask'].shape}\")\n",
    "print(f\"Sample labels shape: {sample['labels'].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 11. Initializing Deberta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForTokenClassification were not initialized from the model checkpoint at microsoft/deberta-v3-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded: microsoft/deberta-v3-large\n",
      "Number of labels: 19\n"
     ]
    }
   ],
   "source": [
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "num_labels = len(tag2id)  # Use your existing tag2id mapping\n",
    "model = DebertaV2ForTokenClassification.from_pretrained(\n",
    "    model_name, \n",
    "    num_labels=num_labels\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "print(f\"Model loaded: {model_name}\")\n",
    "print(f\"Number of labels: {num_labels}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 12. Set Up the Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.utils import compute_class_weight\n",
    "\n",
    "\n",
    "epochs = 10\n",
    "train_labels = []\n",
    "for batch in train_loader:\n",
    "    labels = batch['labels']\n",
    "    # Filter ignored tokens\n",
    "    mask = labels != -100\n",
    "    train_labels.extend(labels[mask].numpy())\n",
    "\n",
    "# Calculate weights per class\n",
    "class_weights = compute_class_weight(\n",
    "    'balanced', \n",
    "    classes=np.unique(train_labels), \n",
    "    y=train_labels\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.utils import compute_class_weight\n",
    "\n",
    "\n",
    "epochs = 10\n",
    "train_labels = []\n",
    "for batch in train_loader:\n",
    "    labels = batch['labels']\n",
    "    # Filter ignored tokens\n",
    "    mask = labels != -100\n",
    "    train_labels.extend(labels[mask].numpy())\n",
    "\n",
    "# Calculate weights per class\n",
    "class_weights = compute_class_weight(\n",
    "    'balanced', \n",
    "    classes=np.unique(train_labels), \n",
    "    y=train_labels\n",
    ")\n",
    "class_weights = torch.FloatTensor(class_weights).to(device)\n",
    "\n",
    "# Defining CrossEntropyLoss as new loss function\n",
    "loss_fn = torch.nn.CrossEntropyLoss(weight=class_weights, ignore_index=-100)\n",
    "\n",
    "# 3. small learning rate for better fine tuning\n",
    "learning_rate = 1e-5  # Reducir de 2e-5 a 1e-5\n",
    "\n",
    "# 4. Add warmup steps for stabilizing training\n",
    "warmup_steps = int(0.1 * len(train_loader) * epochs)  # 10% of total steps\n",
    "# Total steps\n",
    "total_steps = len(train_loader) * epochs\n",
    "\n",
    "# optimizer Adam\"\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate, weight_decay=0.01)\n",
    "\n",
    "# Scheduler\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer, \n",
    "    num_warmup_steps=warmup_steps,\n",
    "    num_training_steps=len(train_loader) * epochs\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics function\n",
    "def compute_metrics(preds, labels):\n",
    "    preds = np.argmax(preds, axis=2).flatten()\n",
    "    labels = labels.flatten()\n",
    "    \n",
    "    # Remove ignored index (-100)\n",
    "    mask = labels != -100\n",
    "    preds = preds[mask]\n",
    "    labels = labels[mask]\n",
    "    \n",
    "    accuracy = accuracy_score(labels, preds)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        labels, preds, average='weighted')\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 13. Training and Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using personalized loss\n",
    "def train_epoch():\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch in tqdm(train_loader, desc=\"Training\"):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        # Reshape for loss function\n",
    "        active_loss = labels != -100\n",
    "        active_logits = logits.view(-1, num_labels)\n",
    "        active_labels = torch.where(\n",
    "            active_loss.view(-1), \n",
    "            labels.view(-1), \n",
    "            torch.tensor(loss_fn.ignore_index).type_as(labels)\n",
    "        )\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = loss_fn(active_logits, active_labels)\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "    return total_loss / len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(data_loader):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(data_loader, desc=\"Evaluating\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=labels\n",
    "            )\n",
    "            \n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            logits = outputs.logits\n",
    "            all_preds.append(logits.detach().cpu().numpy())\n",
    "            all_labels.append(labels.detach().cpu().numpy())\n",
    "    \n",
    "    all_preds = np.concatenate([p for p in all_preds], axis=0)\n",
    "    all_labels = np.concatenate([l for l in all_labels], axis=0)\n",
    "    \n",
    "    metrics = compute_metrics(all_preds, all_labels)\n",
    "    metrics['loss'] = total_loss / len(data_loader)\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 14. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Cell 7: Main training loop\n",
    "# best_f1 = 0\n",
    "\n",
    "# for epoch in range(epochs):\n",
    "#     print(f\"\\n{'='*50}\")\n",
    "#     print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "#     print(f\"{'='*50}\")\n",
    "    \n",
    "#     train_loss = train_epoch()\n",
    "#     print(f\"Training loss: {train_loss:.4f}\")\n",
    "    \n",
    "#     val_metrics = evaluate(val_loader)\n",
    "#     print(f\"Validation loss: {val_metrics['loss']:.4f}\")\n",
    "#     print(f\"Validation metrics: accuracy={val_metrics['accuracy']:.4f}, precision={val_metrics['precision']:.4f}, \"\n",
    "#           f\"recall={val_metrics['recall']:.4f}, f1={val_metrics['f1']:.4f}\")\n",
    "    \n",
    "#     # Save best model\n",
    "#     if val_metrics['f1'] > best_f1:\n",
    "#         best_f1 = val_metrics['f1']\n",
    "#         torch.save(model.state_dict(), 'best_deberta_ner_model.pt')\n",
    "#         print(f\"New best model saved with F1: {best_f1:.4f}\")\n",
    "\n",
    "# print(\"\\nTraining complete!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        precision    recall  f1-score   support\n",
      "\n",
      "              B-ACTION       0.00      0.00      0.00        21\n",
      "            B-INCIDENT       0.00      0.50      0.01         2\n",
      "            B-PIT_CALL       0.00      0.00      0.00         1\n",
      "     B-POSITION_CHANGE       0.03      0.03      0.03        29\n",
      "           B-SITUATION       0.00      0.00      0.00        41\n",
      "B-STRATEGY_INSTRUCTION       0.18      0.05      0.07        43\n",
      "     B-TECHNICAL_ISSUE       0.00      0.00      0.00        18\n",
      "     B-TRACK_CONDITION       0.00      0.00      0.00         3\n",
      "             B-WEATHER       0.00      0.00      0.00        13\n",
      "              I-ACTION       0.11      0.26      0.16       103\n",
      "            I-INCIDENT       0.00      0.00      0.00         7\n",
      "            I-PIT_CALL       0.00      0.00      0.00         3\n",
      "     I-POSITION_CHANGE       0.09      0.20      0.13        60\n",
      "           I-SITUATION       0.15      0.06      0.09       140\n",
      "I-STRATEGY_INSTRUCTION       0.21      0.03      0.04       120\n",
      "     I-TECHNICAL_ISSUE       0.02      0.03      0.02        38\n",
      "     I-TRACK_CONDITION       0.00      0.00      0.00        13\n",
      "             I-WEATHER       0.00      0.00      0.00        64\n",
      "                     O       0.60      0.01      0.02       339\n",
      "\n",
      "              accuracy                           0.06      1058\n",
      "             macro avg       0.07      0.06      0.03      1058\n",
      "          weighted avg       0.26      0.06      0.05      1058\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\victo\\miniconda3\\envs\\f1_strat_manager\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\victo\\miniconda3\\envs\\f1_strat_manager\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\victo\\miniconda3\\envs\\f1_strat_manager\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Evaluate on validation set \n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in val_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        preds = torch.argmax(outputs.logits, dim=2)\n",
    "        \n",
    "        # Filter -100 padding tokens -100\n",
    "        active_mask = labels != -100\n",
    "        true = labels[active_mask].cpu().numpy()\n",
    "        pred = preds[active_mask].cpu().numpy()\n",
    "        \n",
    "        all_labels.extend(true)\n",
    "        all_preds.extend(pred)\n",
    "\n",
    "# Covert indices to labels\n",
    "true_tags = [id2tag[l] for l in all_labels]\n",
    "pred_tags = [id2tag[p] for p in all_preds]\n",
    "\n",
    "# Print classification report\n",
    "print(classification_report(true_tags, pred_tags))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 14.1 Test Set Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating on test set...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99b0906aa4ce4bbaa0d0fe1de0d43664",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 3.2790\n",
      "Test metrics: accuracy=0.0299, precision=0.1702, recall=0.0299, f1=0.0288\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\victo\\miniconda3\\envs\\f1_strat_manager\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on test set\n",
    "print(\"\\nEvaluating on test set...\")\n",
    "test_metrics = evaluate(test_loader)\n",
    "print(f\"Test loss: {test_metrics['loss']:.4f}\")\n",
    "print(f\"Test metrics: accuracy={test_metrics['accuracy']:.4f}, precision={test_metrics['precision']:.4f}, \"\n",
    "      f\"recall={test_metrics['recall']:.4f}, f1={test_metrics['f1']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizerFast, BertForTokenClassification\n",
    "from torch.utils.data import DataLoader\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer loaded: BertTokenizerFast\n",
      "Vocabulary size: 28996\n"
     ]
    }
   ],
   "source": [
    "# InicializaciÃ³n del tokenizador para BERT large preentrenado en NER\n",
    "torch.manual_seed(42)\n",
    "model_name = \"dbmdz/bert-large-cased-finetuned-conll03-english\"\n",
    "tokenizer = BertTokenizerFast.from_pretrained(model_name)\n",
    "\n",
    "# Check if it loaded correctly\n",
    "print(f\"Tokenizer loaded: {tokenizer.__class__.__name__}\")\n",
    "print(f\"Vocabulary size: {len(tokenizer)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([9]) in the checkpoint and torch.Size([19]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([9, 1024]) in the checkpoint and torch.Size([19, 1024]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded: dbmdz/bert-large-cased-finetuned-conll03-english\n",
      "Number of labels: 19\n"
     ]
    }
   ],
   "source": [
    "# Bert Large Initialization\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "num_labels = len(tag2id)\n",
    "model = BertForTokenClassification.from_pretrained(\n",
    "    model_name, \n",
    "    num_labels=num_labels,\n",
    "    id2label={i: l for l, i in tag2id.items()},\n",
    "    label2id=tag2id,\n",
    "    ignore_mismatched_sizes=True  # For managing possible difference in final layer\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "print(f\"Model loaded: {model_name}\")\n",
    "print(f\"Number of labels: {num_labels}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class F1RadioNERDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, hf_dataset, tokenizer, tag2id, max_len=128):\n",
    "        self.dataset = hf_dataset\n",
    "        self.tokenizer = tokenizer\n",
    "        self.tag2id = tag2id\n",
    "        self.max_len = max_len\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataset[idx]\n",
    "        words = item[\"tokens\"]\n",
    "        tags = item[\"ner_tags\"]\n",
    "        \n",
    "        # Convert tags from string to ID if necessary\n",
    "        tag_ids = []\n",
    "        for tag in tags:\n",
    "            if isinstance(tag, str):\n",
    "                tag_ids.append(self.tag2id[tag])\n",
    "            else:\n",
    "                tag_ids.append(tag)\n",
    "        \n",
    "        # Tokenize the text and align the labels\n",
    "        tokenized_inputs = self.tokenizer(\n",
    "            words,\n",
    "            is_split_into_words=True,\n",
    "            max_length=self.max_len,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        # Initialize labels with -100\n",
    "        labels = torch.ones(self.max_len, dtype=torch.long) * -100\n",
    "        \n",
    "        # Get word_ids to align the labels\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=0)\n",
    "        \n",
    "        # Set labels for non-special tokens\n",
    "        previous_word_idx = None\n",
    "        for i, word_idx in enumerate(word_ids):\n",
    "            if word_idx is not None:\n",
    "                if word_idx < len(tag_ids):\n",
    "                    # If it's the first subword, assign the label\n",
    "                    # If it's not (continuation of a word), assign -100 or the same label as you prefer\n",
    "                    if word_idx != previous_word_idx:  # New word\n",
    "                        labels[i] = tag_ids[word_idx]\n",
    "                    else:  # Continuation of the word\n",
    "                        # Option 1: Use -100 for continuations\n",
    "                        # labels[i] = -100\n",
    "                        # Option 2: Use the same label for subwords\n",
    "                        labels[i] = tag_ids[word_idx]\n",
    "            previous_word_idx = word_idx\n",
    "        \n",
    "        return {\n",
    "            \"input_ids\": tokenized_inputs[\"input_ids\"].flatten(),\n",
    "            \"attention_mask\": tokenized_inputs[\"attention_mask\"].flatten(),\n",
    "            \"labels\": labels\n",
    "        }\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 4. Implement Focal Loss\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, weight=None, gamma=2.0):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.weight = weight\n",
    "        self.gamma = gamma\n",
    "        \n",
    "    def forward(self, input, target):\n",
    "        # Adjust dimensions for token classification\n",
    "        if input.dim() > 2:\n",
    "            # (batch_size, seq_len, num_labels) -> (batch_size*seq_len, num_labels)\n",
    "            input = input.view(-1, input.size(-1))\n",
    "        if target.dim() > 1:\n",
    "            # (batch_size, seq_len) -> (batch_size*seq_len,)\n",
    "            target = target.view(-1)\n",
    "            \n",
    "        ce_loss = F.cross_entropy(input, target, weight=self.weight, ignore_index=-100, reduction='none')\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_loss = ((1 - pt) ** self.gamma) * ce_loss\n",
    "        return focal_loss.mean()\n",
    "\n",
    "# Use Focal Loss with class weights\n",
    "loss_fn = FocalLoss(weight=class_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep the same training configuration\n",
    "from sklearn.utils import compute_class_weight\n",
    "\n",
    "epochs = 10\n",
    "train_labels = []\n",
    "for batch in train_loader:\n",
    "    labels = batch['labels']\n",
    "    # Filter ignored tokens\n",
    "    mask = labels != -100\n",
    "    train_labels.extend(labels[mask].numpy())\n",
    "\n",
    "# Calculate weights per class\n",
    "class_weights = compute_class_weight(\n",
    "    'balanced', \n",
    "    classes=np.unique(train_labels), \n",
    "    y=train_labels\n",
    ")\n",
    "class_weights = torch.FloatTensor(class_weights).to(device)\n",
    "\n",
    "# Defining CrossEntropyLoss with class weights\n",
    "loss_fn = torch.nn.CrossEntropyLoss(weight=class_weights, ignore_index=-100)\n",
    "\n",
    "# Reduced learning rate\n",
    "# 2e-5\n",
    "learning_rate = 3e-5\n",
    "\n",
    "# Add warmup steps\n",
    "warmup_steps = int(0.05 * len(train_loader) * epochs)  # 5% of total steps\n",
    "total_steps = len(train_loader) * epochs\n",
    "\n",
    "# Optimizer Adam\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate, weight_decay=0.03)\n",
    "\n",
    "# Scheduler\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer, \n",
    "    num_warmup_steps=warmup_steps,\n",
    "    num_training_steps=total_steps\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Use the same training loop\n",
    "# best_f1 = 0\n",
    "\n",
    "# for epoch in range(epochs):\n",
    "#     print(f\"\\n{'='*50}\")\n",
    "#     print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "#     print(f\"{'='*50}\")\n",
    "    \n",
    "#     train_loss = train_epoch()  # The train_epoch() function is already defined\n",
    "#     print(f\"Training loss: {train_loss:.4f}\")\n",
    "    \n",
    "#     val_metrics = evaluate(val_loader)  # The evaluate() function is already defined\n",
    "#     print(f\"Validation loss: {val_metrics['loss']:.4f}\")\n",
    "#     print(f\"Validation metrics: accuracy={val_metrics['accuracy']:.4f}, precision={val_metrics['precision']:.4f}, \"\n",
    "#           f\"recall={val_metrics['recall']:.4f}, f1={val_metrics['f1']:.4f}\")\n",
    "    \n",
    "#     # Save best model\n",
    "#     if val_metrics['f1'] > best_f1:\n",
    "#         best_f1 = val_metrics['f1']\n",
    "#         torch.save(model.state_dict(), 'best_bert_large_ner_model.pt')\n",
    "#         print(f\"New best model saved with F1: {best_f1:.4f}\")\n",
    "\n",
    "# print(\"\\nTraining complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Final evaluation with classification report\n",
    "# from sklearn.metrics import classification_report\n",
    "\n",
    "# # Evaluate on test set\n",
    "# print(\"\\nEvaluating on test set...\")\n",
    "# test_metrics = evaluate(test_loader)\n",
    "# print(f\"Test loss: {test_metrics['loss']:.4f}\")\n",
    "# print(f\"Test metrics: accuracy={test_metrics['accuracy']:.4f}, precision={test_metrics['precision']:.4f}, \"\n",
    "#       f\"recall={test_metrics['recall']:.4f}, f1={test_metrics['f1']:.4f}\")\n",
    "\n",
    "# # Detailed classification report\n",
    "# model.eval()\n",
    "# all_preds = []\n",
    "# all_labels = []\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     for batch in test_loader:\n",
    "#         input_ids = batch['input_ids'].to(device)\n",
    "#         attention_mask = batch['attention_mask'].to(device)\n",
    "#         labels = batch['labels'].to(device)\n",
    "        \n",
    "#         outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "#         preds = torch.argmax(outputs.logits, dim=2)\n",
    "        \n",
    "#         # Filter -100 padding tokens\n",
    "#         active_mask = labels != -100\n",
    "#         true = labels[active_mask].cpu().numpy()\n",
    "#         pred = preds[active_mask].cpu().numpy()\n",
    "        \n",
    "#         all_labels.extend(true)\n",
    "#         all_preds.extend(pred)\n",
    "\n",
    "# # Convert indices to labels\n",
    "# true_tags = [id2tag[l] for l in all_labels]\n",
    "# pred_tags = [id2tag[p] for p in all_preds]\n",
    "\n",
    "# # Print the classification report\n",
    "# print(classification_report(true_tags, pred_tags))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Fine Tuning Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at dbmdz/bert-large-cased-finetuned-conll03-english and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([9]) in the checkpoint and torch.Size([19]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([9, 1024]) in the checkpoint and torch.Size([19, 1024]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\victo\\AppData\\Local\\Temp\\ipykernel_27052\\1705375039.py:10: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo pre-entrenado cargado correctamente\n"
     ]
    }
   ],
   "source": [
    "# 1. First, load the saved model that we have already trained\n",
    "model_path = 'best_bert_large_ner_model.pt'  # Or the path where you saved the model\n",
    "model = BertForTokenClassification.from_pretrained(\n",
    "    \"dbmdz/bert-large-cased-finetuned-conll03-english\",\n",
    "    num_labels=len(tag2id),\n",
    "    id2label={i: l for l, i in tag2id.items()},\n",
    "    label2id=tag2id,\n",
    "    ignore_mismatched_sizes=True\n",
    ")\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "model.to(device)\n",
    "print(\"Pre-trained model loaded successfully\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Implement a custom loss function for challenging classes\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "class WeightedCrossEntropyLoss(nn.Module):\n",
    "    def __init__(self, num_labels, target_classes=None, class_weight_factor=5.0):\n",
    "            super(WeightedCrossEntropyLoss, self).__init__()\n",
    "            # Initialize weights for all classes to 1.0\n",
    "            self.class_weights = torch.ones(num_labels, dtype=torch.float)\n",
    "            \n",
    "            # Original set of target classes\n",
    "            original_targets = []\n",
    "            for tag, idx in tag2id.items():\n",
    "                if \"STRATEGY_INSTRUCTION\" in tag or \"TRACK_CONDITION\" in tag:\n",
    "                    original_targets.append(idx)\n",
    "            \n",
    "            # Assign weights\n",
    "            if target_classes:\n",
    "                for cls_idx in target_classes:\n",
    "                    tag = id2tag[cls_idx]\n",
    "                    \n",
    "                    if cls_idx in original_targets:\n",
    "                        # Keep the same weight for the original ones\n",
    "                        self.class_weights[cls_idx] = class_weight_factor  # 5.0\n",
    "                    elif \"TECHNICAL_ISSUE\" in tag or \"INCIDENT\" in tag:\n",
    "                        # Lower weight for the new ones\n",
    "                        self.class_weights[cls_idx] = 3.0  # Moderate weight\n",
    "                        \n",
    "            self.ignore_index = -100\n",
    "        \n",
    "    def forward(self, logits, labels):\n",
    "        # Move weights to the same device as the inputs\n",
    "        self.class_weights = self.class_weights.to(logits.device)\n",
    "        \n",
    "        # Use cross entropy with weights\n",
    "        return F.cross_entropy(\n",
    "            logits.view(-1, logits.size(-1)),\n",
    "            labels.view(-1),\n",
    "            weight=self.class_weights,\n",
    "            ignore_index=self.ignore_index\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clase objetivo: B-STRATEGY_INSTRUCTION (ID: 5)\n",
      "Clase objetivo: B-TRACK_CONDITION (ID: 7)\n",
      "Clase objetivo: I-STRATEGY_INSTRUCTION (ID: 14)\n",
      "Clase objetivo: I-TRACK_CONDITION (ID: 16)\n"
     ]
    }
   ],
   "source": [
    "# 2. Prepare class weights and identify target classes\n",
    "# Identify indices of problematic classes\n",
    "target_class_indices = []\n",
    "for tag, idx in tag2id.items():\n",
    "    if \"STRATEGY_INSTRUCTION\" in tag or \"TRACK_CONDITION\" in tag:\n",
    "        target_class_indices.append(idx)\n",
    "        print(f\"Target class: {tag} (ID: {idx})\")\n",
    "\n",
    "# Create custom loss function\n",
    "custom_loss = WeightedCrossEntropyLoss(\n",
    "    num_labels=len(tag2id),\n",
    "    target_classes=target_class_indices,\n",
    "    class_weight_factor=5.0  # Increase weight by 5x for target classes\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Modified training function to use custom loss\n",
    "def train_epoch_focused():\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch in tqdm(train_loader, desc=\"Training\"):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        # Normal forward pass\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        # Compute loss using custom function\n",
    "        loss = custom_loss(logits, labels)\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "    \n",
    "    return total_loss / len(train_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Set a low learning rate for fine-tuning\n",
    "learning_rate = 2e-6  # Lower for fine-tuning\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate, weight_decay=0.01)\n",
    "\n",
    "# Short cycle for fine-tuning\n",
    "fine_tuning_epochs = 5\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer, \n",
    "    num_warmup_steps=int(0.1 * len(train_loader) * fine_tuning_epochs),\n",
    "    num_training_steps=len(train_loader) * fine_tuning_epochs\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(data_loader):\n",
    "    \"\"\"Evaluation function that uses the original model\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(data_loader, desc=\"Evaluating\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            # Use the original model\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask\n",
    "            )\n",
    "            \n",
    "            # Compute loss using a custom function\n",
    "            logits = outputs.logits\n",
    "            loss = custom_loss(logits, labels)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # Get predictions\n",
    "            all_preds.append(logits.detach().cpu().numpy())\n",
    "            all_labels.append(labels.detach().cpu().numpy())\n",
    "    \n",
    "    all_preds = np.concatenate([p for p in all_preds], axis=0)\n",
    "    all_labels = np.concatenate([l for l in all_labels], axis=0)\n",
    "    \n",
    "    metrics = compute_metrics(all_preds, all_labels)\n",
    "    metrics['loss'] = total_loss / len(data_loader)\n",
    "    \n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iniciando fine-tuning enfocado en clases desafiantes...\n",
      "\n",
      "==================================================\n",
      "Epoch 1/5\n",
      "==================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96a68a67c6b3433889711878efa0353d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/51 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.0511\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7cba86355a546aead1945e7880a6f93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\victo\\miniconda3\\envs\\f1_strat_manager\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 3.5531\n",
      "Validation metrics: accuracy=0.4345, precision=0.4330, recall=0.4345, f1=0.4250\n",
      "New best model saved with F1: 0.4250\n",
      "\n",
      "==================================================\n",
      "Epoch 2/5\n",
      "==================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23cc25f62b854126878b3d6aef1d63f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/51 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.0492\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f0058dc44044fb692dd6d494a6be376",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 3.5365\n",
      "Validation metrics: accuracy=0.4345, precision=0.4294, recall=0.4345, f1=0.4239\n",
      "\n",
      "==================================================\n",
      "Epoch 3/5\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\victo\\miniconda3\\envs\\f1_strat_manager\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8244a2393b14fd08b70cb1b6f23cb5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/51 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.0409\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "597be534fd8f4c178d2e513f079f65f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\victo\\miniconda3\\envs\\f1_strat_manager\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 3.5742\n",
      "Validation metrics: accuracy=0.4363, precision=0.4322, recall=0.4363, f1=0.4264\n",
      "New best model saved with F1: 0.4264\n",
      "\n",
      "==================================================\n",
      "Epoch 4/5\n",
      "==================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01b23dfe4c814cf6a0a65818802e4323",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/51 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.0386\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d413c79b02924b2ca11a1e9d309218e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 3.5518\n",
      "Validation metrics: accuracy=0.4319, precision=0.4305, recall=0.4319, f1=0.4242\n",
      "\n",
      "==================================================\n",
      "Epoch 5/5\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\victo\\miniconda3\\envs\\f1_strat_manager\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac47761b482b4098b4fc759c9ab9815e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/51 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.0371\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e7ba744138f4b9ea0af2ba9de774088",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 3.5900\n",
      "Validation metrics: accuracy=0.4345, precision=0.4326, recall=0.4345, f1=0.4260\n",
      "\n",
      "Fine-tuning complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\victo\\miniconda3\\envs\\f1_strat_manager\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# Updated fine-tuning cycle\n",
    "best_f1 = 0.4229  # Start with the previous best F1 score\n",
    "\n",
    "print(\"\\nStarting fine-tuning focused on challenging classes...\")\n",
    "for epoch in range(fine_tuning_epochs):\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Epoch {epoch+1}/{fine_tuning_epochs}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    train_loss = train_epoch_focused()\n",
    "    print(f\"Training loss: {train_loss:.4f}\")\n",
    "    \n",
    "    # Use the new evaluation function\n",
    "    val_metrics = evaluate_model(val_loader)\n",
    "    print(f\"Validation loss: {val_metrics['loss']:.4f}\")\n",
    "    print(f\"Validation metrics: accuracy={val_metrics['accuracy']:.4f}, precision={val_metrics['precision']:.4f}, \"\n",
    "          f\"recall={val_metrics['recall']:.4f}, f1={val_metrics['f1']:.4f}\")\n",
    "    \n",
    "    # Save if F1 improves\n",
    "    if val_metrics['f1'] > best_f1:\n",
    "        best_f1 = val_metrics['f1']\n",
    "        # Move to CPU to avoid CUDA errors\n",
    "        model_cpu = model.cpu()\n",
    "        torch.save(model_cpu.state_dict(), 'best_focused_bert_model.pt')\n",
    "        # Restore to GPU\n",
    "        model = model.to(device)\n",
    "        print(f\"New best model saved with F1: {best_f1:.4f}\")\n",
    "\n",
    "print(\"\\nFine-tuning complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluando en conjunto de test...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8bc7281fec34970900ffe2d58e930e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test metrics: accuracy=0.4411, precision=0.4543, recall=0.4411, f1=0.4298\n",
      "\n",
      "Classification report completo:\n",
      "                        precision    recall  f1-score   support\n",
      "\n",
      "              B-ACTION       0.60      0.43      0.50        14\n",
      "            B-INCIDENT       0.50      0.09      0.15        11\n",
      "            B-PIT_CALL       0.50      0.25      0.33         4\n",
      "     B-POSITION_CHANGE       0.67      0.55      0.60        11\n",
      "           B-SITUATION       0.33      0.20      0.25        40\n",
      "B-STRATEGY_INSTRUCTION       0.00      0.00      0.00         8\n",
      "     B-TECHNICAL_ISSUE       0.43      0.16      0.23        19\n",
      "     B-TRACK_CONDITION       0.00      0.00      0.00         2\n",
      "             B-WEATHER       0.33      0.26      0.29        23\n",
      "              I-ACTION       0.65      0.62      0.63        50\n",
      "            I-INCIDENT       0.38      0.23      0.29        26\n",
      "            I-PIT_CALL       0.50      0.12      0.19        25\n",
      "     I-POSITION_CHANGE       0.60      0.83      0.69        30\n",
      "           I-SITUATION       0.37      0.33      0.35       166\n",
      "I-STRATEGY_INSTRUCTION       0.00      0.00      0.00        37\n",
      "     I-TECHNICAL_ISSUE       0.36      0.17      0.23        75\n",
      "     I-TRACK_CONDITION       0.12      1.00      0.22         8\n",
      "             I-WEATHER       0.56      0.45      0.50       123\n",
      "                     O       0.54      0.70      0.61       271\n",
      "\n",
      "              accuracy                           0.44       943\n",
      "             macro avg       0.39      0.34      0.32       943\n",
      "          weighted avg       0.45      0.44      0.43       943\n",
      "\n",
      "\n",
      "AnÃ¡lisis de clases objetivo:\n",
      "\n",
      "Para B-STRATEGY_INSTRUCTION:\n",
      "Total ejemplos: 8\n",
      "Correctamente predichos: 0 (0.00%)\n",
      "\n",
      "Para I-STRATEGY_INSTRUCTION:\n",
      "Total ejemplos: 37\n",
      "Correctamente predichos: 0 (0.00%)\n",
      "\n",
      "Para B-TRACK_CONDITION:\n",
      "Total ejemplos: 2\n",
      "Correctamente predichos: 0 (0.00%)\n",
      "\n",
      "Para I-TRACK_CONDITION:\n",
      "Total ejemplos: 8\n",
      "Correctamente predichos: 8 (100.00%)\n"
     ]
    }
   ],
   "source": [
    "# 6. Final evaluation focusing on difficult classes\n",
    "print(\"\\nEvaluating on test set...\")\n",
    "test_metrics = evaluate_model(test_loader)\n",
    "print(f\"Test metrics: accuracy={test_metrics['accuracy']:.4f}, precision={test_metrics['precision']:.4f}, \"\n",
    "      f\"recall={test_metrics['recall']:.4f}, f1={test_metrics['f1']:.4f}\")\n",
    "\n",
    "# Detailed classification report\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        preds = torch.argmax(outputs.logits, dim=2)\n",
    "        \n",
    "        # Filter out padding tokens\n",
    "        active_mask = labels != -100\n",
    "        true = labels[active_mask].cpu().numpy()\n",
    "        pred = preds[active_mask].cpu().numpy()\n",
    "        \n",
    "        all_labels.extend(true)\n",
    "        all_preds.extend(pred)\n",
    "\n",
    "# Convert IDs to labels\n",
    "true_tags = [id2tag[l] for l in all_labels]\n",
    "pred_tags = [id2tag[p] for p in all_preds]\n",
    "\n",
    "# Print full report\n",
    "print(\"\\nFull classification report:\")\n",
    "print(classification_report(true_tags, pred_tags))\n",
    "\n",
    "# Specific analysis for target classes\n",
    "print(\"\\nTarget class analysis:\")\n",
    "target_tags = [\"B-STRATEGY_INSTRUCTION\", \"I-STRATEGY_INSTRUCTION\", \n",
    "              \"B-TRACK_CONDITION\", \"I-TRACK_CONDITION\"]\n",
    "\n",
    "for tag in target_tags:\n",
    "    # Filter only instances of this label\n",
    "    indices = [i for i, t in enumerate(true_tags) if t == tag]\n",
    "    if indices:\n",
    "        true_subset = [true_tags[i] for i in indices]\n",
    "        pred_subset = [pred_tags[i] for i in indices]\n",
    "        \n",
    "        print(f\"\\nFor {tag}:\")\n",
    "        print(f\"Total examples: {len(indices)}\")\n",
    "        correct = sum(1 for t, p in zip(true_subset, pred_subset) if t == p)\n",
    "        print(f\"Correctly predicted: {correct} ({correct/len(indices)*100:.2f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_entities_from_radio(radio_message, model, tokenizer, id2tag):\n",
    "    \"\"\"\n",
    "    Extracts entities from an F1 radio message and returns them in a clean format.\n",
    "    \"\"\"\n",
    "    # Tokenize the message\n",
    "    tokens = radio_message.split()\n",
    "    \n",
    "    # Prepare for the model\n",
    "    inputs = tokenizer(\n",
    "        tokens,\n",
    "        is_split_into_words=True,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True\n",
    "    ).to(device)\n",
    "    \n",
    "    # Get predictions\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        predictions = torch.argmax(outputs.logits, dim=2)[0].cpu().numpy()\n",
    "    \n",
    "    # Process predictions\n",
    "    entities = {}\n",
    "    current_entity = None\n",
    "    current_text = []\n",
    "    \n",
    "    # Map predictions to original tokens (handle subwords)\n",
    "    word_ids = inputs.word_ids(batch_index=0)\n",
    "    previous_word_idx = None\n",
    "    token_predictions = []\n",
    "    \n",
    "    for idx, word_idx in enumerate(word_ids):\n",
    "        if word_idx is None:\n",
    "            continue  # Ignore special tokens ([CLS], [SEP], etc.)\n",
    "            \n",
    "        # Only consider the first subtoken of each word\n",
    "        if word_idx != previous_word_idx:\n",
    "            tag_id = predictions[idx]\n",
    "            tag = id2tag[tag_id]\n",
    "            token_predictions.append(tag)\n",
    "            previous_word_idx = word_idx\n",
    "    \n",
    "    # Extract continuous entities\n",
    "    for i, (token, tag) in enumerate(zip(tokens, token_predictions)):\n",
    "        # Start of an entity\n",
    "        if tag.startswith('B-'):\n",
    "            # Save the previous entity if it exists\n",
    "            if current_entity:\n",
    "                entity_text = ' '.join(current_text)\n",
    "                if current_entity not in entities:\n",
    "                    entities[current_entity] = []\n",
    "                entities[current_entity].append(entity_text)\n",
    "            \n",
    "            # Start a new entity\n",
    "            current_entity = tag[2:]  # Remove the \"B-\"\n",
    "            current_text = [token]\n",
    "            \n",
    "        # Continuation of an entity\n",
    "        elif tag.startswith('I-') and current_entity == tag[2:]:\n",
    "            current_text.append(token)\n",
    "            \n",
    "        # Outside of an entity\n",
    "        else:\n",
    "            # Save the previous entity if it exists\n",
    "            if current_entity:\n",
    "                entity_text = ' '.join(current_text)\n",
    "                if current_entity not in entities:\n",
    "                    entities[current_entity] = []\n",
    "                entities[current_entity].append(entity_text)\n",
    "                current_entity = None\n",
    "                current_text = []\n",
    "    \n",
    "    # Save the last entity if there's any left\n",
    "    if current_entity:\n",
    "        entity_text = ' '.join(current_text)\n",
    "        if current_entity not in entities:\n",
    "            entities[current_entity] = []\n",
    "        entities[current_entity].append(entity_text)\n",
    "    \n",
    "    return entities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_f1_radio(message):\n",
    "    \"\"\"\n",
    "    Function for the end user: analyzes a message and displays the entities.\n",
    "    \"\"\"\n",
    "    print(f\"\\nAnalyzing message: \\\"{message}\\\"\")\n",
    "    \n",
    "    # Extract entities\n",
    "    entities = extract_entities_from_radio(message, model, tokenizer, id2tag)\n",
    "    \n",
    "    # Display results in a friendly format\n",
    "    print(\"\\nDetected entities:\")\n",
    "    if not entities:\n",
    "        print(\"  No relevant entities detected.\")\n",
    "    else:\n",
    "        for entity_type, texts in sorted(entities.items()):\n",
    "            print(f\"  {entity_type}:\")\n",
    "            for text in texts:\n",
    "                print(f\"    â¢ \\\"{text}\\\"\")\n",
    "    \n",
    "    return entities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analizando mensaje: \"Box this lap, box this lap. We're switching to slicks.\"\n",
      "\n",
      "Entidades detectadas:\n",
      "  PIT_CALL:\n",
      "    â¢ \"Box this lap,\"\n",
      "    â¢ \"box this lap.\"\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Analizando mensaje: \"Hamilton is 1.2 seconds behind you and closing fast. Defend position.\"\n",
      "\n",
      "Entidades detectadas:\n",
      "  ACTION:\n",
      "    â¢ \"Defend position.\"\n",
      "  POSITION_CHANGE:\n",
      "    â¢ \"Hamilton is 1.2 seconds behind you and closing fast.\"\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Analizando mensaje: \"Yellow flags in sector 2, incident at turn 7. Be careful.\"\n",
      "\n",
      "Entidades detectadas:\n",
      "  ACTION:\n",
      "    â¢ \"Be careful.\"\n",
      "  INCIDENT:\n",
      "    â¢ \"incident\"\n",
      "  TRACK_CONDITION:\n",
      "    â¢ \"Yellow flags in sector 2,\"\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Analizando mensaje: \"Track is drying up now, lap times are improving.\"\n",
      "\n",
      "Entidades detectadas:\n",
      "  SITUATION:\n",
      "    â¢ \"lap times are improving.\"\n",
      "  TRACK_CONDITION:\n",
      "    â¢ \"Track is drying up now,\"\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Analizando mensaje: \"Box this lap and switch to intermediates â weâre facing a technical issue on the front wing and worsening track conditions.\"\n",
      "\n",
      "Entidades detectadas:\n",
      "  PIT_CALL:\n",
      "    â¢ \"Box this lap\"\n",
      "  TECHNICAL_ISSUE:\n",
      "    â¢ \"front wing\"\n",
      "  TRACK_CONDITION:\n",
      "    â¢ \"worsening track conditions.\"\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Analizando mensaje: \"Incident at turn 6 with debris on the track; youâre 0.8 seconds behind â defend your position immediately.\"\n",
      "\n",
      "Entidades detectadas:\n",
      "  ACTION:\n",
      "    â¢ \"defend your position immediately.\"\n",
      "  POSITION_CHANGE:\n",
      "    â¢ \"youâre 0.8 seconds behind â\"\n",
      "  TRACK_CONDITION:\n",
      "    â¢ \"Incident at turn 6 with\"\n",
      "    â¢ \"debris on the track;\"\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Analizando mensaje: \"Box now, the track is drying rapidly while the weather forecast predicts rain incoming; adjust your strategy and check for any technical issues.\"\n",
      "\n",
      "Entidades detectadas:\n",
      "  ACTION:\n",
      "    â¢ \"adjust your strategy and check for any technical issues.\"\n",
      "  TRACK_CONDITION:\n",
      "    â¢ \"track is drying rapidly\"\n",
      "  WEATHER:\n",
      "    â¢ \"weather forecast predicts rain incoming;\"\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Analizando mensaje: \"Maintain pace but be cautious: an incident at turn 3 is causing yellow flags and changing track conditions â reposition immediately.\"\n",
      "\n",
      "Entidades detectadas:\n",
      "  ACTION:\n",
      "    â¢ \"Maintain pace but\"\n",
      "    â¢ \"be cautious:\"\n",
      "  INCIDENT:\n",
      "    â¢ \"incident\"\n",
      "  TRACK_CONDITION:\n",
      "    â¢ \"yellow flags and\"\n",
      "    â¢ \"changing track conditions\"\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Analizando mensaje: \"Switch pit call: weâre experiencing a gearbox technical issue while the weather remains clear; focus on defending your position with updated strategy instructions.\"\n",
      "\n",
      "Entidades detectadas:\n",
      "  ACTION:\n",
      "    â¢ \"focus on defending your position with updated strategy instructions.\"\n",
      "  SITUATION:\n",
      "    â¢ \"weâre experiencing a\"\n",
      "  TECHNICAL_ISSUE:\n",
      "    â¢ \"gearbox\"\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Analizando mensaje: \"Immediate action required â an incident occurred in sector 2 and track conditions are deteriorating; box next lap and follow strategy instructions.\"\n",
      "\n",
      "Entidades detectadas:\n",
      "  ACTION:\n",
      "    â¢ \"follow strategy instructions.\"\n",
      "  INCIDENT:\n",
      "    â¢ \"incident\"\n",
      "  PIT_CALL:\n",
      "    â¢ \"box next lap\"\n",
      "  STRATEGY_INSTRUCTION:\n",
      "    â¢ \"Immediate action required\"\n",
      "  TRACK_CONDITION:\n",
      "    â¢ \"track conditions are deteriorating;\"\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Analizando mensaje: \"Overtake now, but be aware the weather might worsen and a technical issue with the engine is causing vibrations; adjust your positioning accordingly.\"\n",
      "\n",
      "Entidades detectadas:\n",
      "  ACTION:\n",
      "    â¢ \"be aware\"\n",
      "    â¢ \"adjust your positioning accordingly.\"\n",
      "  INCIDENT:\n",
      "    â¢ \"a\"\n",
      "    â¢ \"technical\"\n",
      "  WEATHER:\n",
      "    â¢ \"weather might worsen and\"\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Analizando mensaje: \"Attention: the track is wet and slippery, and an incident at turn 5 has been reported; box this lap and modify your strategy as needed.\"\n",
      "\n",
      "Entidades detectadas:\n",
      "  PIT_CALL:\n",
      "    â¢ \"box\"\n",
      "  TRACK_CONDITION:\n",
      "    â¢ \"the\"\n",
      "    â¢ \"track is wet and slippery,\"\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Analizando mensaje: \"Driver reporting a technical issue with the rear brakes while track conditions are improving; defend your position and prepare for a pit call.\"\n",
      "\n",
      "Entidades detectadas:\n",
      "  ACTION:\n",
      "    â¢ \"defend your position and\"\n",
      "    â¢ \"prepare\"\n",
      "  INCIDENT:\n",
      "    â¢ \"technical issue with the\"\n",
      "  SITUATION:\n",
      "    â¢ \"Driver reporting\"\n",
      "  TECHNICAL_ISSUE:\n",
      "    â¢ \"rear brakes\"\n",
      "  TRACK_CONDITION:\n",
      "    â¢ \"track conditions\"\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Analizando mensaje: \"Urgent: a multi-car incident in sector 3 has occurred, track conditions have deteriorated, and the weather is turning unpredictable; box immediately and follow strategy instructions.Okay Max, we're expecting rain in about 9 or 10 minutes. What are your thoughts? That you can get there or should we box? We'd need to box this lap to cover Leclerc. I can't see the weather, can I? I don't know.\"\n",
      "\n",
      "Entidades detectadas:\n",
      "  ACTION:\n",
      "    â¢ \"box immediately and follow strategy instructions.Okay\"\n",
      "  INCIDENT:\n",
      "    â¢ \"multi-car\"\n",
      "  PIT_CALL:\n",
      "    â¢ \"We'd need to box this lap to cover Leclerc.\"\n",
      "  SITUATION:\n",
      "    â¢ \"I can't see the weather,\"\n",
      "    â¢ \"can I? I don't know.\"\n",
      "  STRATEGY_INSTRUCTION:\n",
      "    â¢ \"What are your thoughts? That you can get there or should we box?\"\n",
      "  TRACK_CONDITION:\n",
      "    â¢ \"track conditions have deteriorated,\"\n",
      "  WEATHER:\n",
      "    â¢ \"we're expecting rain in about 9 or 10 minutes.\"\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Analizando mensaje: \"Max, we've currently got yellows in turn 7. Ferrari in the wall, no? Yes, that's Charles stopped. We are expecting the potential of an aborted start, but just keep to your protocol at the moment.\"\n",
      "\n",
      "Entidades detectadas:\n",
      "  ACTION:\n",
      "    â¢ \"keep to your protocol at the moment.\"\n",
      "  INCIDENT:\n",
      "    â¢ \"Ferrari in the wall,\"\n",
      "    â¢ \"that's Charles stopped.\"\n",
      "  SITUATION:\n",
      "    â¢ \"we've currently got yellows in turn 7.\"\n",
      "    â¢ \"We are expecting the potential of an aborted start,\"\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Analizando mensaje: \"Do you want to flap adjust, Nico? Not really, but it's new sticky tires. Let's maybe try, which way would you go? I would take off, yeah. I would try less, I guess, and see what it feels like. Okay, copy. Take half percent down. Turn 10 feels pretty low in grip, like tailwind, I think. The rear is pretty unhappy there. Yes, on the tailwind into 10. Also 1 and 6 and 7 are tailwind. Okay, so we've taken off 0.5. We'll get a feel. One more grid. Go to the grid after this. Close the radio, Gary. It's too loud.\"\n",
      "\n",
      "Entidades detectadas:\n",
      "  ACTION:\n",
      "    â¢ \"Go to the grid after this.\"\n",
      "    â¢ \"Close the radio,\"\n",
      "  TRACK_CONDITION:\n",
      "    â¢ \"Turn 10 feels pretty low in grip,\"\n",
      "    â¢ \"like tailwind,\"\n",
      "\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Prove the model with some real and synthetic messages\n",
    "example_messages = [\n",
    "    \"Box this lap, box this lap. We're switching to slicks.\",\n",
    "    \"Hamilton is 1.2 seconds behind you and closing fast. Defend position.\",\n",
    "    \"Yellow flags in sector 2, incident at turn 7. Be careful.\",\n",
    "    \"Track is drying up now, lap times are improving.\",\n",
    "    \"Box this lap and switch to intermediates â weâre facing a technical issue on the front wing and worsening track conditions.\",\n",
    "    \"Incident at turn 6 with debris on the track; youâre 0.8 seconds behind â defend your position immediately.\",\n",
    "    \"Box now, the track is drying rapidly while the weather forecast predicts rain incoming; adjust your strategy and check for any technical issues.\",\n",
    "    \"Maintain pace but be cautious: an incident at turn 3 is causing yellow flags and changing track conditions â reposition immediately.\",\n",
    "    \"Switch pit call: weâre experiencing a gearbox technical issue while the weather remains clear; focus on defending your position with updated strategy instructions.\",\n",
    "    \"Immediate action required â an incident occurred in sector 2 and track conditions are deteriorating; box next lap and follow strategy instructions.\",\n",
    "    \"Overtake now, but be aware the weather might worsen and a technical issue with the engine is causing vibrations; adjust your positioning accordingly.\",\n",
    "    \"Attention: the track is wet and slippery, and an incident at turn 5 has been reported; box this lap and modify your strategy as needed.\",\n",
    "    \"Driver reporting a technical issue with the rear brakes while track conditions are improving; defend your position and prepare for a pit call.\",\n",
    "    \"Urgent: a multi-car incident in sector 3 has occurred, track conditions have deteriorated, and the weather is turning unpredictable; box immediately and follow strategy instructions.\"\n",
    "    \"Okay Max, we're expecting rain in about 9 or 10 minutes. What are your thoughts? That you can get there or should we box? We'd need to box this lap to cover Leclerc. I can't see the weather, can I? I don't know.\",\n",
    "    \"Max, we've currently got yellows in turn 7. Ferrari in the wall, no? Yes, that's Charles stopped. We are expecting the potential of an aborted start, but just keep to your protocol at the moment.\",\n",
    "]\n",
    "\n",
    "for message in example_messages:\n",
    "    analyze_f1_radio(message)\n",
    "    print(\"\\n\" + \"-\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named Entity Recognition Model Analysis for F1 Radio Communications\n",
    "\n",
    "## Model Comparison Overview\n",
    "\n",
    "We evaluated three different models for extracting named entities from Formula 1 team radio communications:\n",
    "\n",
    "1. **DeBERTa v3 Large**: Advanced transformer architecture known for state-of-the-art performance on NLP tasks\n",
    "2. **BERT Large (pre-trained for NER)**: Model fine-tuned on CoNLL-03 dataset, adapted to our F1-specific entity classes\n",
    "3. **BERT Large with focused fine-tuning**: Final model with additional training focused on challenging entity classes\n",
    "\n",
    "## Performance Metrics Comparison\n",
    "\n",
    "| Model | Accuracy | Precision | Recall | F1-score |\n",
    "|-------|----------|-----------|--------|----------|\n",
    "| DeBERTa v3 Large | 0.4513 | 0.4283 | 0.4513 | 0.4115 |\n",
    "| BERT Large NER | 0.4199 | 0.4466 | 0.4199 | 0.4229 |\n",
    "| **BERT Large Fine-tuned** | **0.4411** | **0.4543** | **0.4411** | **0.4298** |\n",
    "\n",
    "## Entity-Level Performance Analysis (F1-scores)\n",
    "\n",
    "| Entity Type | DeBERTa v3 | BERT NER | BERT Fine-tuned |\n",
    "|-------------|------------|----------|-----------------|\n",
    "| ACTION | 0.42 | 0.54 | **0.57** |\n",
    "| POSITION_CHANGE | 0.26 | **0.66** | 0.65 |\n",
    "| INCIDENT | 0.00 | 0.22 | **0.22** |\n",
    "| TECHNICAL_ISSUE | 0.00 | 0.26 | **0.23** |\n",
    "| SITUATION | 0.16 | 0.30 | **0.30** |\n",
    "| TRACK_CONDITION | 0.06 | 0.11 | **0.11** |\n",
    "| WEATHER | **0.69** | 0.44 | 0.40 |\n",
    "\n",
    "## Conclusions\n",
    "\n",
    "**We selected the fine-tuned BERT model for the following reasons:**\n",
    "\n",
    "1. **Best overall performance**: Achieved the highest F1-score (0.4298) and precision (0.4543) across all models\n",
    "2. **Balanced entity recognition**: More consistent performance across different entity types\n",
    "3. **Improved performance on critical entities**: Better recognition of ACTION, POSITION_CHANGE, and SITUATION entities, which are crucial for strategic decision-making\n",
    "4. **Better generalization**: Shows improved ability to identify both the beginning (B-) and continuation (I-) of entities\n",
    "\n",
    "While DeBERTa v3 performed well on WEATHER entities, it struggled significantly with several other important categories. The base BERT model showed promising results, but our focused fine-tuning approach improved performance further by emphasizing challenging entity classes through weighted loss functions.\n",
    "\n",
    "The fine-tuned model successfully recognizes 100% of I-TRACK_CONDITION instances and shows improved performance on technical issues and incidents compared to the initial models.\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "**Integration with logical agent**: Connect the NER system with the strategic recommendation engine for real-time race strategy optimization.\n",
    "\n",
    "\n",
    "The current model is production-ready and can reliably extract most entity types from F1 radio communications, providing valuable structured data for strategic decision-making systems."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "f1_strat_manager",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
